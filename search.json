[
  {
    "objectID": "validation.html",
    "href": "validation.html",
    "title": "Validation",
    "section": "",
    "text": "source\n\nvalidate_format\n\n validate_format\n                  (df:Union[pandas.core.frame.DataFrame,polars.dataframe.f\n                  rame.DataFrame], id_col:str='unique_id',\n                  time_col:str='ds', target_col:str='y')\n\nEnsure DataFrame has expected format.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nDataFrame with time series in long format.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestamp.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\nNone\n\n\n\n\n\n\nimport datetime\n\nimport pandas as pd\nfrom fastcore.test import test_fail\n\nfrom utilsforecast.compat import POLARS_INSTALLED, pl\nfrom utilsforecast.data import generate_series\n\n\ntest_fail(lambda: validate_format(1), contains=\"got &lt;class 'int'&gt;\")\nconstructors = [pd.DataFrame]\nif POLARS_INSTALLED:\n    constructors.append(pl.DataFrame)\nfor constructor in constructors:\n    df = constructor({'unique_id': [1]})\n    test_fail(lambda: validate_format(df), contains=\"missing: ['ds', 'y']\")\n    df = constructor({'unique_id': [1], 'time': ['x'], 'y': [1]})\n    test_fail(lambda: validate_format(df, time_col='time'), contains=\"('time') should have either timestamps or integers\")\n    for time in [1, datetime.datetime(2000, 1, 1)]:\n        df = constructor({'unique_id': [1], 'ds': [time], 'sales': ['x']})\n        test_fail(lambda: validate_format(df, target_col='sales'), contains=\"('sales') should have a numeric data type\")\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "source\n\ngenerate_series\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, with_trend:bool=False,\n                  static_as_categorical:bool=True, n_models:int=0,\n                  level:Optional[List[float]]=None, engine:str='pandas',\n                  seed:int=0)\n\nGenerate Synthetic Panel Series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_series\nint\n\nNumber of series for synthetic panel.\n\n\nfreq\nstr\nD\nFrequency of the data, ‘D’ or ‘M’.\n\n\nmin_length\nint\n50\nMinimum length of synthetic panel’s series.\n\n\nmax_length\nint\n500\nMaximum length of synthetic panel’s series.\n\n\nn_static_features\nint\n0\nNumber of static exogenous variables for synthetic panel’s series.\n\n\nequal_ends\nbool\nFalse\nSeries should end in the same date stamp ds.\n\n\nwith_trend\nbool\nFalse\nSeries should have a (positive) trend.\n\n\nstatic_as_categorical\nbool\nTrue\nStatic features should have a categorical data type.\n\n\nn_models\nint\n0\nNumber of models predictions to simulate.\n\n\nlevel\ntyping.Optional[typing.List[float]]\nNone\nConfidence level for intervals to simulate for each model.\n\n\nengine\nstr\npandas\nOutput Dataframe type.\n\n\nseed\nint\n0\nRandom seed used for generating the data.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nSynthetic panel with columns [unique_id, ds, y] and exogenous features.\n\n\n\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id', observed=True).head(4)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2000-01-01\n0.357595\n\n\n1\n0\n2000-01-02\n1.301382\n\n\n2\n0\n2000-01-03\n2.272442\n\n\n3\n0\n2000-01-04\n3.211827\n\n\n222\n1\n2000-01-01\n5.399023\n\n\n223\n1\n2000-01-02\n6.092818\n\n\n224\n1\n2000-01-03\n0.476396\n\n\n225\n1\n2000-01-04\n1.343744\n\n\n\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "Losses",
    "section": "",
    "text": "The most important train signal is the forecast error, which is the difference between the observed value \\(y_{\\tau}\\) and the prediction \\(\\hat{y}_{\\tau}\\), at time \\(y_{\\tau}\\):\n\\[ e_{\\tau} = y_{\\tau}-\\hat{y}_{\\tau} \\qquad \\qquad \\tau \\in \\{t+1,\\dots,t+H \\} \\]\nThe train loss summarizes the forecast errors in different evaluation metrics.\nfrom utilsforecast.data import generate_series\nmodels = ['model0', 'model1']\nseries = generate_series(10, n_models=2, level=[80])\nseries_pl = generate_series(10, n_models=2, level=[80], engine='polars')\nGive us a ⭐ on Github"
  },
  {
    "objectID": "losses.html#mean-absolute-error-mae",
    "href": "losses.html#mean-absolute-error-mae",
    "title": "Losses",
    "section": "Mean Absolute Error (MAE)",
    "text": "Mean Absolute Error (MAE)\n\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\n\n\nsource\n\nmae\n\n mae\n      (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFra\n      me], models:List[str], id_col:str='unique_id', target_col:str='y')\n\nMean Absolute Error (MAE)\nMAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actual values and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\ndef pd_vs_pl(pd_df, pl_df, models):\n    np.testing.assert_allclose(\n        pd_df[models].to_numpy(),\n        pl_df.sort('unique_id').select(models).to_numpy(),\n    )\n\n\npd_vs_pl(\n    mae(series, models),\n    mae(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#mean-squared-error",
    "href": "losses.html#mean-squared-error",
    "title": "Losses",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\n\n\nsource\n\nmse\n\n mse\n      (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFra\n      me], models:List[str], id_col:str='unique_id', target_col:str='y')\n\nMean Squared Error (MSE)\nMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actual values and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    mse(series, models),\n    mse(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#root-mean-squared-error",
    "href": "losses.html#root-mean-squared-error",
    "title": "Losses",
    "section": "Root Mean Squared Error",
    "text": "Root Mean Squared Error\n\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\n\n\nsource\n\nrmse\n\n rmse\n       (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame], models:List[str], id_col:str='unique_id', target_col:str='y')\n\nRoot Mean Squared Error (RMSE)\nRMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actual values and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    rmse(series, models),\n    rmse(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#mean-absolute-percentage-error",
    "href": "losses.html#mean-absolute-percentage-error",
    "title": "Losses",
    "section": "Mean Absolute Percentage Error",
    "text": "Mean Absolute Percentage Error\n\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\n\n\n\nsource\n\nmape\n\n mape\n       (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame], models:List[str], id_col:str='unique_id', target_col:str='y')\n\nMean Absolute Percentage Error (MAPE)\nMAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actual values and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    mape(series, models),\n    mape(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#symmetric-mean-absolute-percentage-error",
    "href": "losses.html#symmetric-mean-absolute-percentage-error",
    "title": "Losses",
    "section": "Symmetric Mean Absolute Percentage Error",
    "text": "Symmetric Mean Absolute Percentage Error\n\n\\[ \\mathrm{SMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\n\n\nsource\n\nsmape\n\n smape\n        (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataF\n        rame], models:List[str], id_col:str='unique_id',\n        target_col:str='y')\n\nSymmetric Mean Absolute Percentage Error (SMAPE)\nSMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desireble compared to normal MAPE that may be undetermined when the target is zero.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actual values and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    smape(series, models),\n    smape(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#mean-absolute-scaled-error",
    "href": "losses.html#mean-absolute-scaled-error",
    "title": "Losses",
    "section": "Mean Absolute Scaled Error",
    "text": "Mean Absolute Scaled Error\n\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) =\n\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\n\n\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n  else: warn(msg)\n\nsource\n\nmase\n\n mase\n       (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame], models:List[str], seasonality:int, train_df:Union[pandas.core\n       .frame.DataFrame,polars.dataframe.frame.DataFrame],\n       id_col:str='unique_id', target_col:str='y')\n\nMean Absolute Scaled Error (MASE)\nMASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nseasonality\nint\n\nMain frequency of the time series;Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n\n\ntrain_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nTraining dataframe with id and actual values. Must be sorted by time.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    mase(series, models, 7, series),\n    mase(series_pl, models, 7, series_pl),\n    models,\n)"
  },
  {
    "objectID": "losses.html#relative-mean-absolute-error",
    "href": "losses.html#relative-mean-absolute-error",
    "title": "Losses",
    "section": "Relative Mean Absolute Error",
    "text": "Relative Mean Absolute Error\n\n\\[ \\mathrm{RMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})} \\]\n\n\n\nsource\n\nrmae\n\n rmae\n       (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame], models:List[str], baseline_models:List[str],\n       id_col:str='unique_id', target_col:str='y')\n\nRelative Mean Absolute Error (RMAE)\nCalculates the RAME between two sets of forecasts (from two different forecasting methods). A number smaller than one implies that the forecast in the numerator is better than the forecast in the denominator.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nbaseline_models\ntyping.List[str]\n\nColumns that identify the baseline models predictions.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    rmae(series, models, list(reversed(models))),\n    rmae(series_pl, models, list(reversed(models))),\n    [f'{m1}_div_{m2}' for m1, m2 in zip(models, reversed(models))],\n)"
  },
  {
    "objectID": "losses.html#quantile-loss",
    "href": "losses.html#quantile-loss",
    "title": "Losses",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) =\n\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n\\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+}\n+ q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\n\n\n\nsource\n\nquantile_loss\n\n quantile_loss\n                (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fra\n                me.DataFrame], models:List[str], q:float=0.5,\n                id_col:str='unique_id', target_col:str='y')\n\nQuantile Loss (QL)\nQL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation.\nA common value for q is 0.5 for the deviation from the median.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nq\nfloat\n0.5\nQuantile for the predictions’ comparison.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    quantile_loss(series, models),\n    quantile_loss(series_pl, models),\n    models,\n)"
  },
  {
    "objectID": "losses.html#multi-quantile-loss",
    "href": "losses.html#multi-quantile-loss",
    "title": "Losses",
    "section": "Multi-Quantile Loss",
    "text": "Multi-Quantile Loss\n\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},\n[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) =\n\\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\n\n\n\nsource\n\nmqloss\n\n mqloss\n         (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Data\n         Frame], models:List[str], quantiles:numpy.ndarray,\n         id_col:str='unique_id', target_col:str='y')\n\nMulti-Quantile loss (MQL)\nMQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nquantiles\nndarray\n\nQuantiles to compare against.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    mqloss(series, models, [0.1, 0.3]),\n    mqloss(series_pl, models, [0.1, 0.3]),\n    models,\n)"
  },
  {
    "objectID": "losses.html#coverage",
    "href": "losses.html#coverage",
    "title": "Losses",
    "section": "Coverage",
    "text": "Coverage\n\nsource\n\ncoverage\n\n coverage\n           (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Da\n           taFrame], models:List[str], level:int, id_col:str='unique_id',\n           target_col:str='y')\n\nCoverage of y with y_hat_lo and y_hat_hi.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nlevel\nint\n\nConfidence level used for intervals.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    coverage(series, models, 80),\n    coverage(series_pl, models, 80),\n    models,\n)"
  },
  {
    "objectID": "losses.html#calibration",
    "href": "losses.html#calibration",
    "title": "Losses",
    "section": "Calibration",
    "text": "Calibration\n\nsource\n\ncalibration\n\n calibration\n              (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame\n              .DataFrame], models:List[str], level:int,\n              id_col:str='unique_id', target_col:str='y')\n\nFraction of y that is lower than y_hat_hi.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nlevel\nint\n\nConfidence level used for intervals.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    calibration(series, models, 80),\n    calibration(series_pl, models, 80),\n    models,\n)"
  },
  {
    "objectID": "losses.html#crps",
    "href": "losses.html#crps",
    "title": "Losses",
    "section": "CRPS",
    "text": "CRPS\n\n\\[ \\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\nWhere \\(\\hat{F}_{\\tau}\\) is the an estimated multivariate distribution, and \\(y_{i,\\tau}\\) are its realizations.\n\n\nsource\n\nscaled_crps\n\n scaled_crps\n              (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame\n              .DataFrame], models:List[str], quantiles:numpy.ndarray,\n              id_col:str='unique_id', target_col:str='y')\n\nScaled Continues Ranked Probability Score\nCalculates a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y. This metric averages percentual weighted absolute deviations as defined by the quantile losses.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nInput dataframe with id, times, actuals and predictions.\n\n\nmodels\ntyping.List[str]\n\nColumns that identify the models predictions.\n\n\nquantiles\nndarray\n\nQuantiles to compare against.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\ndataframe with one row per id and one column per model.\n\n\n\n\npd_vs_pl(\n    scaled_crps(series, models, [0.3, 0.5]),\n    scaled_crps(series_pl, models, [0.3, 0.5]),\n    models,\n)"
  },
  {
    "objectID": "target_transforms.html",
    "href": "target_transforms.html",
    "title": "Target transforms",
    "section": "",
    "text": "source\n\nBaseTargetTransform\n\n BaseTargetTransform ()\n\nBase class used for target transformations.\n\nsource\n\n\nLocalStandardScaler\n\n LocalStandardScaler ()\n\nStandardizes each serie by subtracting its mean and dividing by its standard deviation.\n\nfrom utilsforecast.data import generate_series\n\n\nseries = generate_series(10, min_length=50, max_length=100)\ndata = series['y'].values\nsizes = series.groupby('unique_id', observed=True).size().values\nindptr = np.append(0, sizes.cumsum())\nga = GroupedArray(data, indptr)\n# introduce some constant series\nfor i, c in zip([0, 3], [10, 0]):\n    ga.data[ga.indptr[i] : ga.indptr[i + 1]] = c * np.ones(ga[i].size)\n\n\ndef test_transform(tfm, ga):\n    transformed = tfm.fit_transform(ga)\n    transformed2 = tfm.transform(ga)\n    np.testing.assert_allclose(transformed, transformed2)\n    transformed_ga = GroupedArray(transformed, ga.indptr)\n    np.testing.assert_allclose(\n        tfm.inverse_transform(transformed_ga),\n        ga.data,\n    )\n\n\ntest_transform(LocalStandardScaler(), ga)\n\n\nsource\n\n\nLocalMinMaxScaler\n\n LocalMinMaxScaler ()\n\nScales each serie to be in the [0, 1] interval.\n\ntest_transform(LocalMinMaxScaler(), ga)\n\n\nsource\n\n\nLocalRobustScaler\n\n LocalRobustScaler (scale:str='iqr')\n\nScaler robust to outliers.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscale\nstr\niqr\nStatistic to use for scaling. Can be either ‘iqr’ (Inter Quartile Range) or ‘mad’ (Median Asbolute Deviation)\n\n\n\n\nfor scale in ('iqr', 'mad'):\n    test_transform(LocalRobustScaler(scale=scale), ga)\n\n\nsource\n\n\nLocalBoxCox\n\n LocalBoxCox ()\n\nFinds optimum lambda for each serie and applies Box-Cox transformation.\n\ntest_transform(LocalBoxCox(), ga)\n\n\nsource\n\n\nGlobalFuncTransformer\n\n GlobalFuncTransformer (func:Callable, inverse_func:Callable)\n\nUses func and inverse_func for applying the same transformation to all series.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\ntyping.Callable\nFunction that transforms the data.\n\n\ninverse_func\ntyping.Callable\nFunction that inverse transforms the data.\n\n\n\n\ntest_transform(GlobalFuncTransformer(np.log1p, np.expm1), ga)\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "processing.html",
    "href": "processing.html",
    "title": "utilsforecast",
    "section": "",
    "text": "from fastcore.test import test_eq\nfrom nbdev import show_doc\n\nfrom utilsforecast.compat import POLARS_INSTALLED\nfrom utilsforecast.data import generate_series\n\n\nsource\n\nto_numpy\n\n to_numpy\n           (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Da\n           taFrame])\n\n\nsource\n\n\ncounts_by_id\n\n counts_by_id\n               (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fram\n               e.DataFrame], id_col:str)\n\n\nsource\n\n\nmaybe_compute_sort_indices\n\n maybe_compute_sort_indices\n                             (df:Union[pandas.core.frame.DataFrame,polars.\n                             dataframe.frame.DataFrame], id_col:str,\n                             time_col:str)\n\nCompute indices that would sort dataframe\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\nInput dataframe with id, times and target values.\n\n\nid_col\nstr\n\n\n\ntime_col\nstr\n\n\n\nReturns\ntyping.Optional[numpy.ndarray]\nArray with indices to sort the dataframe or None if it’s already sorted.\n\n\n\n\nsource\n\n\nassign_columns\n\n assign_columns\n                 (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fr\n                 ame.DataFrame], names:Union[str,List[str]], values:Union[\n                 numpy.ndarray,pandas.core.series.Series,polars.series.ser\n                 ies.Series])\n\n\nengines = ['pandas']\nif POLARS_INSTALLED:\n    engines.append('polars')\n\n\nfor engine in engines:\n    series = generate_series(2, engine=engine)\n    x = np.random.rand(series.shape[0])    \n    series = assign_columns(series, 'x', x)\n    series = assign_columns(series, ['y', 'z'], np.vstack([x, x]).T)\n    series = assign_columns(series, 'ones', 1)\n    series = assign_columns(series, 'zeros', np.zeros(series.shape[0]))\n    series = assign_columns(series, 'as', 'a')\n    np.testing.assert_allclose(\n        series[['x', 'y', 'z']],\n        np.vstack([x, x, x]).T\n    )\n    np.testing.assert_equal(series['ones'], np.ones(series.shape[0]))\n    np.testing.assert_equal(series['as'], np.full(series.shape[0], 'a'))\n\n\nsource\n\n\ntake_rows\n\n take_rows (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Da\n            taFrame,pandas.core.series.Series,polars.series.series.Series,\n            numpy.ndarray], idxs:numpy.ndarray)\n\n\nfor engine in engines:\n    series = generate_series(2, engine=engine)\n    subset = take_rows(series, np.array([0, 2]))\n    assert subset.shape[0] == 2\n\n\nsource\n\n\nfilter_with_mask\n\n filter_with_mask (df:Union[pandas.core.series.Series,polars.series.series\n                   .Series,pandas.core.frame.DataFrame,polars.dataframe.fr\n                   ame.DataFrame,pandas.core.indexes.base.Index,numpy.ndar\n                   ray], mask:Union[numpy.ndarray,pandas.core.series.Serie\n                   s,polars.series.series.Series])\n\n\nsource\n\n\nis_nan\n\n is_nan (s:Union[pandas.core.series.Series,polars.series.series.Series])\n\n\nnp.testing.assert_equal(\n    is_nan(pd.Series([np.nan, 1.0, None])).to_numpy(),\n    np.array([True, False, True]),\n)\nif POLARS_INSTALLED:\n    np.testing.assert_equal(\n        is_nan(pl.Series([np.nan, 1.0, None])).to_numpy(),\n        np.array([True, False, None]),\n    )\n\n\nsource\n\n\nis_none\n\n is_none (s:Union[pandas.core.series.Series,polars.series.series.Series])\n\n\nnp.testing.assert_equal(\n    is_none(pd.Series([np.nan, 1.0, None])).to_numpy(),\n    np.array([True, False, True]),\n)\nif POLARS_INSTALLED:\n    np.testing.assert_equal(\n        is_none(pl.Series([np.nan, 1.0, None])).to_numpy(),\n        np.array([False, False, True]),\n    )\n\n\nsource\n\n\nis_nan_or_none\n\n is_nan_or_none\n                 (s:Union[pandas.core.series.Series,polars.series.series.S\n                 eries])\n\n\nnp.testing.assert_equal(\n    is_nan_or_none(pd.Series([np.nan, 1.0, None])).to_numpy(),\n    np.array([True, False, True]),\n)\nif POLARS_INSTALLED:\n    np.testing.assert_equal(\n        is_nan_or_none(pl.Series([np.nan, 1.0, None])).to_numpy(),\n        np.array([True, False, True]),\n    )\n\n\nsource\n\n\nmatch_if_categorical\n\n match_if_categorical (s1:Union[pandas.core.series.Series,polars.series.se\n                       ries.Series,pandas.core.indexes.base.Index], s2:Uni\n                       on[pandas.core.series.Series,polars.series.series.S\n                       eries])\n\n\nsource\n\n\nvertical_concat\n\n vertical_concat (dfs:List[Union[pandas.core.frame.DataFrame,polars.datafr\n                  ame.frame.DataFrame]], match_categories:bool=True)\n\n\ndf1 = pd.DataFrame({'x': ['a', 'b', 'c']}, dtype='category')\ndf2 = pd.DataFrame({'x': ['f', 'b', 'a']}, dtype='category')\npd.testing.assert_series_equal(\n    vertical_concat([df1,df2])['x'],\n    pd.Series(['a', 'b', 'c', 'f', 'b', 'a'], name='x', dtype=pd.CategoricalDtype(categories=['a', 'b', 'c', 'f']))\n)\n\n\ndf1 = pl.DataFrame({'x': ['a', 'b', 'c']}, schema={'x': pl.Categorical})\ndf2 = pl.DataFrame({'x': ['f', 'b', 'a']}, schema={'x': pl.Categorical})\nout = vertical_concat([df1,df2])['x']\nassert out.series_equal(pl.Series('x', ['a', 'b', 'c', 'f', 'b', 'a']))\nassert out.to_physical().series_equal(pl.Series('x', [0, 1, 2, 3, 1, 0]))\nassert out.cat.get_categories().series_equal(\n    pl.Series('x', ['a', 'b', 'c', 'f'])\n)\n\n\nfor engine in engines:\n    series = generate_series(2, engine=engine)\n    doubled = vertical_concat([series, series])\n    assert doubled.shape[0] == 2 * series.shape[0]\n\n\nsource\n\n\nhorizontal_concat\n\n horizontal_concat (dfs:List[Union[pandas.core.frame.DataFrame,polars.data\n                    frame.frame.DataFrame]])\n\n\nfor engine in engines:\n    series = generate_series(2, engine=engine)\n    renamer = {c: f'{c}_2' for c in series.columns}\n    if engine == 'pandas':\n        series2 = series.rename(columns=renamer)\n    else:\n        series2 = series.rename(renamer)\n    doubled = horizontal_concat([series, series2])\n    assert doubled.shape[1] == 2 * series.shape[1]\n\n\nsource\n\n\ncopy_if_pandas\n\n copy_if_pandas\n                 (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fr\n                 ame.DataFrame], deep:bool=False)\n\n\nsource\n\n\njoin\n\n join (df1:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame,pandas.core.series.Series,polars.series.series.Series], df2:Uni\n       on[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFrame,pan\n       das.core.series.Series,polars.series.series.Series],\n       on:Union[str,List[str]], how:str='inner')\n\n\nsource\n\n\ndrop_index_if_pandas\n\n drop_index_if_pandas\n                       (df:Union[pandas.core.frame.DataFrame,polars.datafr\n                       ame.frame.DataFrame])\n\n\nsource\n\n\nrename\n\n rename\n         (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Data\n         Frame], mapping:Dict[str,str])\n\n\nsource\n\n\nsort\n\n sort\n       (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n       ame], by:Union[str,List[str],NoneType]=None)\n\n\npd.testing.assert_frame_equal(\n    sort(pd.DataFrame({'x': [3, 1, 2]}), 'x'),\n    pd.DataFrame({'x': [1, 2, 3]})\n)\npd.testing.assert_frame_equal(\n    sort(pd.DataFrame({'x': [3, 1, 2]}), ['x']),\n    pd.DataFrame({'x': [1, 2, 3]})\n)\npd.testing.assert_series_equal(\n    sort(pd.Series([3, 1, 2])),\n    pd.Series([1, 2, 3])\n)\npd.testing.assert_index_equal(\n    sort(pd.Index([3, 1, 2])),\n    pd.Index([1, 2, 3])\n)\n\n\n# TODO: replace with pl.testing.assert_frame_equal when it's released\npd.testing.assert_frame_equal(\n    sort(pl.DataFrame({'x': [3, 1, 2]}), 'x').to_pandas(),\n    pd.DataFrame({'x': [1, 2, 3]}),\n)\npd.testing.assert_frame_equal(\n    sort(pl.DataFrame({'x': [3, 1, 2]}), ['x']).to_pandas(),\n    pd.DataFrame({'x': [1, 2, 3]}),\n)\npd.testing.assert_series_equal(\n    sort(pl.Series('x', [3, 1, 2])).to_pandas(),\n    pd.Series([1, 2, 3], name='x')\n)\n\n\nsource\n\n\noffset_dates\n\n offset_dates\n               (dates:Union[pandas.core.indexes.base.Index,polars.series.s\n               eries.Series],\n               freq:Union[int,str,pandas._libs.tslibs.offsets.BaseOffset],\n               n:int)\n\n\nsource\n\n\ngroup_by\n\n group_by (df:Union[pandas.core.series.Series,polars.series.series.Series,\n           pandas.core.frame.DataFrame,polars.dataframe.frame.DataFrame],\n           by, maintain_order=False)\n\n\nsource\n\n\ngroup_by_agg\n\n group_by_agg\n               (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fram\n               e.DataFrame], by, aggs, maintain_order=False)\n\n\npd.testing.assert_frame_equal(\n    group_by_agg(pd.DataFrame({'x': [1, 1, 2], 'y': [1, 1, 1]}), 'x', {'y': 'sum'}),\n    pd.DataFrame({'x': [1, 2], 'y': [2, 1]})\n)\n\n\npd.testing.assert_frame_equal(\n    group_by_agg(pl.DataFrame({'x': [1, 1, 2], 'y': [1, 1, 1]}), 'x', {'y': 'sum'}, maintain_order=True).to_pandas(),\n    pd.DataFrame({'x': [1, 2], 'y': [2, 1]})\n)\n\n\nsource\n\n\nis_in\n\n is_in (s:Union[pandas.core.series.Series,polars.series.series.Series],\n        collection)\n\n\nnp.testing.assert_equal(is_in(pd.Series([1, 2, 3]), [1]), np.array([True, False, False]))\n\n\nnp.testing.assert_equal(is_in(pl.Series([1, 2, 3]), [1]), np.array([True, False, False]))\n\n\nsource\n\n\nbetween\n\n between (s:Union[pandas.core.series.Series,polars.series.series.Series],\n          lower:Union[pandas.core.series.Series,polars.series.series.Serie\n          s], upper:Union[pandas.core.series.Series,polars.series.series.S\n          eries])\n\n\nnp.testing.assert_equal(\n    between(pd.Series([1, 2, 3]), pd.Series([0, 1, 4]), pd.Series([4, 1, 2])),\n    np.array([True, False, False]),\n)\n\n\nnp.testing.assert_equal(\n    between(pl.Series([1, 2, 3]), pl.Series([0, 1, 4]), pl.Series([4, 1, 2])),\n    np.array([True, False, False]),\n)\n\n\nsource\n\n\nfill_null\n\n fill_null\n            (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.D\n            ataFrame], mapping:Dict[str,Any])\n\n\npd.testing.assert_frame_equal(\n    fill_null(pd.DataFrame({'x': [1, np.nan], 'y': [np.nan, 2]}), {'x': 2, 'y': 1}),\n    pd.DataFrame({'x': [1, 2], 'y': [1, 2]}, dtype='float64')\n)\n\n\n# TODO: replace with pl.testing.assert_frame_equal when it's released\npd.testing.assert_frame_equal(\n    fill_null(pl.DataFrame({'x': [1, None], 'y': [None, 2]}), {'x': 2, 'y': 1}).to_pandas(),\n    pd.DataFrame({'x': [1, 2], 'y': [1, 2]})\n)\n\n\nsource\n\n\ncast\n\n cast (s:Union[pandas.core.series.Series,polars.series.series.Series],\n       dtype:type)\n\n\npd.testing.assert_series_equal(\n    cast(pd.Series([1, 2, 3]), 'int16'),\n    pd.Series([1, 2, 3], dtype='int16')\n)\n\n\npd.testing.assert_series_equal(\n    cast(pl.Series('x', [1, 2, 3]), pl.Int16).to_pandas(),\n    pd.Series([1, 2, 3], name='x', dtype='int16')\n)\n\n\nsource\n\n\nvalue_cols_to_numpy\n\n value_cols_to_numpy\n                      (df:Union[pandas.core.frame.DataFrame,polars.datafra\n                      me.frame.DataFrame], id_col:str, time_col:str,\n                      target_col:str)\n\n\nsource\n\n\nprocess_df\n\n process_df\n             (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.\n             DataFrame], id_col:str, time_col:str, target_col:str)\n\nExtract components from dataframe\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\nInput dataframe with id, times and target values.\n\n\nid_col\nstr\n\n\n\ntime_col\nstr\n\n\n\ntarget_col\nstr\n\n\n\nReturns\ntyping.Tuple[typing.Union[pandas.core.series.Series, polars.series.series.Series], numpy.ndarray, numpy.ndarray, numpy.ndarray, typing.Optional[numpy.ndarray]]\nserie with the sorted unique ids present in the data.\n\n\n\n\nsource\n\n\nDataFrameProcessor\n\n DataFrameProcessor (id_col:str='unique_id', time_col:str='ds',\n                     target_col:str='y')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nstatic_features = ['static_0', 'static_1']\n\n\nfor n_static_features in [0, 2]:\n    series_pd = generate_series(1_000, n_static_features=n_static_features, equal_ends=False, engine='pandas')\n    for i in range(n_static_features):\n        series_pd[f'static_{i}'] = series_pd[f'static_{i}'].map(lambda x: f'x_{x}').astype('category')\n    scrambled_series_pd = series_pd.sample(frac=1.0)\n    dfp = DataFrameProcessor('unique_id', 'ds', 'y')\n    uids, times, data, indptr, _ = dfp.process(scrambled_series_pd)\n    test_eq(times, series_pd.groupby('unique_id', observed=True)['ds'].max().values)\n    test_eq(uids, np.sort(series_pd['unique_id'].unique()))\n    for i in range(n_static_features):\n        series_pd[f'static_{i}'] = series_pd[f'static_{i}'].cat.codes\n    test_eq(data, series_pd[['y'] + static_features[:n_static_features]].to_numpy())\n    test_eq(np.diff(indptr), series_pd.groupby('unique_id', observed=True).size().values)\n\n\nfor n_static_features in [0, 2]:\n    series_pl = generate_series(1_000, n_static_features=n_static_features, equal_ends=False, engine='polars')\n    scrambled_series_pl = series_pl.sample(fraction=1.0, shuffle=True)\n    dfp = DataFrameProcessor('unique_id', 'ds', 'y')\n    uids, times, data, indptr, _ = dfp.process(scrambled_series_pl)\n    grouped = group_by(series_pl, 'unique_id')\n    test_eq(times, grouped.agg(pl.col('ds').max()).sort('unique_id')['ds'].to_numpy())\n    test_eq(uids, series_pl['unique_id'].unique().sort())\n    test_eq(data, series_pl.select(pl.col(c).map_batches(lambda s: s.to_physical()) for c in ['y'] + static_features[:n_static_features]).to_numpy())\n    test_eq(np.diff(indptr), grouped.count().sort('unique_id')['count'].to_numpy())\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "compat.html",
    "href": "compat.html",
    "title": "utilsforecast",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "source\n\nevaluate\n\n evaluate\n           (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.Da\n           taFrame], metrics:List[Callable],\n           models:Optional[List[str]]=None, train_df:Union[pandas.core.fra\n           me.DataFrame,polars.dataframe.frame.DataFrame,NoneType]=None,\n           id_col:str='unique_id', time_col:str='ds', target_col:str='y')\n\nEvaluate forecast using different metrics.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nForecasts to evaluate.Must have id_col, time_col, target_col and models’ predictions.\n\n\nmetrics\ntyping.List[typing.Callable]\n\nFunctions with arguments df, models, id_col, target_col and optionally train_df.\n\n\nmodels\ntyping.Optional[typing.List[str]]\nNone\nNames of the models to evaluate.If None will use every column in the dataframe after removing id, time and target.\n\n\ntrain_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nTraining set. Used to evaluate metrics such as mase.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nMetrics with one row per (id, metric) combination and one column per model.\n\n\n\n\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\n\nfrom utilsforecast.losses import *\nfrom utilsforecast.data import generate_series\n\n\nseries = generate_series(10, n_models=2, level=[80])\n\n\nmodels = ['model0', 'model1']\nmetrics = [\n    mae,\n    mse,\n    rmse,\n    mape,\n    smape,\n    partial(mase, seasonality=7),\n    partial(rmae, baseline_models=list(reversed(models))),\n    quantile_loss,\n    partial(mqloss, quantiles=np.array([0.3, 0.5])),\n    partial(coverage, level=80),\n    partial(calibration, level=80),\n    partial(scaled_crps, quantiles=np.array([0.3, 0.5])),\n]\n\n\nevaluation = evaluate(\n    series,\n    metrics=metrics,\n    models=models,\n    train_df=series,\n)\nevaluation\n\n\n\n\n\n\n\n\nunique_id\nmetric\nmodel0\nmodel1\nmodel0_div_model1\nmodel1_div_model0\n\n\n\n\n0\n0\nmae\n0.158108\n0.163246\nNaN\nNaN\n\n\n1\n1\nmae\n0.160109\n0.143805\nNaN\nNaN\n\n\n2\n2\nmae\n0.159815\n0.170510\nNaN\nNaN\n\n\n3\n3\nmae\n0.168537\n0.161595\nNaN\nNaN\n\n\n4\n4\nmae\n0.170182\n0.163329\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n115\n5\nscaled_crps\n7.564046\n7.580635\nNaN\nNaN\n\n\n116\n6\nscaled_crps\n6.063454\n5.748035\nNaN\nNaN\n\n\n117\n7\nscaled_crps\n9.691468\n10.132641\nNaN\nNaN\n\n\n118\n8\nscaled_crps\n1.481827\n1.559418\nNaN\nNaN\n\n\n119\n9\nscaled_crps\n6.278162\n6.563539\nNaN\nNaN\n\n\n\n\n120 rows × 6 columns\n\n\n\n\nsummary = evaluation.drop(columns='unique_id').groupby('metric').mean().reset_index()\nsummary\n\n\n\n\n\n\n\n\nmetric\nmodel0\nmodel1\nmodel0_div_model1\nmodel1_div_model0\n\n\n\n\n0\ncalibration\n0.833993\n0.815833\nNaN\nNaN\n\n\n1\ncoverage\n0.833993\n0.815833\nNaN\nNaN\n\n\n2\nmae\n0.161286\n0.162281\nNaN\nNaN\n\n\n3\nmape\n0.048894\n0.049624\nNaN\nNaN\n\n\n4\nmase\n0.966846\n0.975354\nNaN\nNaN\n\n\n5\nmqloss\n0.080764\n0.079965\nNaN\nNaN\n\n\n6\nmse\n0.048653\n0.049198\nNaN\nNaN\n\n\n7\nquantile_loss\n0.080643\n0.081141\nNaN\nNaN\n\n\n8\nrmae\nNaN\nNaN\n0.996604\n1.007927\n\n\n9\nrmse\n0.220357\n0.221543\nNaN\nNaN\n\n\n10\nscaled_crps\n5.917607\n5.887435\nNaN\nNaN\n\n\n11\nsmape\n0.024475\n0.024902\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "source\n\nplot_series\n\n plot_series\n              (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame\n              .DataFrame], forecasts_df:Union[pandas.core.frame.DataFrame,\n              polars.dataframe.frame.DataFrame,NoneType]=None,\n              ids:Optional[List[str]]=None, plot_random:bool=True,\n              max_ids:int=8, models:Optional[List[str]]=None,\n              level:Optional[List[float]]=None,\n              max_insample_length:Optional[int]=None,\n              plot_anomalies:bool=False, engine:str='matplotlib',\n              palette:str='viridis', id_col:str='unique_id',\n              time_col:str='ds', target_col:str='y', seed:int=0,\n              resampler_kwargs:Optional[Dict]=None)\n\nPlot forecasts and insample values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nDataFrame with columns [id_col, time_col, target_col].\n\n\nforecasts_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [id_col, time_col] and models.\n\n\nids\ntyping.Optional[typing.List[str]]\nNone\nTime Series to plot.If None, time series are selected randomly.\n\n\nplot_random\nbool\nTrue\nSelect time series to plot randomly.\n\n\nmax_ids\nint\n8\nMaximum number of ids to plot.\n\n\nmodels\ntyping.Optional[typing.List[str]]\nNone\nModels to plot.\n\n\nlevel\ntyping.Optional[typing.List[float]]\nNone\nPrediction intervals to plot.\n\n\nmax_insample_length\ntyping.Optional[int]\nNone\nMaximum number of train/insample observations to be plotted.\n\n\nplot_anomalies\nbool\nFalse\nPlot anomalies for each prediction interval.\n\n\nengine\nstr\nmatplotlib\nLibrary used to plot. ‘plotly’, ‘plotly-resampler’ or ‘matplotlib’.\n\n\npalette\nstr\nviridis\nName of the matplotlib colormap to use.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nseed\nint\n0\nSeed used for the random number generator. Only used if plot_random is True.\n\n\nresampler_kwargs\ntyping.Optional[typing.Dict]\nNone\nKeyword arguments to be passed to plotly-resampler constructor.For further custumization (“show_dash”) call the method,store the plotting object and add the extra arguments toits show_dash method.\n\n\nReturns\nmatplotlib or plotly figure\n\nPlot’s figure\n\n\n\n\nfrom utilsforecast.data import generate_series\n\n\nlevel = [80, 95]\nseries = generate_series(4, freq='D', equal_ends=True, with_trend=True, n_models=2, level=level)\ntest_pd = series.groupby('unique_id', observed=True).tail(10).copy()\ntrain_pd = series.drop(test_pd.index)\n\n\nfig = plot_series(\n    train_pd,\n    forecasts_df=test_pd,\n    ids=[0, 3],\n    plot_random=False,\n    level=level,    \n    max_insample_length=50,\n    engine='matplotlib',\n    plot_anomalies=True,\n)\nfig.savefig('imgs/plotting.png', bbox_inches='tight')\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "source\n\nfill_gaps\n\n fill_gaps (df:pandas.core.frame.DataFrame, freq:Union[str,int],\n            start:Union[str,int]='per_serie', end:Union[str,int]='global',\n            id_col:str='unique_id', time_col:str='ds')\n\nEnforce start and end datetimes for dataframe.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nInput data\n\n\nfreq\ntyping.Union[str, int]\n\nSeries’ frequency\n\n\nstart\ntyping.Union[str, int]\nper_serie\nInitial timestamp for the series. * ‘per_serie’ uses each serie’s first timestamp * ‘global’ uses the first timestamp seen in the data * Can also be a specific timestamp or integer, e.g. ‘2000-01-01’ or 2000\n\n\nend\ntyping.Union[str, int]\nglobal\nInitial timestamp for the series. * ‘per_serie’ uses each serie’s last timestamp * ‘global’ uses the last timestamp seen in the data * Can also be a specific timestamp or integer, e.g. ‘2000-01-01’ or 2000\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestamp.\n\n\nReturns\nDataFrame\n\nDataframe with gaps filled.\n\n\n\n\ndf = pd.DataFrame(\n    {\n        'unique_id': [0, 0, 0, 1, 1],\n        'ds': pd.to_datetime(['2020', '2021', '2023', '2021', '2022']),\n        'y': np.arange(5),\n    }\n)\ndf\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020-01-01\n0\n\n\n1\n0\n2021-01-01\n1\n\n\n2\n0\n2023-01-01\n2\n\n\n3\n1\n2021-01-01\n3\n\n\n4\n1\n2022-01-01\n4\n\n\n\n\n\n\n\nThe default functionality is taking the current starts and only extending the end date to be the same for all series.\n\nfill_gaps(\n    df,\n    freq='YS',\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020-01-01\n0.0\n\n\n1\n0\n2021-01-01\n1.0\n\n\n2\n0\n2022-01-01\nNaN\n\n\n3\n0\n2023-01-01\n2.0\n\n\n4\n1\n2021-01-01\n3.0\n\n\n5\n1\n2022-01-01\n4.0\n\n\n6\n1\n2023-01-01\nNaN\n\n\n\n\n\n\n\nWe can also specify end='per_serie' to only fill possible gaps within each serie.\n\nfill_gaps(\n    df,\n    freq='YS',\n    end='per_serie',\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020-01-01\n0.0\n\n\n1\n0\n2021-01-01\n1.0\n\n\n2\n0\n2022-01-01\nNaN\n\n\n3\n0\n2023-01-01\n2.0\n\n\n4\n1\n2021-01-01\n3.0\n\n\n5\n1\n2022-01-01\n4.0\n\n\n\n\n\n\n\nWe can also specify an end date in the future.\n\nfill_gaps(\n    df,\n    freq='YS',\n    end='2024',\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020-01-01\n0.0\n\n\n1\n0\n2021-01-01\n1.0\n\n\n2\n0\n2022-01-01\nNaN\n\n\n3\n0\n2023-01-01\n2.0\n\n\n4\n0\n2024-01-01\nNaN\n\n\n5\n1\n2021-01-01\n3.0\n\n\n6\n1\n2022-01-01\n4.0\n\n\n7\n1\n2023-01-01\nNaN\n\n\n8\n1\n2024-01-01\nNaN\n\n\n\n\n\n\n\nWe can set all series to start at the same time.\n\nfill_gaps(\n    df,\n    freq='YS',\n    start='global'\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020-01-01\n0.0\n\n\n1\n0\n2021-01-01\n1.0\n\n\n2\n0\n2022-01-01\nNaN\n\n\n3\n0\n2023-01-01\n2.0\n\n\n4\n1\n2020-01-01\nNaN\n\n\n5\n1\n2021-01-01\n3.0\n\n\n6\n1\n2022-01-01\n4.0\n\n\n7\n1\n2023-01-01\nNaN\n\n\n\n\n\n\n\nWe can also set a common start date for all series (which can be earlier than their current starts).\n\nfill_gaps(\n    df,\n    freq='YS',\n    start='2019',\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2019-01-01\nNaN\n\n\n1\n0\n2020-01-01\n0.0\n\n\n2\n0\n2021-01-01\n1.0\n\n\n3\n0\n2022-01-01\nNaN\n\n\n4\n0\n2023-01-01\n2.0\n\n\n5\n1\n2019-01-01\nNaN\n\n\n6\n1\n2020-01-01\nNaN\n\n\n7\n1\n2021-01-01\n3.0\n\n\n8\n1\n2022-01-01\n4.0\n\n\n9\n1\n2023-01-01\nNaN\n\n\n\n\n\n\n\nIn case the times are integers the frequency, start and end must also be integers.\n\ndf = pd.DataFrame(\n    {\n        'unique_id': [0, 0, 0, 1, 1],\n        'ds': [2020, 2021, 2023, 2021, 2022],\n        'y': np.arange(5),\n    }\n)\ndf\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2020\n0\n\n\n1\n0\n2021\n1\n\n\n2\n0\n2023\n2\n\n\n3\n1\n2021\n3\n\n\n4\n1\n2022\n4\n\n\n\n\n\n\n\n\nfill_gaps(\n    df,\n    freq=1,\n    start=2019,\n    end=2024,\n)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2019\nNaN\n\n\n1\n0\n2020\n0.0\n\n\n2\n0\n2021\n1.0\n\n\n3\n0\n2022\nNaN\n\n\n4\n0\n2023\n2.0\n\n\n5\n0\n2024\nNaN\n\n\n6\n1\n2019\nNaN\n\n\n7\n1\n2020\nNaN\n\n\n8\n1\n2021\n3.0\n\n\n9\n1\n2022\n4.0\n\n\n10\n1\n2023\nNaN\n\n\n11\n1\n2024\nNaN\n\n\n\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "grouped_array.html",
    "href": "grouped_array.html",
    "title": "utilsforecast",
    "section": "",
    "text": "# test _append_one\ndata = np.arange(5).reshape(-1, 1)\nindptr = np.array([0, 2, 5])\nnew = np.array([7, 8])\nnew_data, new_indptr = _append_one(data, indptr, new)\nnp.testing.assert_equal(\n    new_data,\n    np.array([0, 1, 7, 2, 3, 4, 8]).reshape(-1, 1),\n)\nnp.testing.assert_equal(\n    new_indptr,\n    np.array([0, 3, 7]),\n)\n\n\n# test append several\ndata = np.arange(5).reshape(-1, 1)\nindptr = np.array([0, 2, 5])\nnew_sizes = np.array([0, 2, 1])\nnew_values = np.array([6, 7, 5]).reshape(-1, 1)\nnew_groups = np.array([False, True, False])\nnew_data, new_indptr = _append_several(data, indptr, new_sizes, new_values, new_groups)\nnp.testing.assert_equal(\n    new_data,\n    np.array([0, 1, 6, 7, 2, 3, 4, 5]).reshape(-1, 1),\n)\nnp.testing.assert_equal(\n    new_indptr,\n    np.array([0, 2, 4, 8]),\n)\n\n\nsource\n\nGroupedArray\n\n GroupedArray (data:numpy.ndarray, indptr:numpy.ndarray)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nfrom fastcore.test import test_eq, test_fail\n\nfrom utilsforecast.data import generate_series\n\n\n# The `GroupedArray` is used internally for storing the series values and performing transformations.\ndata = np.arange(20, dtype=np.float32).reshape(-1, 2)\nindptr = np.array([0, 2, 10])  # group 1: [0, 1], group 2: [2..9]\nga = GroupedArray(data, indptr)\ntest_eq(len(ga), 2)\n\n\n# Iterate through the groups\nga_iter = iter(ga)\nnp.testing.assert_equal(next(ga_iter), np.arange(4).reshape(-1, 2))\nnp.testing.assert_equal(next(ga_iter), np.arange(4, 20).reshape(-1, 2))\n\n\n# Take the last two observations from each group\nlast_2 = ga.take_from_groups(slice(-2, None))\nnp.testing.assert_equal(\n    last_2.data,\n    np.vstack([\n        np.arange(4).reshape(-1, 2),\n        np.arange(16, 20).reshape(-1, 2),\n    ]),\n)\nnp.testing.assert_equal(last_2.indptr, np.array([0, 2, 4]))\n\n# Take the second observation from each group\nsecond = ga.take_from_groups(1)\nnp.testing.assert_equal(second.data, np.array([[2, 3], [6, 7]]))\nnp.testing.assert_equal(second.indptr, np.array([0, 1, 2]))\n\n\n# Take the last four observations from every group. Note that since group 1 only has two elements, only these are returned.\nlast_4 = ga.take_from_groups(slice(-4, None))\nnp.testing.assert_equal(\n    last_4.data,\n    np.vstack([\n        np.arange(4).reshape(-1, 2),\n        np.arange(12, 20).reshape(-1, 2),\n    ]),\n)\nnp.testing.assert_equal(last_4.indptr, np.array([0, 2, 6]))\n\n\n# Select a specific subset of groups\nindptr = np.array([0, 2, 4, 7, 10])\nga2 = GroupedArray(data, indptr)\nsubset = ga2.take([0, 2])\nnp.testing.assert_allclose(subset[0].data, ga2[0].data)\nnp.testing.assert_allclose(subset[1].data, ga2[2].data)\n\n\n# try to append new values that don't match the number of groups\ntest_fail(lambda: ga.append(np.array([1., 2., 3.])), contains='new must have 2 rows')\n\n\n# build from df\nseries_pd = generate_series(10, static_as_categorical=False, engine='pandas')\nga_pd = GroupedArray.from_sorted_df(series_pd, 'unique_id', 'ds', 'y')\nseries_pl = generate_series(10, static_as_categorical=False, engine='polars')\nga_pl = GroupedArray.from_sorted_df(series_pl, 'unique_id', 'ds', 'y')\nnp.testing.assert_allclose(ga_pd.data, ga_pl.data)\nnp.testing.assert_equal(ga_pd.indptr, ga_pl.indptr)\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "utilsforecast",
    "section": "",
    "text": "pip install utilsforecast\n\n\n\nconda install -c conda-forge utilsforecast\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "utilsforecast",
    "section": "",
    "text": "pip install utilsforecast\n\n\n\nconda install -c conda-forge utilsforecast"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "utilsforecast",
    "section": "How to use",
    "text": "How to use\n\nGenerate synthetic data\n\nfrom utilsforecast.data import generate_series\n\n\nseries = generate_series(3, with_trend=True, static_as_categorical=False)\nseries\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2000-01-01\n0.422133\n\n\n1\n0\n2000-01-02\n1.501407\n\n\n2\n0\n2000-01-03\n2.568495\n\n\n3\n0\n2000-01-04\n3.529085\n\n\n4\n0\n2000-01-05\n4.481929\n\n\n...\n...\n...\n...\n\n\n481\n2\n2000-06-11\n163.914625\n\n\n482\n2\n2000-06-12\n166.018479\n\n\n483\n2\n2000-06-13\n160.839176\n\n\n484\n2\n2000-06-14\n162.679603\n\n\n485\n2\n2000-06-15\n165.089288\n\n\n\n\n486 rows × 3 columns\n\n\n\n\n\nPlotting\n\nfrom utilsforecast.plotting import plot_series\n\n\nfig = plot_series(series, plot_random=False, max_insample_length=50, engine='matplotlib')\nfig.savefig('imgs/index.png', bbox_inches='tight')\n\n\n\n\nPreprocessing\n\nfrom utilsforecast.preprocessing import fill_gaps\n\n\nserie = series[series['unique_id'].eq(0)].tail(10)\n# drop some points\nwith_gaps = serie.sample(frac=0.5, random_state=0).sort_values('ds')\nwith_gaps\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n213\n0\n2000-08-01\n18.543147\n\n\n214\n0\n2000-08-02\n19.941764\n\n\n216\n0\n2000-08-04\n21.968733\n\n\n220\n0\n2000-08-08\n19.091509\n\n\n221\n0\n2000-08-09\n20.220739\n\n\n\n\n\n\n\n\nfill_gaps(with_gaps, freq='D')\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\n0\n2000-08-01\n18.543147\n\n\n1\n0\n2000-08-02\n19.941764\n\n\n2\n0\n2000-08-03\nNaN\n\n\n3\n0\n2000-08-04\n21.968733\n\n\n4\n0\n2000-08-05\nNaN\n\n\n5\n0\n2000-08-06\nNaN\n\n\n6\n0\n2000-08-07\nNaN\n\n\n7\n0\n2000-08-08\n19.091509\n\n\n8\n0\n2000-08-09\n20.220739\n\n\n\n\n\n\n\n\n\nEvaluating\n\nfrom functools import partial\n\nimport numpy as np\n\nfrom utilsforecast.evaluation import evaluate\nfrom utilsforecast.losses import mape, mase\n\n\nvalid = series.groupby('unique_id').tail(7).copy()\ntrain = series.drop(valid.index)\nrng = np.random.RandomState(0)\nvalid['seas_naive'] = train.groupby('unique_id')['y'].tail(7).values\nvalid['rand_model'] = valid['y'] * rng.rand(valid['y'].shape[0])\ndaily_mase = partial(mase, seasonality=7)\nevaluate(valid, metrics=[mape, daily_mase], train_df=train)\n\n\n\n\n\n\n\n\nunique_id\nmetric\nseas_naive\nrand_model\n\n\n\n\n0\n0\nmape\n0.024139\n0.440173\n\n\n1\n1\nmape\n0.054259\n0.278123\n\n\n2\n2\nmape\n0.042642\n0.480316\n\n\n3\n0\nmase\n0.907149\n16.418014\n\n\n4\n1\nmase\n0.991635\n6.404254\n\n\n5\n2\nmase\n1.013596\n11.365040"
  }
]