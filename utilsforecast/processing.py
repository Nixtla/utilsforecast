# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/processing.ipynb.

# %% auto 0
__all__ = ['to_numpy', 'counts_by_id', 'maybe_compute_sort_indices', 'assign_columns', 'take_rows', 'filter_with_mask', 'is_nan',
           'is_none', 'is_nan_or_none', 'match_if_categorical', 'vertical_concat', 'horizontal_concat',
           'copy_if_pandas', 'join', 'drop_index_if_pandas', 'rename', 'sort', 'offset_dates', 'group_by',
           'group_by_agg', 'is_in', 'between', 'fill_null', 'cast', 'value_cols_to_numpy', 'process_df',
           'DataFrameProcessor', 'backtest_splits']

# %% ../nbs/processing.ipynb 2
import re
import reprlib
import warnings
from typing import Any, Dict, Generator, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from pandas.tseries.offsets import BaseOffset

from .compat import DataFrame, Series, pl, pl_DataFrame, pl_Series
from .validation import validate_format

# %% ../nbs/processing.ipynb 4
def _polars_categorical_to_numerical(serie: pl_Series) -> pl_Series:
    if serie.dtype == pl.Categorical:
        serie = serie.to_physical()
    return serie

# %% ../nbs/processing.ipynb 5
def to_numpy(df: DataFrame) -> np.ndarray:
    if isinstance(df, pd.DataFrame):
        cat_cols = [
            c
            for c, dtype in df.dtypes.items()
            if isinstance(dtype, pd.CategoricalDtype)
        ]
        if cat_cols:
            df = df.copy(deep=False)
            for col in cat_cols:
                df[col] = df[col].cat.codes
        df = df.to_numpy()
    else:
        try:
            expr = pl.all().map_batches(_polars_categorical_to_numerical)
        except AttributeError:
            expr = pl.all().map(_polars_categorical_to_numerical)
        df = df.select(expr).to_numpy(order="c")
    return df

# %% ../nbs/processing.ipynb 6
def counts_by_id(df: DataFrame, id_col: str) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        id_counts = df.groupby(id_col, observed=True).size()
        if not id_counts.index.is_monotonic_increasing:
            id_counts = id_counts.sort_index()
        id_counts = id_counts.reset_index()
        id_counts.columns = [id_col, "counts"]
    else:
        id_counts = df[id_col].value_counts().sort(id_col)
    return id_counts

# %% ../nbs/processing.ipynb 7
def maybe_compute_sort_indices(
    df: DataFrame, id_col: str, time_col: str
) -> Optional[np.ndarray]:
    """Compute indices that would sort dataframe

    Parameters
    ----------
    df : pandas or polars DataFrame
        Input dataframe with id, times and target values.

    Returns
    -------
    numpy array or None
        Array with indices to sort the dataframe or None if it's already sorted.
    """
    if isinstance(df, pd.DataFrame):
        idx = pd.MultiIndex.from_frame(df[[id_col, time_col]])
    else:
        # this was faster than trying to build the multi index from polars
        sort_idxs = df.select(pl.arg_sort_by([id_col, time_col]).alias("idx"))["idx"]
        idx = pd.Index(sort_idxs.to_numpy())
    if idx.is_monotonic_increasing:
        return None
    if isinstance(df, pd.DataFrame):
        sort_idxs = idx.argsort()
    return sort_idxs

# %% ../nbs/processing.ipynb 8
def assign_columns(
    df: DataFrame,
    names: Union[str, List[str]],
    values: Union[np.ndarray, pd.Series, pl_Series],
) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        df[names] = values
    else:
        is_scalar = isinstance(values, str) or not hasattr(values, "__len__")
        if is_scalar:
            assert isinstance(names, str)
            vals: Union[pl_DataFrame, pl_Series, pl.Expr] = pl.lit(values).alias(names)
        elif isinstance(values, pl_Series):
            assert isinstance(names, str)
            vals = values.alias(names)
        else:
            if isinstance(names, str):
                names = [names]
            vals = pl.from_numpy(values, schema=names)
        df = df.with_columns(vals)
    return df

# %% ../nbs/processing.ipynb 11
def take_rows(df: Union[DataFrame, Series, np.ndarray], idxs: np.ndarray) -> DataFrame:
    if isinstance(df, (pd.DataFrame, pd.Series)):
        df = df.iloc[idxs]
    else:
        df = df[idxs]
    return df

# %% ../nbs/processing.ipynb 13
def filter_with_mask(
    df: Union[Series, DataFrame, pd.Index, np.ndarray],
    mask: Union[np.ndarray, pd.Series, pl_Series],
) -> DataFrame:
    if isinstance(df, (pd.DataFrame, pd.Series, pd.Index, np.ndarray)):
        out = df[mask]
    else:
        out = df.filter(mask)  # type: ignore
    return out

# %% ../nbs/processing.ipynb 14
def is_nan(s: Series) -> Series:
    if isinstance(s, pd.Series):
        out = s.isna()
    else:
        out = s.is_nan()
    return out

# %% ../nbs/processing.ipynb 16
def is_none(s: Series) -> Series:
    if isinstance(s, pd.Series):
        out = is_nan(s)
    else:
        out = s.is_null()
    return out

# %% ../nbs/processing.ipynb 18
def is_nan_or_none(s: Series) -> Series:
    return is_nan(s) | is_none(s)

# %% ../nbs/processing.ipynb 20
def match_if_categorical(
    s1: Union[Series, pd.Index], s2: Series
) -> Tuple[Series, Series]:
    if isinstance(s1.dtype, pd.CategoricalDtype):
        if isinstance(s1, pd.Index):
            cat1 = s1.categories
        else:
            cat1 = s1.cat.categories
        if isinstance(s2.dtype, pd.CategoricalDtype):
            cat2 = s2.cat.categories
        else:
            cat2 = s2.unique().astype(cat1.dtype)
        missing = set(cat2) - set(cat1)
        if missing:
            # we assume the original is s1, so we extend its categories
            new_dtype = pd.CategoricalDtype(categories=cat1.tolist() + sorted(missing))
            s1 = s1.astype(new_dtype)
            s2 = s2.astype(new_dtype)
    elif isinstance(s1, pl_Series) and s1.dtype == pl.Categorical:
        with pl.StringCache():
            cat1 = s1.cat.get_categories()
            if s2.dtype == pl.Categorical:
                cat2 = s2.cat.get_categories()
            else:
                cat2 = s2.unique().sort().cast(cat1.dtype)
            # populate cache, keep original categories first
            pl.concat([cat1, cat2]).cast(pl.Categorical)
            s1 = s1.cast(pl.Utf8).cast(pl.Categorical)
            s2 = s2.cast(pl.Utf8).cast(pl.Categorical)
    return s1, s2

# %% ../nbs/processing.ipynb 21
def vertical_concat(dfs: List[DataFrame], match_categories: bool = True) -> DataFrame:
    if not dfs:
        raise ValueError("Can't concatenate empty list.")
    if isinstance(dfs[0], pd.DataFrame):
        cat_cols = [
            c
            for c, dtype in dfs[0].dtypes.items()
            if isinstance(dtype, pd.CategoricalDtype)
        ]
        if match_categories and cat_cols:
            if len(dfs) > 2:
                raise NotImplementedError(
                    "Categorical replacement for more than two dataframes"
                )
            assert len(dfs) == 2
            df1, df2 = dfs
            df1 = df1.copy(deep=False)
            df2 = df2.copy(deep=False)
            for col in cat_cols:
                s1, s2 = match_if_categorical(df1[col], df2[col])
                df1[col] = s1
                df2[col] = s2
            dfs = [df1, df2]
        out = pd.concat(dfs).reset_index(drop=True)
    else:
        all_cols = dfs[0].columns
        cat_cols = [
            all_cols[i]
            for i, dtype in enumerate(dfs[0].dtypes)
            if dtype == pl.Categorical
        ]
        if match_categories and cat_cols:
            if len(dfs) > 2:
                raise NotImplementedError(
                    "Categorical replacement for more than two dataframes"
                )
            assert len(dfs) == 2
            df1, df2 = dfs
            for col in cat_cols:
                s1, s2 = match_if_categorical(df1[col], df2[col])
                df1 = df1.with_columns(s1)
                df2 = df2.with_columns(s2)
            dfs = [df1, df2]
        out = pl.concat(dfs)
    return out

# %% ../nbs/processing.ipynb 25
def horizontal_concat(dfs: List[DataFrame]) -> DataFrame:
    if not dfs:
        raise ValueError("Can't concatenate empty list.")
    if isinstance(dfs[0], pd.DataFrame):
        out = pd.concat(dfs, axis=1)
    elif isinstance(dfs[0], pl_DataFrame):
        out = pl.concat(dfs, how="horizontal")
    else:
        raise ValueError(f"Got list of unexpected types: {type(dfs[0])}.")
    return out

# %% ../nbs/processing.ipynb 27
def copy_if_pandas(df: DataFrame, deep: bool = False) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        df = df.copy(deep=deep)
    return df

# %% ../nbs/processing.ipynb 28
def join(
    df1: Union[DataFrame, Series],
    df2: Union[DataFrame, Series],
    on: Union[str, List[str]],
    how: str = "inner",
) -> DataFrame:
    if isinstance(df1, (pd.Series, pl_Series)):
        df1 = df1.to_frame()
    if isinstance(df2, (pd.Series, pl_Series)):
        df2 = df2.to_frame()
    if isinstance(df1, pd.DataFrame):
        out = df1.merge(df2, on=on, how=how)
    else:
        out = df1.join(df2, on=on, how=how)  # type: ignore
    return out

# %% ../nbs/processing.ipynb 29
def drop_index_if_pandas(df: DataFrame) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        df = df.reset_index(drop=True)
    return df

# %% ../nbs/processing.ipynb 30
def rename(df: DataFrame, mapping: Dict[str, str]) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        df = df.rename(columns=mapping, copy=False)
    else:
        df = df.rename(mapping)
    return df

# %% ../nbs/processing.ipynb 31
def sort(df: DataFrame, by: Optional[Union[str, List[str]]] = None) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        out = df.sort_values(by).reset_index(drop=True)
    elif isinstance(df, (pd.Series, pd.Index)):
        out = df.sort_values()
        if isinstance(out, pd.Series):
            out = out.reset_index(drop=True)
    elif isinstance(df, pl_DataFrame):
        out = df.sort(by)
    else:
        out = df.sort()
    return out

# %% ../nbs/processing.ipynb 34
def offset_dates(
    dates: Union[pd.Index, pl_Series],
    freq: Union[int, str, BaseOffset],
    n: int,
):
    if isinstance(dates, (pd.DatetimeIndex, pd.Series, pd.Index)) and isinstance(
        freq, (int, BaseOffset)
    ):
        out = dates + n * freq
    elif isinstance(dates, pl_Series) and isinstance(freq, int):
        out = dates + n * freq
    elif isinstance(dates, pl_Series) and isinstance(freq, str):
        freq_n, freq_offset = re.findall(r"(\d+)(\w+)", freq)[0]
        total_n = int(freq_n) * n
        out = dates.dt.offset_by(f"{total_n}{freq_offset}")
    else:
        raise ValueError(
            f"Can't process the following combination {(type(dates), type(freq))}"
        )
    return out

# %% ../nbs/processing.ipynb 35
def group_by(df: Union[Series, DataFrame], by, maintain_order=False):
    if isinstance(df, (pd.Series, pd.DataFrame)):
        out = df.groupby(by, observed=True, sort=not maintain_order)
    else:
        if isinstance(df, pl_Series):
            df = df.to_frame()
        try:
            out = df.group_by(by, maintain_order=maintain_order)
        except AttributeError:
            out = df.groupby(by, maintain_order=maintain_order)
    return out

# %% ../nbs/processing.ipynb 36
def group_by_agg(df: DataFrame, by, aggs, maintain_order=False) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        out = group_by(df, by, maintain_order).agg(aggs).reset_index()
    else:
        out = group_by(df, by, maintain_order).agg(
            *[getattr(pl.col(c), agg)() for c, agg in aggs.items()]
        )
    return out

# %% ../nbs/processing.ipynb 39
def is_in(s: Series, collection) -> Series:
    if isinstance(s, pl_Series):
        out = s.is_in(collection)
    else:
        out = s.isin(collection)
    return out

# %% ../nbs/processing.ipynb 42
def between(s: Series, lower: Series, upper: Series) -> Series:
    if isinstance(s, pd.Series):
        out = s.between(lower, upper)
    else:
        out = s.is_between(lower, upper)
    return out

# %% ../nbs/processing.ipynb 45
def fill_null(df: DataFrame, mapping: Dict[str, Any]) -> DataFrame:
    if isinstance(df, pd.DataFrame):
        out = df.fillna(mapping)
    else:
        out = df.with_columns(*[pl.col(col).fill_null(v) for col, v in mapping.items()])
    return out

# %% ../nbs/processing.ipynb 48
def cast(s: Series, dtype: type) -> Series:
    if isinstance(s, pd.Series):
        s = s.astype(dtype)
    else:
        s = s.cast(dtype)
    return s

# %% ../nbs/processing.ipynb 51
def value_cols_to_numpy(
    df: DataFrame, id_col: str, time_col: str, target_col: str
) -> np.ndarray:
    exclude_cols = [id_col, time_col, target_col]
    value_cols = [target_col] + [col for col in df.columns if col not in exclude_cols]
    data = to_numpy(df[value_cols])
    if data.dtype not in (np.float32, np.float64):
        data = data.astype(np.float32)
    return data

# %% ../nbs/processing.ipynb 52
def process_df(
    df: DataFrame, id_col: str, time_col: str, target_col: str
) -> Tuple[Series, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
    """Extract components from dataframe

    Parameters
    ----------
    df : pandas or polars DataFrame
        Input dataframe with id, times and target values.

    Returns
    -------
    ids : pandas or polars Serie
        serie with the sorted unique ids present in the data.
    last_times : numpy array
        array with the last time for each serie.
    data : numpy ndarray
        2d array with target plus features values.
    indptr : numpy ndarray
        1d array with indices to the start and end of each serie.
    sort_idxs : numpy array or None
        array with the indices that would sort the original data.
        If the data is already sorted this is `None`.
    """
    # validations
    validate_format(df, id_col, time_col, target_col)

    # ids
    id_counts = counts_by_id(df, id_col)
    uids = id_counts[id_col]

    # indices
    sizes = id_counts["counts"].to_numpy()
    indptr = np.append(0, sizes.cumsum()).astype(np.int32)
    last_idxs = indptr[1:] - 1

    # data
    data = value_cols_to_numpy(df, id_col, time_col, target_col)

    # check if we need to sort
    sort_idxs = maybe_compute_sort_indices(df, id_col, time_col)
    if sort_idxs is not None:
        data = data[sort_idxs]
        last_idxs = sort_idxs[last_idxs]
    times = df[time_col].to_numpy()[last_idxs]
    return uids, times, data, indptr, sort_idxs

# %% ../nbs/processing.ipynb 53
class DataFrameProcessor:
    def __init__(
        self,
        id_col: str = "unique_id",
        time_col: str = "ds",
        target_col: str = "y",
    ):
        self.id_col = id_col
        self.time_col = time_col
        self.target_col = target_col

    def process(
        self, df: DataFrame
    ) -> Tuple[Series, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
        return process_df(df, self.id_col, self.time_col, self.target_col)

# %% ../nbs/processing.ipynb 57
def _single_split(
    df: DataFrame,
    i_window: int,
    n_windows: int,
    h: int,
    id_col: str,
    time_col: str,
    freq: Union[int, str, pd.offsets.BaseOffset],
    max_dates: Series,
    step_size: Optional[int] = None,
    input_size: Optional[int] = None,
) -> Tuple[DataFrame, Series, Series]:
    if step_size is None:
        step_size = h
    test_size = h + step_size * (n_windows - 1)
    offset = test_size - i_window * step_size
    train_ends = offset_dates(max_dates, freq, -offset)
    valid_ends = offset_dates(train_ends, freq, h)
    train_mask = df[time_col].le(train_ends)
    valid_mask = df[time_col].gt(train_ends) & df[time_col].le(valid_ends)
    if input_size is not None:
        train_starts = offset_dates(train_ends, freq, -input_size)
        train_mask &= df[time_col].gt(train_starts)
    train_sizes = group_by(train_mask, df[id_col], maintain_order=True).sum()
    if isinstance(train_sizes, pd.Series):
        train_sizes = train_sizes.reset_index()
    zeros_mask = train_sizes[time_col].eq(0)
    if zeros_mask.all():
        raise ValueError(
            "All series are too short for the cross validation settings, "
            f"at least {offset + 1} samples are required.\n"
            "Please reduce `n_windows` or `h`."
        )
    elif zeros_mask.any():
        ids = filter_with_mask(train_sizes[id_col], zeros_mask)
        warnings.warn(
            "The following series are too short for the window "
            f"and will be dropped: {reprlib.repr(list(ids))}"
        )
        dropped_ids = is_in(df[id_col], ids)
        valid_mask &= ~dropped_ids
    if isinstance(train_ends, pd.Series):
        cutoffs: DataFrame = (
            train_ends.set_axis(df[id_col])
            .groupby(id_col, observed=True)
            .head(1)
            .rename("cutoff")
            .reset_index()
        )
    else:
        cutoffs = train_ends.to_frame().with_columns(df[id_col])
        cutoffs = (
            group_by(cutoffs, id_col)
            .agg(pl.col(time_col).head(1))
            .explode(pl.col(time_col))
            .rename({time_col: "cutoff"})
        )
    return cutoffs, train_mask, valid_mask

# %% ../nbs/processing.ipynb 58
def backtest_splits(
    df: DataFrame,
    n_windows: int,
    h: int,
    id_col: str,
    time_col: str,
    freq: Union[int, str, pd.offsets.BaseOffset],
    step_size: Optional[int] = None,
    input_size: Optional[int] = None,
) -> Generator[Tuple[DataFrame, DataFrame, DataFrame], None, None]:
    if isinstance(df, pd.DataFrame):
        max_dates = df.groupby(id_col, observed=True)[time_col].transform("max")
    else:
        max_dates = df.select(pl.col(time_col).max().over(id_col))[time_col]
    for i in range(n_windows):
        cutoffs, train_mask, valid_mask = _single_split(
            df,
            i_window=i,
            n_windows=n_windows,
            h=h,
            id_col=id_col,
            time_col=time_col,
            freq=freq,
            max_dates=max_dates,
            step_size=step_size,
            input_size=input_size,
        )
        train = filter_with_mask(df, train_mask)
        valid = filter_with_mask(df, valid_mask)
        yield cutoffs, train, valid
