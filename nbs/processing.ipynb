{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9284ac-4d6c-42fc-8d05-13ff8fa7460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96e02b-4e1b-4284-8808-58c7dbb1fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dfc5b-5bd4-44b6-aac4-012cd326010c",
   "metadata": {},
   "source": [
    "# Processing\n",
    "> DataFrame processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133caf6-09b0-4de3-b168-b7cff92040a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utilsforecast.compat import DataFrame, pl_DataFrame\n",
    "from utilsforecast.grouped_array import GroupedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302ecbc-e89a-4c60-8919-b6881c497a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataFrameProcessing:\n",
    "    \"\"\"\n",
    "    A utility to process Pandas or Polars dataframes for time series forecasting.\n",
    "\n",
    "    This class ensures the dataframe is properly structured, with required columns\n",
    "    ('unique_id', 'ds', 'y'), and the 'ds' column is of datetime type. It also\n",
    "    provides options for sorting the dataframe based on a unique identifier and a\n",
    "    timestamp, and separates the data into different arrays for easy access during\n",
    "    forecasting operations.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame or pl.DataFrame\n",
    "        A pandas or polars dataframe to be processed.\n",
    "    sort_dataframe : bool\n",
    "        A boolean indicating whether the dataframe should be sorted.\n",
    "    validate : bool, (default=True)\n",
    "        Ensure the dataframe matches the required format.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    __call__():\n",
    "        Processes the dataframe by ensuring the columns are in the correct format,\n",
    "        sorts the dataframe if required, and separates the data into different\n",
    "        arrays for future operations.\n",
    "    _to_np_and_engine():\n",
    "        Converts the dataframe to a numpy structured array and identifies the\n",
    "        dataframe engine (pandas or polars).\n",
    "    _validate_dataframe(dataframe: Union[pd.DataFrame, pl.DataFrame]):\n",
    "        Checks if the required columns ('unique_id', 'ds', 'y') are present in the\n",
    "        dataframe.\n",
    "    _check_datetime(arr: np.array) -> np.array:\n",
    "        Validates that the 'ds' column is of datetime type, and if not, attempts to\n",
    "        convert it to datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: DataFrame,\n",
    "        sort_dataframe: bool,\n",
    "        validate: bool = True,\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.sort_dataframe = sort_dataframe\n",
    "        self.validate = validate\n",
    "\n",
    "        # Columns declaration\n",
    "        self.non_value_columns = [\"unique_id\", \"ds\"]\n",
    "        self.datetime_column_name = \"ds\"\n",
    "        self.dt_dtype = np.dtype(\"datetime64\")\n",
    "        self.__call__()\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Sequential execution of the code\"\"\"\n",
    "        # Declaring values that will be utilized\n",
    "        self.np_df = self._to_np_and_engine()\n",
    "        self.dataframe_columns = self.np_df.dtype.names\n",
    "\n",
    "        # Processing value columns\n",
    "        value_columns = [\n",
    "            column\n",
    "            for column in self.dataframe_columns\n",
    "            if column not in self.non_value_columns\n",
    "        ]\n",
    "        self.value_array = self.np_df[value_columns]\n",
    "        if self.value_array.ndim == 1 and len(value_columns) > 1:\n",
    "            self.value_array = np.stack(\n",
    "                [\n",
    "                    self.value_array[name].astype(float)\n",
    "                    for name in self.value_array.dtype.names\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        if self.value_array.ndim == 1 and len(value_columns) == 1:\n",
    "            self.value_array = (\n",
    "                self.value_array[value_columns].astype(float).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "        # Processing unique_id\n",
    "        self.unique_id = self.np_df[\"unique_id\"]\n",
    "        if self.unique_id.dtype.kind == \"O\":\n",
    "            self.unique_id.astype(str)\n",
    "\n",
    "        # If values are already int or float then they won't be converted\n",
    "        if self.unique_id.dtype.kind not in [\"i\", \"f\"]:\n",
    "            # If all values in the numpy array are numerical then proceed with conversion\n",
    "            if np.char.isnumeric(self.unique_id.astype(str)).all():\n",
    "                # If number are whole then they will be converted to `int`, else `float`\n",
    "                # This is pure aesthetics addition.\n",
    "                self.unique_id = self.unique_id.astype(float)\n",
    "                if np.isclose(self.unique_id, np.round(self.unique_id)).all():\n",
    "                    self.unique_id = self.unique_id.astype(int)\n",
    "        # NOTE: When sorting with Numpy, character values may be prioritized over numerical values if the data\n",
    "        # type is set to 'object'. For instance, the value '10' would come before '3' because it contains '1' and '0'\n",
    "        # at the beginning. One solution to this problem is to convert the data to 'float' if it is numerical.\n",
    "        unique_id_count = pd.Series(self.unique_id).value_counts(sort=False)\n",
    "        self.indices, sizes = unique_id_count.index, unique_id_count.values\n",
    "        cum_sizes = np.cumsum(sizes)\n",
    "\n",
    "        # Processing datestamp\n",
    "        self.dates = self.np_df[self.datetime_column_name]\n",
    "        if self.engine_dataframe == pd.DataFrame:\n",
    "            self.dates = self.dataframe.index.get_level_values(\n",
    "                self.datetime_column_name\n",
    "            )\n",
    "        self.dates = self.dates[cum_sizes - 1]\n",
    "        self.indptr = np.append(0, cum_sizes).astype(np.int32)\n",
    "\n",
    "        # Index that will be used by pandas, not polars\n",
    "        self.index = pd.MultiIndex.from_arrays(\n",
    "            [\n",
    "                self.np_df[\"unique_id\"],\n",
    "                self.np_df[\"ds\"],\n",
    "            ],\n",
    "            names=[\"unique_id\", \"ds\"],\n",
    "        )\n",
    "\n",
    "    def grouped_array(self):\n",
    "        return GroupedArray(self.value_array, self.indptr)\n",
    "\n",
    "    def _to_np_and_engine(self):\n",
    "        \"\"\"\n",
    "        This function will be utilised to convert DataFrame to dictionary.\n",
    "\n",
    "        Returns:\n",
    "            tuple[pd.DataFrame or pl.DataFrame, dict]: the engine that will be used to construct\n",
    "                the output DataFrame and dictionary of DataFrame values\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If DataFrame engine is not supported and/or accounted for.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################\n",
    "        # Polars DataFrame #\n",
    "        ####################\n",
    "        if isinstance(self.dataframe, pl_DataFrame):\n",
    "            from packaging.version import Version\n",
    "            \n",
    "            import polars as pl\n",
    "\n",
    "            # Ensure that all required columns are present in the DataFrame:\n",
    "            self.engine_dataframe = pl_DataFrame\n",
    "            if self.validate:\n",
    "                self._validate_dataframe(self.dataframe)\n",
    "            elif self.validate == False:\n",
    "                self._partial_val_df(self.dataframe)\n",
    "\n",
    "            # datetime check\n",
    "            dt_arr = self.dataframe[\"ds\"].to_numpy()\n",
    "            processed_dt_arr = self._check_datetime(dt_arr)\n",
    "            if type(dt_arr) != type(processed_dt_arr):\n",
    "                self.dataframe = self.datafraFme.with_columns(\n",
    "                    pl.from_numpy(processed_dt_arr.to_numpy(), schema=[\"ds\"])\n",
    "                )\n",
    "\n",
    "            sample_index_df = self.dataframe[self.non_value_columns]\n",
    "            sorted_index_df = sample_index_df.sort(self.non_value_columns)\n",
    "            is_monotonic_increasing = sample_index_df.frame_equal(sorted_index_df)\n",
    "\n",
    "            # Sorting will be performed if sort is set to true and values are unsorted\n",
    "            if not is_monotonic_increasing and self.sort_dataframe:\n",
    "                self.dataframe = self.dataframe.sort(self.non_value_columns)\n",
    "\n",
    "            # resources: https://github.com/pola-rs/polars/blob/4fca1ae51864f74e0367d8bc91b4a2db00e54174/py-polars/polars/dataframe/frame.py#L1975\n",
    "            # resources: https://numpy.org/doc/stable/user/basics.rec.html\n",
    "            # resources: https://numpy.org/doc/stable/reference/generated/numpy.core.records.fromarrays.html\n",
    "            # NOTE: Structured array is not available in polars under the version 0.17.12\n",
    "            pl_version = Version(pl.__version__)\n",
    "            min_pl_v = Version(\"0.17.12\")\n",
    "            if pl_version >= min_pl_v:\n",
    "                return self.dataframe.to_numpy(structured=True)\n",
    "            else:\n",
    "                arrays = []\n",
    "                for column, column_dtype in self.dataframe.schema.items():\n",
    "                    ser = self.dataframe[column]\n",
    "                    arr = ser.to_numpy()\n",
    "                    arrays.append(\n",
    "                        arr.astype(str, copy=False)\n",
    "                        if str(column_dtype) == \"Utf8\" and not ser.has_validity()\n",
    "                        else arr\n",
    "                    )\n",
    "                arr_dtypes = list(\n",
    "                    zip(self.dataframe.columns, (a.dtype for a in arrays))\n",
    "                )\n",
    "                return np.rec.fromarrays(arrays, dtype=np.dtype(arr_dtypes))\n",
    "\n",
    "        ####################\n",
    "        # Pandas DataFrame #\n",
    "        ####################\n",
    "        elif isinstance(self.dataframe, pd.DataFrame):\n",
    "            self.engine_dataframe = pd.DataFrame\n",
    "            # Ensure that all required columns are present in the DataFrame:\n",
    "            # Full validation\n",
    "            if self.validate and self.dataframe.index.name == \"unique_id\":\n",
    "                reset_df = self.dataframe.reset_index()\n",
    "                self._validate_dataframe(reset_df)\n",
    "                del reset_df\n",
    "\n",
    "            elif self.validate and self.dataframe.index.name != \"unique_id\":\n",
    "                self._validate_dataframe(self.dataframe)\n",
    "                self.dataframe = self.dataframe.set_index(\"unique_id\")\n",
    "\n",
    "            # Partial validation\n",
    "            elif self.validate == False and self.dataframe.index.name == \"unique_id\":\n",
    "                reset_df = self.dataframe.reset_index()\n",
    "                self._partial_val_df(reset_df)\n",
    "                del reset_df\n",
    "\n",
    "            elif self.validate == False and self.dataframe.index.name != \"unique_id\":\n",
    "                self._partial_val_df(self.dataframe)\n",
    "                self.dataframe = self.dataframe.set_index(\"unique_id\")\n",
    "\n",
    "            # Datetime check\n",
    "            dt_arr = self.dataframe[\"ds\"].values\n",
    "            self.dataframe[\"ds\"] = self._check_datetime(dt_arr)\n",
    "\n",
    "            self.dataframe = self.dataframe.set_index(\"ds\", append=True)\n",
    "\n",
    "            # Sorting will be performed if sort is set to true and values are unsorted\n",
    "            if not self.dataframe.index.is_monotonic_increasing and self.sort_dataframe:\n",
    "                self.dataframe = self.dataframe.sort_values(self.non_value_columns)\n",
    "\n",
    "            np_df = self.dataframe.to_records(index=True)\n",
    "\n",
    "            return np_df\n",
    "\n",
    "        ####################\n",
    "        # Not Supported DF #\n",
    "        ####################\n",
    "        else:\n",
    "            raise ValueError(f\"{type(self.dataframe)} is not supported\")\n",
    "\n",
    "    def _validate_dataframe(self, dataframe: DataFrame):\n",
    "        \"\"\"\n",
    "        Will ensure that all DataFrame columns match the required columns.\n",
    "\n",
    "        This code requires a pandas DataFrame with the following structure:\n",
    "\n",
    "        Columns:\n",
    "        - `unique_id` Union[str, int, categorical]: an identifier for the series\n",
    "        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a\n",
    "            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "        - `y` Union[float, int]: represents the measurement we wish to forecast.\n",
    "\n",
    "        Raise:\n",
    "            KeyError: DataFrame is missing `unique_id`, `ds`, `y` columns.\n",
    "        \"\"\"\n",
    "        required_columns = [\"unique_id\", \"ds\", \"y\"]\n",
    "        matches = all(rc in dataframe.columns for rc in required_columns)\n",
    "        if not matches:\n",
    "            raise KeyError(\n",
    "                \"The DataFrame doesn't contain {} columns\".format(\n",
    "                    \", \".join(required_columns)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _partial_val_df(self, dataframe: DataFrame):\n",
    "        \"\"\"\n",
    "        Will ensure that all DataFrame columns match the required columns.\n",
    "\n",
    "        This code requires a pandas DataFrame with the following structure:\n",
    "\n",
    "        Columns:\n",
    "        - `unique_id` Union[str, int, categorical]: an identifier for the series\n",
    "        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a\n",
    "            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "        Raise:\n",
    "            KeyError: DataFrame is missing `unique_id` and/or `ds` columns.\n",
    "        \"\"\"\n",
    "        required_columns = [\"unique_id\", \"ds\"]\n",
    "        matches = all(rc in dataframe.columns for rc in required_columns)\n",
    "        if not matches:\n",
    "            raise KeyError(\n",
    "                \"The DataFrame doesn't contain {} columns\".format(\n",
    "                    \", \".join(required_columns)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _check_datetime(self, arr: np.ndarray) -> Union[pd.DatetimeIndex, np.ndarray]:\n",
    "        dt_check = pd.api.types.is_datetime64_any_dtype(arr)\n",
    "        int_float_check = arr.dtype.kind in [\"i\", \"f\"]\n",
    "        if not dt_check and not int_float_check:\n",
    "            self._ds_is_dt = True\n",
    "            try:\n",
    "                return pd.to_datetime(arr)\n",
    "            except Exception as e:\n",
    "                msg = (\n",
    "                    \"Failed to parse `ds` column as datetime. \"\n",
    "                    \"Please use `pd.to_datetime` outside to fix the error. \"\n",
    "                    f\"{e}\"\n",
    "                )\n",
    "                raise Exception(msg) from e\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52bd54-9cf6-4aea-b43d-609a476c02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from fastcore.test import test_eq\n",
    "\n",
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d26e46-d370-48ef-9197-151be69ca75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_series(10_000, n_static_features=2, equal_ends=False, engine='pandas').set_index('unique_id')\n",
    "sorted_series = series.sort_values(['unique_id', 'ds'])\n",
    "unsorted_series = sorted_series.sample(frac=1.0)\n",
    "\n",
    "df_process = DataFrameProcessing(dataframe=unsorted_series, sort_dataframe=True)\n",
    "ga = df_process.grouped_array()\n",
    "indices = df_process.indices\n",
    "dates = df_process.dates\n",
    "ds = df_process.index\n",
    "\n",
    "np.testing.assert_allclose(ga.data, sorted_series.drop(columns='ds').values)\n",
    "test_eq(indices, sorted_series.index.unique(level='unique_id'))\n",
    "test_eq(dates, series.groupby('unique_id')['ds'].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dd36b-c666-4c2d-9b6a-2c40e2d0a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_series(10_000, n_static_features=2, equal_ends=False, engine='pandas').set_index('unique_id')\n",
    "sorted_series = series.sort_values(['unique_id', 'ds'])\n",
    "unsorted_series = sorted_series.sample(frac=1.0)\n",
    "\n",
    "df_process = DataFrameProcessing(dataframe=unsorted_series, sort_dataframe=True)\n",
    "ga = df_process.grouped_array()\n",
    "indices = df_process.indices\n",
    "dates = df_process.dates\n",
    "ds = df_process.index\n",
    "\n",
    "np.testing.assert_allclose(ga.data, sorted_series.drop(columns='ds').values)\n",
    "test_eq(indices, sorted_series.index.unique(level='unique_id'))\n",
    "test_eq(dates, series.groupby('unique_id')['ds'].max().values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
