{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9284ac-4d6c-42fc-8d05-13ff8fa7460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96e02b-4e1b-4284-8808-58c7dbb1fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133caf6-09b0-4de3-b168-b7cff92040a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "import reprlib\n",
    "import warnings\n",
    "from typing import Any, Dict, Generator, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BaseOffset\n",
    "\n",
    "from utilsforecast.compat import DataFrame, Series, pl, pl_DataFrame, pl_Series\n",
    "from utilsforecast.validation import (\n",
    "    _is_dt_dtype,\n",
    "    _is_int_dtype,\n",
    "    ensure_shallow_copy,\n",
    "    validate_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970c3e9-b00a-4485-a8de-26471d515e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from nbdev import show_doc\n",
    "\n",
    "from utilsforecast.compat import POLARS_INSTALLED\n",
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236dbb3c-4358-451c-a9e6-7c17b43003fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "import polars.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688032bd-838d-4086-be34-d0b02590a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _polars_categorical_to_numerical(serie: pl_Series) -> pl_Series:\n",
    "    if serie.dtype == pl.Categorical:\n",
    "        serie = serie.to_physical()\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04df186-1663-45b9-90ed-cdec91a137fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_numpy(df: DataFrame) -> np.ndarray:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        cat_cols = [c for c, dtype in df.dtypes.items() if isinstance(dtype, pd.CategoricalDtype)]\n",
    "        if cat_cols:\n",
    "            df = df.copy(deep=False)\n",
    "            df = ensure_shallow_copy(df)\n",
    "            for col in cat_cols:\n",
    "                df[col] = df[col].cat.codes\n",
    "        df = df.to_numpy()\n",
    "    else:\n",
    "        try:\n",
    "            expr = pl.all().map_batches(_polars_categorical_to_numerical)\n",
    "        except AttributeError:\n",
    "            expr = pl.all().map(_polars_categorical_to_numerical)\n",
    "        df = df.select(expr).to_numpy(order='c')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73199e1f-2104-47c5-9415-07b20e8a61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def counts_by_id(df: DataFrame, id_col: str) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        id_counts = df[id_col].value_counts(sort=False, dropna=False)\n",
    "        ids = id_counts.index\n",
    "        if isinstance(ids.dtype, pd.CategoricalDtype):\n",
    "            # there's no observed argument in value_counts\n",
    "            # so this can return unseen categories\n",
    "            id_counts = id_counts[id_counts > 0]\n",
    "            ids = id_counts.index.codes\n",
    "        sort_idxs = ids.argsort()\n",
    "        id_counts = id_counts.iloc[sort_idxs].reset_index()\n",
    "    else:\n",
    "        id_counts = df[id_col].value_counts().sort(id_col)\n",
    "    id_counts.columns = [id_col, 'counts']\n",
    "    return id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29656557-2fb3-420f-ba19-fdc118118ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def maybe_compute_sort_indices(\n",
    "    df: DataFrame, id_col: str, time_col: str\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Compute indices that would sort the dataframe\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times and target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array or None\n",
    "        Array with indices to sort the dataframe or None if it's already sorted.\n",
    "    \"\"\"\n",
    "    ids = df[id_col]\n",
    "    times = df[time_col]\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        if isinstance(ids.dtype, pd.CategoricalDtype):\n",
    "            # we sort categoricals by their codes, this is also done in counts_by_id\n",
    "            ids = ids.cat.codes\n",
    "        # pandas series alignment makes this slow, cast to numpy\n",
    "        ids = ids.to_numpy()\n",
    "        times = times.to_numpy()\n",
    "    ids_are_sorted = (ids[:-1] <= ids[1:]).all()\n",
    "    if ids_are_sorted:\n",
    "        times_are_sorted = (\n",
    "            (times[:-1] < times[1:])  # times are ascending\n",
    "            | (ids[:-1] != ids[1:])  # except when the id changes\n",
    "        ).all()\n",
    "        if times_are_sorted:\n",
    "            return None\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        if pd.api.types.is_object_dtype(df.dtypes[id_col]):\n",
    "            # MultiIndex.argsort is faster than lexsort for strings            \n",
    "            sort_idxs = pd.MultiIndex.from_frame(df[[id_col, time_col]]).argsort()\n",
    "        else:\n",
    "            sort_idxs = np.lexsort((times, ids))\n",
    "    else:\n",
    "        sort_idxs = df.select(\n",
    "            pl.arg_sort_by([id_col, time_col])\n",
    "        ).to_series(0).to_numpy()\n",
    "    return sort_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc46c8b-8aeb-4a0f-be20-5db503cf03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assign_columns(df: DataFrame, names: Union[str, List[str]], values: Union[np.ndarray, pd.Series, pl_Series, List[float]]) -> DataFrame:\n",
    "    if isinstance(values, list) and (len(values) != df.shape[0] or not isinstance(names, str)):\n",
    "        raise ValueError('Only single column assignment is supported for lists.')\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df[names] = values\n",
    "    else:\n",
    "        is_scalar = isinstance(values, str) or not hasattr(values, '__len__')\n",
    "        if is_scalar:\n",
    "            assert isinstance(names, str)\n",
    "            vals: Union[pl_DataFrame, pl_Series, pl.Expr] = pl.lit(values).alias(names)\n",
    "        elif isinstance(values, pl_Series):\n",
    "            assert isinstance(names, str)\n",
    "            vals = values.alias(names)\n",
    "        else:\n",
    "            if isinstance(values, np.ndarray):\n",
    "                if isinstance(names, str):\n",
    "                    names = [names]\n",
    "                vals = pl.from_numpy(values, schema=names, orient='row')\n",
    "            elif isinstance(values, list):\n",
    "                assert isinstance(names, str)\n",
    "                vals = pl_Series(name=names, values=values)\n",
    "        df = df.with_columns(vals)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb147d-7aba-49d9-b741-aff36be7caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = ['pandas']\n",
    "if POLARS_INSTALLED:\n",
    "    engines.append('polars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb9058-3993-4881-82fa-4fbe874ab56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for engine in engines:\n",
    "    series = generate_series(2, engine=engine)\n",
    "    x = np.random.rand(series.shape[0])\n",
    "    series = assign_columns(series, 'x', x)\n",
    "    series = assign_columns(series, ['y', 'z'], np.vstack([x, x]).T)\n",
    "    series = assign_columns(series, 'ones', 1)\n",
    "    series = assign_columns(series, 'zeros', np.zeros(series.shape[0]))\n",
    "    series = assign_columns(series, 'as', 'a')\n",
    "    np.testing.assert_allclose(\n",
    "        series[['x', 'y', 'z']],\n",
    "        np.vstack([x, x, x]).T\n",
    "    )\n",
    "    np.testing.assert_equal(series['ones'], np.ones(series.shape[0]))\n",
    "    np.testing.assert_equal(series['as'], np.full(series.shape[0], 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15335a-9ea5-4793-8e62-5b47d51b1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def drop_columns(df: DataFrame, columns: Union[str, List[str]]) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.drop(columns=columns)\n",
    "    else:\n",
    "        df = df.drop(columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e41bbb-a541-4957-8c28-7af820488f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def take_rows(df: Union[DataFrame, Series, np.ndarray], idxs: np.ndarray) -> DataFrame:\n",
    "    if isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "        df = df.iloc[idxs]\n",
    "    else:\n",
    "        df = df[idxs]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c4e57-5ef8-4803-bfd0-19af4313340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for engine in engines:\n",
    "    series = generate_series(2, engine=engine)\n",
    "    subset = take_rows(series, np.array([0, 2]))\n",
    "    assert subset.shape[0] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135ff8a-9abc-48fa-8adb-ddbb65fbcf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_with_mask(\n",
    "    df: Union[Series, DataFrame, pd.Index, np.ndarray],\n",
    "    mask: Union[np.ndarray, pd.Series, pl_Series]\n",
    ") -> DataFrame:\n",
    "    if isinstance(df, (pd.DataFrame, pd.Series, pd.Index, np.ndarray)):\n",
    "        out = df[mask]\n",
    "    else:\n",
    "        out = df.filter(mask)  # type: ignore\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49529350-f0b2-4416-a78a-4f67d27e293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_nan(s: Series) -> Series:\n",
    "    if isinstance(s, pd.Series):\n",
    "        out = s.isna()\n",
    "    else:\n",
    "        out = s.is_nan()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f13160-171f-4961-b13c-e58808c7983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(\n",
    "    is_nan(pd.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "    np.array([True, False, True]),\n",
    ")\n",
    "if POLARS_INSTALLED:\n",
    "    np.testing.assert_equal(\n",
    "        is_nan(pl.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "        np.array([True, False, None]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb895f6f-f4f1-4e15-8b4e-34ab8593c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_none(s: Series) -> Series:\n",
    "    if isinstance(s, pd.Series):\n",
    "        out = is_nan(s)\n",
    "    else:\n",
    "        out = s.is_null()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2df1c-4779-49cf-97a3-8198723d4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(\n",
    "    is_none(pd.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "    np.array([True, False, True]),\n",
    ")\n",
    "if POLARS_INSTALLED:\n",
    "    np.testing.assert_equal(\n",
    "        is_none(pl.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "        np.array([False, False, True]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19737f8-0898-4ee6-9f10-6a1152e6bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_nan_or_none(s: Series) -> Series:\n",
    "    return is_nan(s) | is_none(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139f563-0ea4-43c4-86d0-0d7cff621a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(\n",
    "    is_nan_or_none(pd.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "    np.array([True, False, True]),\n",
    ")\n",
    "if POLARS_INSTALLED:\n",
    "    np.testing.assert_equal(\n",
    "        is_nan_or_none(pl.Series([np.nan, 1.0, None])).to_numpy(),\n",
    "        np.array([True, False, True]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e7421-017d-49f9-bdd0-c59fc7556488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def match_if_categorical(s1: Union[Series, pd.Index], s2: Series) -> Tuple[Series, Series]:\n",
    "    if isinstance(s1.dtype, pd.CategoricalDtype):\n",
    "        if isinstance(s1, pd.Index):\n",
    "            cat1 = s1.categories\n",
    "        else:\n",
    "            cat1 = s1.cat.categories\n",
    "        if isinstance(s2.dtype, pd.CategoricalDtype):\n",
    "            cat2 = s2.cat.categories\n",
    "        else:\n",
    "            cat2 = s2.unique().astype(cat1.dtype)\n",
    "        missing = set(cat2) - set(cat1)\n",
    "        if missing:\n",
    "            # we assume the original is s1, so we extend its categories\n",
    "            new_dtype = pd.CategoricalDtype(categories=cat1.tolist() + sorted(missing))\n",
    "            s1 = s1.astype(new_dtype)\n",
    "            s2 = s2.astype(new_dtype)\n",
    "    elif isinstance(s1, pl_Series) and s1.dtype == pl.Categorical:\n",
    "        with pl.StringCache():\n",
    "            cat1 = s1.cat.get_categories()\n",
    "            if s2.dtype == pl.Categorical:\n",
    "                cat2 = s2.cat.get_categories()\n",
    "            else:\n",
    "                cat2 = s2.unique().sort().cast(cat1.dtype)\n",
    "            # populate cache, keep original categories first\n",
    "            pl.concat([cat1, cat2]).cast(pl.Categorical)\n",
    "            s1 = s1.cast(pl.Utf8).cast(pl.Categorical)\n",
    "            s2 = s2.cast(pl.Utf8).cast(pl.Categorical)\n",
    "    return s1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af783f-97a2-4af3-8b55-f24cb63d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def vertical_concat(\n",
    "    dfs: List[Union[DataFrame, Series]],\n",
    "    match_categories: bool = True\n",
    ") -> Union[DataFrame, Series]:\n",
    "    if not dfs:\n",
    "        raise ValueError(\"Can't concatenate empty list.\")\n",
    "    if isinstance(dfs[0], pd.Series):\n",
    "        out = pd.concat(dfs).reset_index(drop=True)\n",
    "    elif isinstance(dfs[0], pl_Series):\n",
    "        out = pl.concat(dfs)\n",
    "    elif isinstance(dfs[0], pd.DataFrame):\n",
    "        cat_cols = [c for c, dtype in dfs[0].dtypes.items() if isinstance(dtype, pd.CategoricalDtype)]\n",
    "        if match_categories and cat_cols:\n",
    "            if len(dfs) > 2:\n",
    "                raise NotImplementedError('Categorical replacement for more than two dataframes')\n",
    "            assert len(dfs) == 2\n",
    "            df1, df2 = dfs\n",
    "            df1 = df1.copy(deep=False)\n",
    "            df2 = df2.copy(deep=False)            \n",
    "            for col in cat_cols:\n",
    "                s1, s2 = match_if_categorical(df1[col], df2[col])\n",
    "                df1[col] = s1\n",
    "                df2[col] = s2\n",
    "            dfs = [df1, df2]\n",
    "        out = pd.concat(dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        all_cols = dfs[0].columns\n",
    "        cat_cols = [all_cols[i] for i, dtype in enumerate(dfs[0].dtypes) if dtype == pl.Categorical]\n",
    "        if match_categories and cat_cols:\n",
    "            if len(dfs) > 2:\n",
    "                raise NotImplementedError('Categorical replacement for more than two dataframes')\n",
    "            assert len(dfs) == 2\n",
    "            df1, df2 = dfs\n",
    "            for col in cat_cols:\n",
    "                s1, s2 = match_if_categorical(df1[col], df2[col])\n",
    "                df1 = df1.with_columns(s1)\n",
    "                df2 = df2.with_columns(s2)\n",
    "            dfs = [df1, df2]\n",
    "        out = pl.concat(dfs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c0461-3964-4c82-a406-9fb7ea624f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'x': ['a', 'b', 'c']}, dtype='category')\n",
    "df2 = pd.DataFrame({'x': ['f', 'b', 'a']}, dtype='category')\n",
    "pd.testing.assert_series_equal(\n",
    "    vertical_concat([df1,df2])['x'],\n",
    "    pd.Series(['a', 'b', 'c', 'f', 'b', 'a'], name='x', dtype=pd.CategoricalDtype(categories=['a', 'b', 'c', 'f']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ab374-90fc-4ba8-b442-797abc63d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "df1 = pl.DataFrame({'x': ['a', 'b', 'c']}, schema={'x': pl.Categorical})\n",
    "df2 = pl.DataFrame({'x': ['f', 'b', 'a']}, schema={'x': pl.Categorical})\n",
    "out = vertical_concat([df1,df2])['x']\n",
    "assert out.equals(pl.Series('x', ['a', 'b', 'c', 'f', 'b', 'a']))\n",
    "assert out.to_physical().equals(pl.Series('x', [0, 1, 2, 3, 1, 0]))\n",
    "assert out.cat.get_categories().equals(\n",
    "    pl.Series('x', ['a', 'b', 'c', 'f'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785253d8-46a6-4e5d-80dc-c013fdb33872",
   "metadata": {},
   "outputs": [],
   "source": [
    "for engine in engines:\n",
    "    series = generate_series(2, engine=engine)\n",
    "    doubled = vertical_concat([series, series])\n",
    "    assert doubled.shape[0] == 2 * series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da156ead-8920-4d09-9dc7-b4fd483b11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def horizontal_concat(dfs: List[DataFrame]) -> DataFrame:\n",
    "    if not dfs:\n",
    "        raise ValueError(\"Can't concatenate empty list.\")\n",
    "    if isinstance(dfs[0], pd.DataFrame):\n",
    "        out = pd.concat(dfs, axis=1)\n",
    "    elif isinstance(dfs[0], pl_DataFrame):\n",
    "        out = pl.concat(dfs, how='horizontal')\n",
    "    else:\n",
    "        raise ValueError(f'Got list of unexpected types: {type(dfs[0])}.')        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ca91e-d2a9-4bbc-9ba3-0a5f56c8d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for engine in engines:\n",
    "    series = generate_series(2, engine=engine)\n",
    "    renamer = {c: f'{c}_2' for c in series.columns}\n",
    "    if engine == 'pandas':\n",
    "        series2 = series.rename(columns=renamer)\n",
    "    else:\n",
    "        series2 = series.rename(renamer)\n",
    "    doubled = horizontal_concat([series, series2])\n",
    "    assert doubled.shape[1] == 2 * series.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5276cb-b14a-4435-b116-996e9cf241ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def copy_if_pandas(df: DataFrame, deep: bool = False) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.copy(deep=deep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfab719-b261-4953-91eb-b5226ec72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def join(\n",
    "    df1: Union[DataFrame, Series],\n",
    "    df2: Union[DataFrame, Series],\n",
    "    on: Union[str, List[str]],\n",
    "    how: str = 'inner'\n",
    ") -> DataFrame:\n",
    "    if isinstance(df1, (pd.Series, pl_Series)):\n",
    "        df1 = df1.to_frame()\n",
    "    if isinstance(df2, (pd.Series, pl_Series)):\n",
    "        df2 = df2.to_frame()\n",
    "    if isinstance(df1, pd.DataFrame):\n",
    "        out = df1.merge(df2, on=on, how=how)\n",
    "    else:\n",
    "        out = df1.join(df2, on=on, how=how)  # type: ignore\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23563a68-16c9-4379-93d8-2cfaa2120109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def drop_index_if_pandas(df: DataFrame) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bca15-5dc4-4fea-96be-911b6afb6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rename(df: DataFrame, mapping: Dict[str, str]) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.rename(columns=mapping, copy=False)\n",
    "    else:\n",
    "        df = df.rename(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140cf59-6c61-4af6-82b9-874299ad3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sort(df: DataFrame, by: Optional[Union[str, List[str]]] = None) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        out = df.sort_values(by).reset_index(drop=True)\n",
    "    elif isinstance(df, (pd.Series, pd.Index)):\n",
    "        out = df.sort_values()\n",
    "        if isinstance(out, pd.Series):\n",
    "            out = out.reset_index(drop=True)\n",
    "    elif isinstance(df, pl_DataFrame):\n",
    "        out = df.sort(by)\n",
    "    else:\n",
    "        out = df.sort()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e0b1c-3770-4d8d-a8d0-63ed2bdf147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    sort(pd.DataFrame({'x': [3, 1, 2]}), 'x'),\n",
    "    pd.DataFrame({'x': [1, 2, 3]})\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    sort(pd.DataFrame({'x': [3, 1, 2]}), ['x']),\n",
    "    pd.DataFrame({'x': [1, 2, 3]})\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    sort(pd.Series([3, 1, 2])),\n",
    "    pd.Series([1, 2, 3])\n",
    ")\n",
    "pd.testing.assert_index_equal(\n",
    "    sort(pd.Index([3, 1, 2])),\n",
    "    pd.Index([1, 2, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1c151-1f81-442d-9f32-d88ca85a5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pl.testing.assert_frame_equal(\n",
    "    sort(pl.DataFrame({'x': [3, 1, 2]}), 'x'),\n",
    "    pl.DataFrame({'x': [1, 2, 3]}),\n",
    ")\n",
    "pl.testing.assert_frame_equal(\n",
    "    sort(pl.DataFrame({'x': [3, 1, 2]}), ['x']),\n",
    "    pl.DataFrame({'x': [1, 2, 3]}),\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    sort(pl.Series('x', [3, 1, 2])),\n",
    "    pl.Series('x', [1, 2, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c15c35-62d7-4c13-a70f-94f7de4fc383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _multiply_pl_freq(freq: str, n: Union[int, Series]) -> str:\n",
    "    freq_n, freq_offset = re.findall(r'(\\d+)(\\w+)', freq)[0]\n",
    "    freq_n = int(freq_n)\n",
    "    if isinstance(n, int):\n",
    "        total_n = freq_n * n\n",
    "        out = f'{total_n}{freq_offset}'\n",
    "    else:\n",
    "        try:\n",
    "            is_int = n.dtype.is_integer()\n",
    "        except AttributeError:\n",
    "            is_int = n.is_integer()\n",
    "        if not is_int:\n",
    "            raise ValueError('`n` must be an integer or a polars series of integers.')\n",
    "        out = (n * freq_n).cast(pl.Utf8) + freq_offset\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829eeeb-649e-436c-a3b2-7b51fe42a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "test_eq(_multiply_pl_freq('1d', 4), '4d')\n",
    "test_eq(_multiply_pl_freq('2d', 4), '8d')\n",
    "pl.testing.assert_series_equal(\n",
    "    _multiply_pl_freq('1d', pl_Series([1, 2])),\n",
    "    pl_Series(['1d', '2d']),\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    _multiply_pl_freq('4m', pl_Series([2, 4])),\n",
    "    pl_Series(['8m', '16m']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9970d-a8b2-4bd9-9789-b240e494b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _ensure_month_ends(\n",
    "    times: pl_Series,\n",
    "    orig_times: pl_Series,\n",
    "    freq: Union[str, int, BaseOffset]\n",
    ") -> pl_Series:\n",
    "    if not isinstance(freq, str) or 'mo' not in freq:\n",
    "        return times\n",
    "    next_days = orig_times.dt.offset_by('1d')\n",
    "    month_ends = (next_days.dt.month() != orig_times.dt.month()).all()\n",
    "    if month_ends:\n",
    "        times = times.dt.month_end()\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde33c6c-b9ab-4cf0-8d66-2e41c3769329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def offset_times(\n",
    "    times: Union[Series, pd.Index],\n",
    "    freq: Union[int, str, BaseOffset],\n",
    "    n: Union[int, np.ndarray],\n",
    ") -> Union[Series, pd.Index]:\n",
    "    if isinstance(times, (pd.Series, pd.Index)):\n",
    "        if isinstance(freq, str):\n",
    "            freq = pd.tseries.frequencies.to_offset(freq)\n",
    "        ints = _is_int_dtype(times) and isinstance(freq, int)\n",
    "        dts = _is_dt_dtype(times) and isinstance(freq, BaseOffset)\n",
    "        if not ints and not dts:\n",
    "            raise ValueError(\n",
    "                f\"Cannot offset times with data type: '{times.dtype}' \"\n",
    "                f\"using a frequency of type: '{type(freq)}'.\"\n",
    "            )\n",
    "        out = times + n * freq\n",
    "    elif isinstance(times, pl_Series) and isinstance(freq, int):\n",
    "        out = times + n * freq\n",
    "    elif isinstance(times, pl_Series) and isinstance(freq, str):\n",
    "        total_offset = _multiply_pl_freq(freq, n)\n",
    "        out = times.dt.offset_by(total_offset)\n",
    "        out = _ensure_month_ends(out, times, freq)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Cannot offset times of type: '{type(times)}' \"\n",
    "            f\"using a frequency of type: '{type(freq)}'.\"\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9f253-3753-4c11-8a7a-410ea924469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_index_equal(\n",
    "    offset_times(pd.to_datetime(['2020-01-31', '2020-02-29', '2020-03-31']), pd.offsets.MonthEnd(), 1),\n",
    "    pd.Index(pd.to_datetime(['2020-02-29', '2020-03-31', '2020-04-30'])),\n",
    ")\n",
    "pd.testing.assert_index_equal(\n",
    "    offset_times(pd.to_datetime(['2020-01-01', '2020-02-01', '2020-03-01']), pd.offsets.MonthBegin(), 1),\n",
    "    pd.Index(pd.to_datetime(['2020-02-01', '2020-03-01', '2020-04-01'])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad919f-fea3-4f61-a766-6f952da8bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pl.testing.assert_series_equal(\n",
    "    offset_times(pl_Series([dt(2020, 1, 31), dt(2020, 2, 28), dt(2020, 3, 31)]), '1mo', 1),\n",
    "    pl_Series([dt(2020, 2, 29), dt(2020, 3, 28), dt(2020, 4, 30)]),\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    offset_times(pl_Series([dt(2020, 1, 31), dt(2020, 2, 29), dt(2020, 3, 31)]), '1mo', 1),\n",
    "    pl_Series([dt(2020, 2, 29), dt(2020, 3, 31), dt(2020, 4, 30)]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22459a37-d1ce-426c-8b69-846632794379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def offset_dates(\n",
    "    dates: Union[Series, pd.Index],\n",
    "    freq: Union[int, str, BaseOffset],\n",
    "    n: Union[int, Series],\n",
    ") -> Union[Series, pd.Index]:\n",
    "    warnings.warn(\"`offset_dates` has been renamed to `offset_times`\", category=DeprecationWarning)\n",
    "    return offset_times(dates, freq, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c4aa5-1fee-44f8-9788-13d915d2d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def time_ranges(\n",
    "    starts: Union[Series, pd.Index],\n",
    "    freq: Union[int, str, BaseOffset],\n",
    "    periods: int,\n",
    ") -> Series:\n",
    "    if isinstance(starts, pd.Series):\n",
    "        starts = pd.Index(starts)\n",
    "    if isinstance(starts, pd.Index):\n",
    "        if _is_int_dtype(starts):\n",
    "            starts_np = starts.to_numpy(copy=False)  # may be pyarrow\n",
    "            out = np.hstack(\n",
    "                [\n",
    "                    np.arange(start, start + freq * periods, freq, dtype=starts_np.dtype)\n",
    "                    for start in starts_np\n",
    "                ]\n",
    "            )\n",
    "        elif _is_dt_dtype(starts):\n",
    "            if isinstance(freq, str):\n",
    "                freq = pd.tseries.frequencies.to_offset(freq)\n",
    "            out = []\n",
    "            for i in range(periods):\n",
    "                out.append([starts + i * freq])\n",
    "            # pyarrow timestamps don't seem to work with offsets yet, keeping np.vstack\n",
    "            out = np.vstack(out).ravel(order='F')\n",
    "        else:\n",
    "            raise ValueError(f\"`starts` must be integers or timestamps, got '{starts.dtype}'.\")\n",
    "        out = pd.Series(out, dtype=starts.dtype)\n",
    "    else:\n",
    "        try:\n",
    "            is_int = starts.dtype.is_integer()\n",
    "        except AttributeError:\n",
    "            is_int = starts.is_integer()\n",
    "        if is_int:\n",
    "            ends = starts + freq * periods\n",
    "            out = pl.int_ranges(starts, ends, freq, eager=True).explode()\n",
    "        else:\n",
    "            ends = offset_times(starts, freq, periods - 1)\n",
    "            if starts.dtype == pl.Date:\n",
    "                ranges_fn = pl.date_ranges\n",
    "            else:\n",
    "                ranges_fn = pl.datetime_ranges\n",
    "            out = ranges_fn(starts, ends, interval=freq, eager=True).explode()\n",
    "            out = _ensure_month_ends(out, starts, freq)\n",
    "        out = out.alias(starts.name)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a12c5-956d-4b13-969c-ab04d6f880d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetimes\n",
    "dates = pd.to_datetime(['2000-01-01', '2010-10-10'])\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='D', periods=3),\n",
    "    pd.Series(pd.to_datetime(['2000-01-01', '2000-01-02', '2000-01-03', '2010-10-10', '2010-10-11', '2010-10-12']))\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='2D', periods=3),\n",
    "    pd.Series(pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-05', '2010-10-10', '2010-10-12', '2010-10-14']))\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='4D', periods=3),\n",
    "    pd.Series(pd.to_datetime(['2000-01-01', '2000-01-05', '2000-01-09', '2010-10-10', '2010-10-14', '2010-10-18']))\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(pd.to_datetime(['2000-01-01', '2010-10-01']), freq=2 * pd.offsets.MonthBegin(), periods=2),\n",
    "    pd.Series(pd.to_datetime(['2000-01-01', '2000-03-01', '2010-10-01', '2010-12-01']))\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(pd.to_datetime(['2000-01-01', '2010-01-01']).tz_localize('US/Eastern'), freq=2 * pd.offsets.YearBegin(), periods=2),\n",
    "    pd.Series(pd.to_datetime(['2000-01-01', '2002-01-01', '2010-01-01', '2012-01-01']).tz_localize('US/Eastern'))\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(pd.to_datetime(['2000-12-31', '2010-12-31']), freq=2 * pd.offsets.YearEnd(), periods=2),\n",
    "    pd.Series(pd.to_datetime(['2000-12-31', '2002-12-31', '2010-12-31', '2012-12-31']))\n",
    ")\n",
    "# ints\n",
    "dates = pd.Series([1, 10])\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=1, periods=3),\n",
    "    pd.Series([1, 2, 3, 10, 11, 12])\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=2, periods=3),\n",
    "    pd.Series([1, 3, 5, 10, 12, 14])\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=4, periods=3),\n",
    "    pd.Series([1, 5, 9, 10, 14, 18])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d951f7c-424f-491f-8212-77e6c4a98b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "# datetimes\n",
    "dates = pl.Series([dt(2000, 1, 1), dt(2010, 10, 10)])\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='1d', periods=3),\n",
    "    pl.Series([dt(2000, 1, 1), dt(2000, 1, 2), dt(2000, 1, 3), dt(2010, 10, 10), dt(2010, 10, 11), dt(2010, 10, 12)])\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='2d', periods=3),\n",
    "    pl.Series([dt(2000, 1, 1), dt(2000, 1, 3), dt(2000, 1, 5), dt(2010, 10, 10), dt(2010, 10, 12), dt(2010, 10, 14)])\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='4d', periods=3),\n",
    "    pl.Series([dt(2000, 1, 1), dt(2000, 1, 5), dt(2000, 1, 9), dt(2010, 10, 10), dt(2010, 10, 14), dt(2010, 10, 18)])\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(pl.Series([dt(2010, 2, 28), dt(2000, 1, 31)]), '1mo', 3),\n",
    "    pl.Series([dt(2010, 2, 28), dt(2010, 3, 31), dt(2010, 4, 30), dt(2000, 1, 31), dt(2000, 2, 29), dt(2000, 3, 31)])\n",
    ")\n",
    "# dates\n",
    "dates = pl.Series([datetime.date(2000, 1, 1), datetime.date(2010, 10, 10)])\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq='1d', periods=2),\n",
    "    pl.Series([\n",
    "        datetime.date(2000, 1, 1), datetime.date(2000, 1, 2),\n",
    "        datetime.date(2010, 10, 10), datetime.date(2010, 10, 11),\n",
    "    ])\n",
    ")\n",
    "# ints\n",
    "dates = pl.Series([1, 10])\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=1, periods=3),\n",
    "    pl.Series([1, 2, 3, 10, 11, 12]),\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=2, periods=3),\n",
    "    pl.Series([1, 3, 5, 10, 12, 14]),\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    time_ranges(dates, freq=4, periods=3),\n",
    "    pl.Series([1, 5, 9, 10, 14, 18]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f9f98-6e60-43df-b1d1-1aa399ccd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def repeat(\n",
    "    s: Union[Series, pd.Index, np.ndarray],\n",
    "    n: Union[int, np.ndarray, Series]\n",
    ") -> Union[Series, pd.Index, np.ndarray]:\n",
    "    if isinstance(s, pl_Series):\n",
    "        if isinstance(n, np.ndarray):\n",
    "            n = pl_Series(n)\n",
    "        out = pl.DataFrame(s.alias('x')).select(\n",
    "            pl.col('x').repeat_by(n)\n",
    "        )['x'].explode().alias(s.name)\n",
    "    else:\n",
    "        out = np.repeat(s, n)\n",
    "        if isinstance(out, pd.Series):\n",
    "            out = out.reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329aa91-af7d-48fa-8454-6e97300316b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_index_equal(\n",
    "    repeat(pd.CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c']), 2),\n",
    "    pd.CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'c'], categories=['a', 'b', 'c'])\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    repeat(pd.Series([1, 2]), 2),\n",
    "    pd.Series([1, 1, 2, 2])\n",
    ")\n",
    "pd.testing.assert_series_equal(\n",
    "    repeat(pd.Series([1, 2]), pd.Series([2, 3])),\n",
    "    pd.Series([1, 1, 2, 2, 2]),\n",
    ")\n",
    "np.testing.assert_array_equal(\n",
    "    repeat(np.array([np.datetime64('2000-01-01'), np.datetime64('2010-10-10')]), 2),\n",
    "    np.array([\n",
    "        np.datetime64('2000-01-01'), np.datetime64('2000-01-01'),\n",
    "        np.datetime64('2010-10-10'), np.datetime64('2010-10-10')\n",
    "    ])\n",
    ")\n",
    "np.testing.assert_array_equal(\n",
    "    repeat(np.array([1, 2]), np.array([2, 3])),\n",
    "    np.array([1, 1, 2, 2, 2]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7904d-e565-4fd1-a105-9ecfbd7a2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "s = pl.Series(['a', 'b', 'c'], dtype=pl.Categorical)\n",
    "pl.testing.assert_series_equal(\n",
    "    repeat(s, 2),\n",
    "    pl.concat([s, s]).sort()\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    repeat(pl.Series([2, 4]), 2),\n",
    "    pl.Series([2, 2, 4, 4])\n",
    ")\n",
    "pl.testing.assert_series_equal(\n",
    "    repeat(pl.Series([1, 2]), np.array([2, 3])),\n",
    "    pl.Series([1, 1, 2, 2, 2]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf5b49-9f96-45da-8256-63c875651475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cv_times(\n",
    "    times: np.ndarray,\n",
    "    uids: Union[Series, pd.Index],\n",
    "    indptr: np.ndarray,\n",
    "    h: int,\n",
    "    test_size: int,\n",
    "    step_size: int,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    ") -> DataFrame:\n",
    "    if test_size < h:\n",
    "        raise ValueError('`test_size` should be greater or equal to `h`.')\n",
    "    n, resid = divmod(test_size - h, step_size)\n",
    "    if resid != 0:\n",
    "        raise ValueError('`test_size - h` should be a multiple `step_size`')\n",
    "    n_windows = n + 1\n",
    "    if isinstance(uids, pl_Series):\n",
    "        df_constructor = pl_DataFrame\n",
    "    else:\n",
    "        df_constructor = pd.DataFrame\n",
    "    sizes = np.diff(indptr)\n",
    "    out_times = []\n",
    "    out_cutoffs = []\n",
    "    out_ids = []\n",
    "    for i in range(n_windows):\n",
    "        offset = test_size - i * step_size + 1\n",
    "        use_series = sizes >= offset\n",
    "        cutoff_idxs = indptr[1:][use_series] - offset\n",
    "        valid_idxs = np.repeat(cutoff_idxs + 1, h) + np.tile(np.arange(h), cutoff_idxs.size)\n",
    "        out_times.append(times[valid_idxs])\n",
    "        out_cutoffs.append(np.repeat(times[cutoff_idxs], h))\n",
    "        if isinstance(uids, pl_Series):\n",
    "            use_series = pl_Series(use_series)\n",
    "        out_ids.append(repeat(filter_with_mask(uids, use_series), h))\n",
    "    return df_constructor(\n",
    "        {\n",
    "            id_col: vertical_concat(out_ids),\n",
    "            time_col: np.hstack(out_times),\n",
    "            'cutoff': np.hstack(out_cutoffs)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f4eb1-5c5c-4e02-a9e2-3734fb5f83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.arange(51, dtype=np.int64)\n",
    "uids = pd.Series(['id_0'])\n",
    "indptr = np.array([0, 51])\n",
    "h = 3\n",
    "test_size = 5\n",
    "actual = cv_times(\n",
    "    times=times,\n",
    "    uids=uids,\n",
    "    indptr=indptr,\n",
    "    h=h,\n",
    "    test_size=test_size,\n",
    "    step_size=1,\n",
    ")\n",
    "expected = pd.DataFrame({\n",
    "    'unique_id': 9 * ['id_0'],\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [47, 48, 49],\n",
    "        [48, 49, 50]\n",
    "    ], dtype=np.int64),\n",
    "    'cutoff': np.repeat(np.array([45, 46, 47], dtype=np.int64), h),\n",
    "})\n",
    "pd.testing.assert_frame_equal(actual, expected)\n",
    "\n",
    "# step_size=2\n",
    "actual = cv_times(\n",
    "    times=times,\n",
    "    uids=uids,\n",
    "    indptr=indptr,\n",
    "    h=h,\n",
    "    test_size=test_size,\n",
    "    step_size=2,\n",
    ")\n",
    "expected = pd.DataFrame({\n",
    "    'unique_id': 6 * ['id_0'],\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [48, 49, 50]\n",
    "    ], dtype=np.int64),\n",
    "    'cutoff': np.repeat(np.array([45, 47], dtype=np.int64), h)\n",
    "})\n",
    "pd.testing.assert_frame_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c81ccf-4655-4cee-8940-004633d87f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def group_by(df: Union[Series, DataFrame], by, maintain_order=False):\n",
    "    if isinstance(df, (pd.Series, pd.DataFrame)):\n",
    "        out = df.groupby(by, observed=True, sort=not maintain_order)\n",
    "    else:\n",
    "        if isinstance(df, pl_Series):\n",
    "            df = df.to_frame()\n",
    "        try:\n",
    "            out = df.group_by(by, maintain_order=maintain_order)\n",
    "        except AttributeError:\n",
    "            out = df.groupby(by, maintain_order=maintain_order)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3ff2c-9e70-46b3-9bf0-bbfcd339d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def group_by_agg(df: DataFrame, by, aggs, maintain_order=False) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        out = group_by(df, by, maintain_order).agg(aggs).reset_index()\n",
    "    else:\n",
    "        out = group_by(df, by, maintain_order).agg(*[getattr(pl.col(c), agg)() for c, agg in aggs.items()])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f92cd4-d3f7-4de1-b438-c3c5891c3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    group_by_agg(pd.DataFrame({'x': [1, 1, 2], 'y': [1, 1, 1]}), 'x', {'y': 'sum'}),\n",
    "    pd.DataFrame({'x': [1, 2], 'y': [2, 1]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cfc66-a218-498e-b674-96491f47a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd.testing.assert_frame_equal(\n",
    "    group_by_agg(pl.DataFrame({'x': [1, 1, 2], 'y': [1, 1, 1]}), 'x', {'y': 'sum'}, maintain_order=True).to_pandas(),\n",
    "    pd.DataFrame({'x': [1, 2], 'y': [2, 1]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d909cbb-dbb3-4860-af16-6f1dd399297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_in(s: Series, collection) -> Series:\n",
    "    if isinstance(s, pl_Series):\n",
    "        out = s.is_in(collection)\n",
    "    else:\n",
    "        out = s.isin(collection)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3c95d-9112-48c0-a104-c725a2390bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(is_in(pd.Series([1, 2, 3]), [1]), np.array([True, False, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d522a-a806-44ea-9c99-70059515ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "np.testing.assert_equal(is_in(pl.Series([1, 2, 3]), [1]), np.array([True, False, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717022e-2c6f-47dc-8b19-da069341b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def between(s: Series, lower: Series, upper: Series) -> Series:\n",
    "    if isinstance(s, pd.Series):\n",
    "        out = s.between(lower, upper)\n",
    "    else:\n",
    "        out = s.is_between(lower, upper)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca138b4-e771-4b8e-aa54-35dc37802d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(\n",
    "    between(pd.Series([1, 2, 3]), pd.Series([0, 1, 4]), pd.Series([4, 1, 2])),\n",
    "    np.array([True, False, False]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c773bf-fe23-4428-84f6-c5afaefdad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "np.testing.assert_equal(\n",
    "    between(pl.Series([1, 2, 3]), pl.Series([0, 1, 4]), pl.Series([4, 1, 2])),\n",
    "    np.array([True, False, False]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667302bf-3b54-4298-8fcc-82cd6b12fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fill_null(df: DataFrame, mapping: Dict[str, Any]) -> DataFrame:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        out = df.fillna(mapping)\n",
    "    else:\n",
    "        out = df.with_columns(*[pl.col(col).fill_null(v) for col, v in mapping.items()])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74993c58-0886-4290-ab90-8065651886c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    fill_null(pd.DataFrame({'x': [1, np.nan], 'y': [np.nan, 2]}), {'x': 2, 'y': 1}),\n",
    "    pd.DataFrame({'x': [1, 2], 'y': [1, 2]}, dtype='float64')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d835a-f1dc-4c1a-be2d-e7dd5b9895ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pl.testing.assert_frame_equal(\n",
    "    fill_null(pl.DataFrame({'x': [1, None], 'y': [None, 2]}), {'x': 2, 'y': 1}),\n",
    "    pl.DataFrame({'x': [1, 2], 'y': [1, 2]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d5993-1af2-4eb5-a60e-4cefa07624ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cast(s: Series, dtype: type) -> Series:\n",
    "    if isinstance(s, pd.Series):\n",
    "        s = s.astype(dtype)\n",
    "    else:\n",
    "        s = s.cast(dtype)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb4cb1-87c7-4464-b09a-522d14b9896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_series_equal(\n",
    "    cast(pd.Series([1, 2, 3]), 'int16'),\n",
    "    pd.Series([1, 2, 3], dtype='int16')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8bf34-a8be-4e10-a239-f8357cf5fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd.testing.assert_series_equal(\n",
    "    cast(pl.Series('x', [1, 2, 3]), pl.Int16).to_pandas(),\n",
    "    pd.Series([1, 2, 3], name='x', dtype='int16')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfc2f3-f571-4ea5-91aa-d3e4cc1a1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def value_cols_to_numpy(\n",
    "    df: DataFrame, id_col: str, time_col: str, target_col: Optional[str]\n",
    ") -> np.ndarray:\n",
    "    exclude_cols = [id_col, time_col]\n",
    "    if target_col is not None:\n",
    "        exclude_cols.append(target_col)\n",
    "    value_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    if target_col is not None:\n",
    "        value_cols = [target_col, *value_cols]\n",
    "    data = to_numpy(df[value_cols])\n",
    "    if data.dtype not in (np.float32, np.float64):\n",
    "        data = data.astype(np.float32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ca8ad-b6a1-4068-95e0-47622dfdd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_future_dataframe(\n",
    "    uids: Series,\n",
    "    last_times: Union[Series, pd.Index],\n",
    "    freq: Union[int, str, BaseOffset],\n",
    "    h: int,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds'\n",
    ") -> DataFrame:\n",
    "    starts = offset_times(last_times, freq, 1)\n",
    "    if isinstance(uids, pl_Series):\n",
    "        df_constructor = pl_DataFrame\n",
    "    else:\n",
    "        df_constructor = pd.DataFrame\n",
    "    return df_constructor(\n",
    "        {\n",
    "            id_col: repeat(uids, h),\n",
    "            time_col: time_ranges(starts, freq=freq, periods=h),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e79f31-3a71-4a77-9307-9e2faa26ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    make_future_dataframe(\n",
    "        pd.Series([1, 2]), pd.to_datetime(['2000-01-01', '2010-10-10']), freq='D', h=2\n",
    "    ),\n",
    "    pd.DataFrame({\n",
    "        'unique_id': [1, 1, 2, 2],\n",
    "        'ds': pd.to_datetime(['2000-01-02', '2000-01-03', '2010-10-11', '2010-10-12'])\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea8d85-d3cb-4d50-a49e-2c4f4d3839dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pl.testing.assert_frame_equal(\n",
    "    make_future_dataframe(\n",
    "        pl.Series([1, 2]),\n",
    "        pl.Series([dt(2000, 1, 1), dt(2010, 10, 10)]),\n",
    "        freq='1d',\n",
    "        h=2,\n",
    "        id_col='uid',\n",
    "        time_col='dates',\n",
    "    ),\n",
    "    pl.DataFrame({\n",
    "        'uid': [1, 1, 2, 2],\n",
    "        'dates': [dt(2000, 1, 2), dt(2000, 1, 3), dt(2010, 10, 11), dt(2010, 10, 12)]\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59040953-1fbe-444a-b16c-d0ed798c9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def anti_join(df1: DataFrame, df2: DataFrame, on: Union[str, List[str]]) -> DataFrame:\n",
    "    if isinstance(df1, pd.DataFrame) and isinstance(df2, pd.DataFrame):\n",
    "        out = df1.merge(df2, on=on, how='left', indicator=True)\n",
    "        out = out[out['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "        out = out.reset_index(drop=True)\n",
    "    elif isinstance(df1, pl_DataFrame) and isinstance(df2, pl_DataFrame):\n",
    "        out = join(df1, df2, on=on, how='anti')\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'df1 and df2 must be pandas or polars dataframes of the same type. '\n",
    "            f\"Got type(df1): '{type(df1)}', type(df2): '{type(df2)}'\"\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93939c2-ef45-4e62-bb34-8cdbc58e3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(\n",
    "    anti_join(pd.DataFrame({'x': [1, 2]}), pd.DataFrame({'x': [1]}), on='x'),\n",
    "    pd.DataFrame({'x': [2]})\n",
    ")\n",
    "test_eq(\n",
    "    anti_join(pd.DataFrame({'x': [1]}), pd.DataFrame({'x': [1]}), on='x').shape[0],\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf656584-b459-4c7a-848e-46970dd1cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pl.testing.assert_frame_equal(\n",
    "    anti_join(pl_DataFrame({'x': [1, 2]}), pl_DataFrame({'x': [1]}), on='x'),\n",
    "    pl_DataFrame({'x': [2]})\n",
    ")\n",
    "test_eq(\n",
    "    anti_join(pl_DataFrame({'x': [1]}), pl_DataFrame({'x': [1]}), on='x').shape[0],\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b0a8a-98b1-486b-9414-1f5ef698f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ensure_sorted(df: DataFrame, id_col: str, time_col: str) -> DataFrame:\n",
    "    sort_idxs = maybe_compute_sort_indices(df=df, id_col=id_col, time_col=time_col)\n",
    "    if sort_idxs is not None:\n",
    "        df = take_rows(df=df, idxs=sort_idxs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de12264-0bd1-4eed-935b-7b7fb1cbebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _ProcessedDF(NamedTuple):\n",
    "    uids: Series\n",
    "    times: np.ndarray\n",
    "    data: np.ndarray\n",
    "    indptr: np.ndarray\n",
    "    sort_idxs: Optional[np.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62293bd2-b921-40b2-b1af-25f0b8e55006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_df(\n",
    "    df: DataFrame,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: Optional[str],\n",
    ") -> _ProcessedDF:\n",
    "    \"\"\"Extract components from dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times and target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids : pandas or polars Serie\n",
    "        serie with the sorted unique ids present in the data.\n",
    "    last_times : numpy array\n",
    "        array with the last time for each serie.\n",
    "    data : numpy ndarray\n",
    "        2d array with target plus features values.\n",
    "    indptr : numpy ndarray\n",
    "        1d array with indices to the start and end of each serie.\n",
    "    sort_idxs : numpy array or None\n",
    "        array with the indices that would sort the original data.\n",
    "        If the data is already sorted this is `None`.            \n",
    "    \"\"\"\n",
    "    # validations\n",
    "    validate_format(df, id_col, time_col, target_col)\n",
    "\n",
    "    # ids\n",
    "    id_counts = counts_by_id(df, id_col)\n",
    "    uids = id_counts[id_col]\n",
    "\n",
    "    # indices\n",
    "    sizes = id_counts['counts'].to_numpy()\n",
    "    indptr = np.append(0, sizes.cumsum()).astype(np.int32)\n",
    "    last_idxs = indptr[1:] - 1\n",
    "\n",
    "    # data\n",
    "    data = value_cols_to_numpy(df, id_col, time_col, target_col)\n",
    "\n",
    "    # check if we need to sort\n",
    "    sort_idxs = maybe_compute_sort_indices(df, id_col, time_col)\n",
    "    if sort_idxs is not None:\n",
    "        data = data[sort_idxs]\n",
    "        last_idxs = sort_idxs[last_idxs]\n",
    "    times = df[time_col].to_numpy()[last_idxs]\n",
    "    return _ProcessedDF(uids, times, data, indptr, sort_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227bb19-67cc-45cc-8972-395aef7cb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "horizon = 3\n",
    "test_size = 5\n",
    "for equal_ends in [True, False]:\n",
    "    n_series = 2\n",
    "    series = generate_series(n_series, equal_ends=equal_ends)\n",
    "    freq = pd.tseries.frequencies.to_offset('D')\n",
    "    uids, last_times, data, indptr, _ = process_df(series, 'unique_id', 'ds', 'y')\n",
    "    times = series['ds'].to_numpy()\n",
    "    df_dates = cv_times(\n",
    "        times=times,\n",
    "        uids=uids,\n",
    "        indptr=indptr,\n",
    "        h=horizon,\n",
    "        test_size=test_size,\n",
    "        step_size=1\n",
    "    )\n",
    "    test_eq(len(df_dates), n_series * horizon * (test_size - horizon + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd1f31-05f4-4bda-9599-8f8922233c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataFrameProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ):\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        df: DataFrame\n",
    "    ) -> Tuple[Series, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:\n",
    "        return process_df(df, self.id_col, self.time_col, self.target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2dee9-d54d-4ff9-9279-b91fa4968758",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = ['static_0', 'static_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7661-0c1a-43ef-a88e-da6288cdac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_static_features in [0, 2]:\n",
    "    series_pd = generate_series(1_000, n_static_features=n_static_features, equal_ends=False, engine='pandas')\n",
    "    for i in range(n_static_features):\n",
    "        series_pd[f'static_{i}'] = series_pd[f'static_{i}'].map(lambda x: f'x_{x}').astype('category')\n",
    "    scrambled_series_pd = series_pd.sample(frac=1.0)\n",
    "    dfp = DataFrameProcessor('unique_id', 'ds', 'y')\n",
    "    uids, times, data, indptr, _ = dfp.process(scrambled_series_pd)\n",
    "    test_eq(times, series_pd.groupby('unique_id', observed=True)['ds'].max().values)\n",
    "    test_eq(uids, np.sort(series_pd['unique_id'].unique()))\n",
    "    for i in range(n_static_features):\n",
    "        series_pd[f'static_{i}'] = series_pd[f'static_{i}'].cat.codes\n",
    "    test_eq(data, series_pd[['y'] + static_features[:n_static_features]].to_numpy())\n",
    "    test_eq(np.diff(indptr), series_pd.groupby('unique_id', observed=True).size().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de463ecf-31f1-4050-a4e7-a08211a1f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test process_df with target_col=None\n",
    "series_pd = generate_series(10, n_static_features=2, equal_ends=False, engine='pandas')\n",
    "series_pd = series_pd.rename(columns={'y': 'exog_0'})\n",
    "_, _, data, indptr, _ = process_df(series_pd, 'unique_id', 'ds', None)\n",
    "np.testing.assert_allclose(\n",
    "    data,\n",
    "    to_numpy(series_pd.drop(columns=['unique_id', 'ds'])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c959fc-206e-4eec-a61b-76de3856468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "for n_static_features in [0, 2]:\n",
    "    series_pl = generate_series(1_000, n_static_features=n_static_features, equal_ends=False, engine='polars')\n",
    "    scrambled_series_pl = series_pl.sample(fraction=1.0, shuffle=True)\n",
    "    dfp = DataFrameProcessor('unique_id', 'ds', 'y')\n",
    "    uids, times, data, indptr, _ = dfp.process(scrambled_series_pl)\n",
    "    grouped = group_by(series_pl, 'unique_id')\n",
    "    test_eq(times, grouped.agg(pl.col('ds').max()).sort('unique_id')['ds'].to_numpy())\n",
    "    test_eq(uids, series_pl['unique_id'].unique().sort())\n",
    "    test_eq(data, series_pl.select(pl.col(c).map_batches(lambda s: s.to_physical()) for c in ['y'] + static_features[:n_static_features]).to_numpy())\n",
    "    test_eq(np.diff(indptr), grouped.count().sort('unique_id')['count'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc6aa4-ce39-4559-9964-01c06b7d7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _single_split(\n",
    "    df: DataFrame,\n",
    "    i_window: int,    \n",
    "    n_windows: int,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    freq: Union[int, str, pd.offsets.BaseOffset],\n",
    "    max_dates: Series,  \n",
    "    step_size: Optional[int] = None,\n",
    "    input_size: Optional[int] = None,\n",
    ") -> Tuple[DataFrame, Series, Series]:\n",
    "    if step_size is None:\n",
    "        step_size = h\n",
    "    test_size = h + step_size * (n_windows - 1)\n",
    "    offset = test_size - i_window * step_size\n",
    "    train_ends = offset_times(max_dates, freq, -offset)\n",
    "    valid_ends = offset_times(train_ends, freq, h)\n",
    "    train_mask = df[time_col].le(train_ends)\n",
    "    valid_mask = df[time_col].gt(train_ends) & df[time_col].le(valid_ends)    \n",
    "    if input_size is not None:\n",
    "        train_starts = offset_times(train_ends, freq, -input_size)\n",
    "        train_mask &= df[time_col].gt(train_starts)\n",
    "    if isinstance(train_mask, pd.Series):\n",
    "        train_sizes = train_mask.groupby(df[id_col], observed=True, sort=False).sum()\n",
    "        train_sizes = train_sizes.reset_index()\n",
    "    else:\n",
    "        tmp_df = pl.DataFrame({id_col: df[id_col], time_col: train_mask})\n",
    "        train_sizes = group_by_agg(tmp_df, id_col, {time_col: 'sum'}, maintain_order=True)\n",
    "    zeros_mask = train_sizes[time_col].eq(0)\n",
    "    if zeros_mask.all():\n",
    "        raise ValueError(\n",
    "            'All series are too short for the cross validation settings, '\n",
    "            f'at least {offset + 1} samples are required.\\n'\n",
    "            'Please reduce `n_windows` or `h`.'\n",
    "        )\n",
    "    elif zeros_mask.any():\n",
    "        ids = filter_with_mask(train_sizes[id_col], zeros_mask)\n",
    "        warnings.warn(\n",
    "            'The following series are too short for the window '\n",
    "            f'and will be dropped: {reprlib.repr(list(ids))}'\n",
    "        )\n",
    "        dropped_ids = is_in(df[id_col], ids)\n",
    "        valid_mask &= ~dropped_ids\n",
    "    if isinstance(train_ends, pd.Series):\n",
    "        cutoffs: DataFrame = (\n",
    "            train_ends\n",
    "            .set_axis(df[id_col])\n",
    "            .groupby(id_col, observed=True)\n",
    "            .head(1)\n",
    "            .rename(\"cutoff\")\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        cutoffs = train_ends.to_frame().with_columns(df[id_col])\n",
    "        cutoffs = (\n",
    "            group_by(cutoffs, id_col)\n",
    "            .agg(pl.col(time_col).head(1))\n",
    "            .explode(pl.col(time_col))\n",
    "            .rename({time_col: 'cutoff'})\n",
    "        )\n",
    "    return cutoffs, train_mask, valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3370a-9a55-4436-9326-b459d03525dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def backtest_splits(\n",
    "    df: DataFrame,\n",
    "    n_windows: int,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    freq: Union[int, str, pd.offsets.BaseOffset],\n",
    "    step_size: Optional[int] = None,\n",
    "    input_size: Optional[int] = None,\n",
    ") -> Generator[Tuple[DataFrame, DataFrame, DataFrame], None, None]:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        max_dates = df.groupby(id_col, observed=True)[time_col].transform('max')\n",
    "    else:\n",
    "        max_dates = df.select(pl.col(time_col).max().over(id_col))[time_col]\n",
    "    for i in range(n_windows):\n",
    "        cutoffs, train_mask, valid_mask = _single_split(\n",
    "            df,\n",
    "            i_window=i,\n",
    "            n_windows=n_windows,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            freq=freq,\n",
    "            max_dates=max_dates,\n",
    "            step_size=step_size,\n",
    "            input_size=input_size,\n",
    "        )\n",
    "        train = filter_with_mask(df, train_mask)\n",
    "        valid = filter_with_mask(df, valid_mask)\n",
    "        yield cutoffs, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ef1ca-418c-4506-990f-0502481c6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "short_series = generate_series(100, max_length=50)\n",
    "backtest_results = list(\n",
    "    backtest_splits(\n",
    "        short_series,\n",
    "        n_windows=1,\n",
    "        h=49,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=pd.offsets.Day(),\n",
    "    )\n",
    ")[0]\n",
    "test_fail(\n",
    "    lambda: list(\n",
    "        backtest_splits(\n",
    "            short_series,\n",
    "            n_windows=1,\n",
    "            h=50,\n",
    "            id_col='unique_id',\n",
    "            time_col='ds',\n",
    "            freq=pd.offsets.Day(),\n",
    "        )\n",
    "    ),\n",
    "    contains='at least 51 samples are required'\n",
    ")\n",
    "some_short_series = generate_series(100, min_length=20, max_length=100)\n",
    "with warnings.catch_warnings(record=True) as issued_warnings:\n",
    "    warnings.simplefilter('always', UserWarning)\n",
    "    splits = list(\n",
    "        backtest_splits(\n",
    "            some_short_series,\n",
    "            n_windows=1,\n",
    "            h=50,\n",
    "            id_col='unique_id',\n",
    "            time_col='ds',\n",
    "            freq=pd.offsets.Day(),\n",
    "        )\n",
    "    )\n",
    "    assert any('will be dropped' in str(w.message) for w in issued_warnings)\n",
    "short_series_int = short_series.copy()\n",
    "short_series_int['ds'] = short_series.groupby('unique_id', observed=True).transform('cumcount')\n",
    "backtest_int_results = list(\n",
    "    backtest_splits(\n",
    "        short_series_int,\n",
    "        n_windows=1,\n",
    "        h=40,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=1\n",
    "    )\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecf3fc-0354-4d3c-82ac-39929e50a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_backtest_splits(df, n_windows, h, step_size, input_size):\n",
    "    max_dates = df.groupby('unique_id', observed=True)['ds'].max()\n",
    "    day_offset = pd.offsets.Day()    \n",
    "    common_kwargs = dict(\n",
    "        n_windows=n_windows,\n",
    "        h=h,\n",
    "        id_col='unique_id',\n",
    "        time_col='ds',\n",
    "        freq=pd.offsets.Day(), \n",
    "        step_size=step_size,\n",
    "        input_size=input_size,        \n",
    "    )\n",
    "    permuted_df = df.sample(frac=1.0)\n",
    "    splits = backtest_splits(df, **common_kwargs)\n",
    "    splits_on_permuted = list(backtest_splits(permuted_df, **common_kwargs))\n",
    "    if step_size is None:\n",
    "        step_size = h\n",
    "    test_size = h + step_size * (n_windows - 1)\n",
    "    for window, (cutoffs, train, valid) in enumerate(splits):\n",
    "        offset = test_size - window * step_size\n",
    "        expected_max_train_dates = max_dates - day_offset * offset\n",
    "        max_train_dates = train.groupby('unique_id', observed=True)['ds'].max()\n",
    "        pd.testing.assert_series_equal(max_train_dates, expected_max_train_dates)\n",
    "        pd.testing.assert_frame_equal(cutoffs, max_train_dates.rename('cutoff').reset_index())\n",
    "        \n",
    "        if input_size is not None:\n",
    "            expected_min_train_dates = expected_max_train_dates - day_offset * (input_size - 1)\n",
    "            min_train_dates = train.groupby('unique_id', observed=True)['ds'].min()\n",
    "            pd.testing.assert_series_equal(min_train_dates, expected_min_train_dates)\n",
    "\n",
    "        expected_min_valid_dates = expected_max_train_dates + day_offset\n",
    "        min_valid_dates = valid.groupby('unique_id', observed=True)['ds'].min()\n",
    "        pd.testing.assert_series_equal(min_valid_dates, expected_min_valid_dates)\n",
    "\n",
    "        expected_max_valid_dates = expected_max_train_dates + day_offset * h\n",
    "        max_valid_dates = valid.groupby('unique_id', observed=True)['ds'].max()\n",
    "        pd.testing.assert_series_equal(max_valid_dates, expected_max_valid_dates)\n",
    "\n",
    "        if window == n_windows - 1:\n",
    "            pd.testing.assert_series_equal(max_valid_dates, max_dates)\n",
    "\n",
    "        _, permuted_train, permuted_valid = splits_on_permuted[window]            \n",
    "        pd.testing.assert_frame_equal(train, permuted_train.sort_values(['unique_id', 'ds']))\n",
    "    pd.testing.assert_frame_equal(valid, permuted_valid.sort_values(['unique_id', 'ds']))\n",
    "\n",
    "n_series = 20\n",
    "min_length = 100\n",
    "max_length = 1000\n",
    "series = generate_series(n_series, freq='D', min_length=min_length, max_length=max_length)\n",
    "\n",
    "for step_size in (None, 1, 2):\n",
    "    for input_size in (None, 4):\n",
    "        test_backtest_splits(series, n_windows=3, h=14, step_size=step_size, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a828df0-d7dc-4694-9aea-8abaec447acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "h = 10\n",
    "series_pl = generate_series(n_series, freq='D', min_length=min_length, max_length=max_length, engine='polars')\n",
    "splits = backtest_splits(series_pl, n_windows=3, h=h, id_col='unique_id', time_col='ds', freq='1d')\n",
    "for cutoffs, train, valid in splits:\n",
    "    train_ends = train.group_by('unique_id', maintain_order=True).agg(pl.col('ds').max())\n",
    "    valid_starts = valid.group_by('unique_id', maintain_order=True).agg(pl.col('ds').min())\n",
    "    valid_ends = valid.group_by('unique_id', maintain_order=True).agg(pl.col('ds').max())\n",
    "    expected_valid_starts = offset_times(train_ends['ds'], '1d', 1)\n",
    "    expected_valid_ends = offset_times(train_ends['ds'], '1d', h)\n",
    "    pl.testing.assert_series_equal(valid_starts['ds'], expected_valid_starts)\n",
    "    pl.testing.assert_series_equal(valid_ends['ds'], expected_valid_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7784f1b-1f6d-4848-95c0-f7d7a496939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_insample_levels(\n",
    "    df: DataFrame,\n",
    "    models: List[str],\n",
    "    level: List[Union[int, float]],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DataFrame:\n",
    "    import operator\n",
    "\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    df = copy_if_pandas(df, deep=False)\n",
    "    cuts = norm.ppf(0.5 + np.asarray(level) / 200).reshape(1, -1)\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        errors = df[models].sub(df[target_col], axis=0)\n",
    "        stds = errors.groupby(df[id_col], observed=True).transform('std')\n",
    "    else:\n",
    "        exprs = (pl.col(m).sub(pl.col(target_col)).std().over(id_col) for m in models)\n",
    "        stds = df.select(exprs)\n",
    "    stds = to_numpy(stds)\n",
    "    preds = to_numpy(df[models])\n",
    "    vals = np.empty_like(preds, shape=(preds.shape[0], len(models) * 2 * len(level)))\n",
    "    cols = []\n",
    "    k = 0\n",
    "    for i, model in enumerate(models):\n",
    "        widths = cuts * stds[:, [i]]\n",
    "        for side, op in {'lo': operator.sub, 'hi': operator.add}.items():\n",
    "            for j, lvl in enumerate(level):        \n",
    "                cols.append(f'{model}-{side}-{lvl}')\n",
    "                vals[:, k] = op(preds[:, i], widths[:, j])\n",
    "                k += 1\n",
    "    return assign_columns(df, cols, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd9374-8ae7-47e0-ac77-9a50cb43c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| scipy\n",
    "series = generate_series(100, n_models=2)\n",
    "models = ['model0', 'model1']\n",
    "levels = [80, 95]\n",
    "with_levels = add_insample_levels(series, models, levels)\n",
    "for model in models:\n",
    "    for lvl in levels:\n",
    "        assert with_levels[f'{model}-lo-{lvl}'].lt(with_levels[f'{model}-hi-{lvl}']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cfd9e-9ea7-498a-9634-dd2183cf34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "#| scipy\n",
    "series_pl = generate_series(100, n_models=2, engine='polars')\n",
    "with_levels_pl = add_insample_levels(series_pl, ['model0', 'model1'], [80, 95])\n",
    "pd.testing.assert_frame_equal(\n",
    "    with_levels.drop(columns='unique_id'),\n",
    "    with_levels_pl.to_pandas().drop(columns='unique_id')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
