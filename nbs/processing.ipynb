{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9284ac-4d6c-42fc-8d05-13ff8fa7460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96e02b-4e1b-4284-8808-58c7dbb1fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dfc5b-5bd4-44b6-aac4-012cd326010c",
   "metadata": {},
   "source": [
    "# Processing\n",
    "> DataFrame processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133caf6-09b0-4de3-b168-b7cff92040a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utilsforecast.compat import DataFrame, pl_DataFrame\n",
    "from utilsforecast.grouped_array import GroupedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bddba-f31e-4daf-9aee-1513e212c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e56ea9-8f56-4a49-85f5-2d739c7f989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameProcessing:\n",
    "    \"\"\"\n",
    "    A utility to process Pandas or Polars dataframes for time series forecasting.\n",
    "\n",
    "    This class ensures the dataframe is properly structured, with required columns\n",
    "    ('unique_id', 'ds', 'y'), and the 'ds' column is of datetime type. It also\n",
    "    provides options for sorting the dataframe based on a unique identifier and a\n",
    "    timestamp, and separates the data into different arrays for easy access during\n",
    "    forecasting operations.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame or pl.DataFrame\n",
    "        A pandas or polars dataframe to be processed.\n",
    "    sort_dataframe : bool\n",
    "        A boolean indicating whether the dataframe should be sorted.\n",
    "    validate : bool, (default=True)\n",
    "        Ensure the dataframe matches the required format.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    __call__():\n",
    "        Processes the dataframe by ensuring the columns are in the correct format,\n",
    "        sorts the dataframe if required, and separates the data into different\n",
    "        arrays for future operations.\n",
    "    _to_np_and_engine():\n",
    "        Converts the dataframe to a numpy structured array and identifies the\n",
    "        dataframe engine (pandas or polars).\n",
    "    _validate_dataframe(dataframe: Union[pd.DataFrame, pl.DataFrame]):\n",
    "        Checks if the required columns ('unique_id', 'ds', 'y') are present in the\n",
    "        dataframe.\n",
    "    _check_datetime(arr: np.array) -> np.array:\n",
    "        Validates that the 'ds' column is of datetime type, and if not, attempts to\n",
    "        convert it to datetime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: DataFrame,\n",
    "        sort_dataframe: bool,\n",
    "        validate: bool = True,\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.sort_dataframe = sort_dataframe\n",
    "        self.validate = validate\n",
    "\n",
    "        # Columns declaration\n",
    "        self.non_value_columns = [\"unique_id\", \"ds\"]\n",
    "        self.datetime_column_name = \"ds\"\n",
    "        self.dt_dtype = np.dtype(\"datetime64\")\n",
    "        self.__call__()\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Sequential execution of the code\"\"\"\n",
    "        # Declaring values that will be utilized\n",
    "        self.np_df = self._to_np_and_engine()\n",
    "        self.dataframe_columns = self.np_df.dtype.names\n",
    "\n",
    "        # Processing value columns\n",
    "        value_columns = [\n",
    "            column\n",
    "            for column in self.dataframe_columns\n",
    "            if column not in self.non_value_columns\n",
    "        ]\n",
    "        self.value_array = self.np_df[value_columns]\n",
    "        if self.value_array.ndim == 1 and len(value_columns) > 1:\n",
    "            self.value_array = np.stack(\n",
    "                [\n",
    "                    self.value_array[name].astype(float)\n",
    "                    for name in self.value_array.dtype.names\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        if self.value_array.ndim == 1 and len(value_columns) == 1:\n",
    "            self.value_array = (\n",
    "                self.value_array[value_columns].astype(float).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "        # Processing unique_id\n",
    "        self.unique_id = self.np_df[\"unique_id\"]\n",
    "        if self.unique_id.dtype.kind == \"O\":\n",
    "            self.unique_id.astype(str)\n",
    "\n",
    "        # If values are already int or float then they won't be converted\n",
    "        if self.unique_id.dtype.kind not in [\"i\", \"f\"]:\n",
    "            # If all values in the numpy array are numerical then proceed with conversion\n",
    "            if np.char.isnumeric(self.unique_id.astype(str)).all():\n",
    "                # If number are whole then they will be converted to `int`, else `float`\n",
    "                # This is pure aesthetics addition.\n",
    "                self.unique_id = self.unique_id.astype(float)\n",
    "                if np.isclose(self.unique_id, np.round(self.unique_id)).all():\n",
    "                    self.unique_id = self.unique_id.astype(int)\n",
    "        # NOTE: When sorting with Numpy, character values may be prioritized over numerical values if the data\n",
    "        # type is set to 'object'. For instance, the value '10' would come before '3' because it contains '1' and '0'\n",
    "        # at the beginning. One solution to this problem is to convert the data to 'float' if it is numerical.\n",
    "        unique_id_count = pd.Series(self.unique_id).value_counts(sort=False)\n",
    "        self.indices, sizes = unique_id_count.index, unique_id_count.values\n",
    "        cum_sizes = np.cumsum(sizes)\n",
    "\n",
    "        # Processing datestamp\n",
    "        self.dates = self.np_df[self.datetime_column_name]\n",
    "        if self.engine_dataframe == pd.DataFrame:\n",
    "            self.dates = self.dataframe.index.get_level_values(\n",
    "                self.datetime_column_name\n",
    "            )\n",
    "        self.dates = self.dates[cum_sizes - 1]\n",
    "        self.indptr = np.append(0, cum_sizes).astype(np.int32)\n",
    "\n",
    "        # Index that will be used by pandas, not polars\n",
    "        self.index = pd.MultiIndex.from_arrays(\n",
    "            [\n",
    "                self.np_df[\"unique_id\"],\n",
    "                self.np_df[\"ds\"],\n",
    "            ],\n",
    "            names=[\"unique_id\", \"ds\"],\n",
    "        )\n",
    "\n",
    "    def grouped_array(self):\n",
    "        return GroupedArray(self.value_array, self.indptr)\n",
    "\n",
    "    def _to_np_and_engine(self):\n",
    "        \"\"\"\n",
    "        This function will be utilised to convert DataFrame to dictionary.\n",
    "\n",
    "        Returns:\n",
    "            tuple[pd.DataFrame or pl.DataFrame, dict]: the engine that will be used to construct\n",
    "                the output DataFrame and dictionary of DataFrame values\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If DataFrame engine is not supported and/or accounted for.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################\n",
    "        # Polars DataFrame #\n",
    "        ####################\n",
    "        if isinstance(self.dataframe, pl_DataFrame):\n",
    "            from packaging.version import Version\n",
    "\n",
    "            import polars as pl\n",
    "\n",
    "            # Ensure that all required columns are present in the DataFrame:\n",
    "            self.engine_dataframe = pl_DataFrame\n",
    "            if self.validate:\n",
    "                self._validate_dataframe(self.dataframe)\n",
    "            elif self.validate == False:\n",
    "                self._partial_val_df(self.dataframe)\n",
    "\n",
    "            # datetime check\n",
    "            dt_arr = self.dataframe[\"ds\"].to_numpy()\n",
    "            processed_dt_arr = self._check_datetime(dt_arr)\n",
    "            if type(dt_arr) != type(processed_dt_arr):\n",
    "                self.dataframe = self.datafraFme.with_columns(\n",
    "                    pl.from_numpy(processed_dt_arr.to_numpy(), schema=[\"ds\"])\n",
    "                )\n",
    "\n",
    "            sample_index_df = self.dataframe[self.non_value_columns]\n",
    "            sorted_index_df = sample_index_df.sort(self.non_value_columns)\n",
    "            is_monotonic_increasing = sample_index_df.frame_equal(sorted_index_df)\n",
    "\n",
    "            # Sorting will be performed if sort is set to true and values are unsorted\n",
    "            if not is_monotonic_increasing and self.sort_dataframe:\n",
    "                self.dataframe = self.dataframe.sort(self.non_value_columns)\n",
    "\n",
    "            # resources: https://github.com/pola-rs/polars/blob/4fca1ae51864f74e0367d8bc91b4a2db00e54174/py-polars/polars/dataframe/frame.py#L1975\n",
    "            # resources: https://numpy.org/doc/stable/user/basics.rec.html\n",
    "            # resources: https://numpy.org/doc/stable/reference/generated/numpy.core.records.fromarrays.html\n",
    "            # NOTE: Structured array is not available in polars under the version 0.17.12\n",
    "            pl_version = Version(pl.__version__)\n",
    "            min_pl_v = Version(\"0.17.12\")\n",
    "            if pl_version >= min_pl_v:\n",
    "                return self.dataframe.to_numpy(structured=True)\n",
    "            else:\n",
    "                arrays = []\n",
    "                for column, column_dtype in self.dataframe.schema.items():\n",
    "                    ser = self.dataframe[column]\n",
    "                    arr = ser.to_numpy()\n",
    "                    arrays.append(\n",
    "                        arr.astype(str, copy=False)\n",
    "                        if str(column_dtype) == \"Utf8\" and not ser.has_validity()\n",
    "                        else arr\n",
    "                    )\n",
    "                arr_dtypes = list(\n",
    "                    zip(self.dataframe.columns, (a.dtype for a in arrays))\n",
    "                )\n",
    "                return np.rec.fromarrays(arrays, dtype=np.dtype(arr_dtypes))\n",
    "\n",
    "        ####################\n",
    "        # Pandas DataFrame #\n",
    "        ####################\n",
    "        elif isinstance(self.dataframe, pd.DataFrame):\n",
    "            self.engine_dataframe = pd.DataFrame\n",
    "            # Ensure that all required columns are present in the DataFrame:\n",
    "            # Full validation\n",
    "            if self.validate and self.dataframe.index.name == \"unique_id\":\n",
    "                reset_df = self.dataframe.reset_index()\n",
    "                self._validate_dataframe(reset_df)\n",
    "                del reset_df\n",
    "\n",
    "            elif self.validate and self.dataframe.index.name != \"unique_id\":\n",
    "                self._validate_dataframe(self.dataframe)\n",
    "                self.dataframe = self.dataframe.set_index(\"unique_id\")\n",
    "\n",
    "            # Partial validation\n",
    "            elif self.validate == False and self.dataframe.index.name == \"unique_id\":\n",
    "                reset_df = self.dataframe.reset_index()\n",
    "                self._partial_val_df(reset_df)\n",
    "                del reset_df\n",
    "\n",
    "            elif self.validate == False and self.dataframe.index.name != \"unique_id\":\n",
    "                self._partial_val_df(self.dataframe)\n",
    "                self.dataframe = self.dataframe.set_index(\"unique_id\")\n",
    "\n",
    "            # Datetime check\n",
    "            dt_arr = self.dataframe[\"ds\"].values\n",
    "            self.dataframe[\"ds\"] = self._check_datetime(dt_arr)\n",
    "\n",
    "            self.dataframe = self.dataframe.set_index(\"ds\", append=True)\n",
    "\n",
    "            # Sorting will be performed if sort is set to true and values are unsorted\n",
    "            if not self.dataframe.index.is_monotonic_increasing and self.sort_dataframe:\n",
    "                self.dataframe = self.dataframe.sort_values(self.non_value_columns)\n",
    "\n",
    "            np_df = self.dataframe.to_records(index=True)\n",
    "\n",
    "            return np_df\n",
    "\n",
    "        ####################\n",
    "        # Not Supported DF #\n",
    "        ####################\n",
    "        else:\n",
    "            raise ValueError(f\"{type(self.dataframe)} is not supported\")\n",
    "\n",
    "    def _validate_dataframe(self, dataframe: DataFrame):\n",
    "        \"\"\"\n",
    "        Will ensure that all DataFrame columns match the required columns.\n",
    "\n",
    "        This code requires a pandas DataFrame with the following structure:\n",
    "\n",
    "        Columns:\n",
    "        - `unique_id` Union[str, int, categorical]: an identifier for the series\n",
    "        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a\n",
    "            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "        - `y` Union[float, int]: represents the measurement we wish to forecast.\n",
    "\n",
    "        Raise:\n",
    "            KeyError: DataFrame is missing `unique_id`, `ds`, `y` columns.\n",
    "        \"\"\"\n",
    "        required_columns = [\"unique_id\", \"ds\", \"y\"]\n",
    "        matches = all(rc in dataframe.columns for rc in required_columns)\n",
    "        if not matches:\n",
    "            raise KeyError(\n",
    "                \"The DataFrame doesn't contain {} columns\".format(\n",
    "                    \", \".join(required_columns)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _partial_val_df(self, dataframe: DataFrame):\n",
    "        \"\"\"\n",
    "        Will ensure that all DataFrame columns match the required columns.\n",
    "\n",
    "        This code requires a pandas DataFrame with the following structure:\n",
    "\n",
    "        Columns:\n",
    "        - `unique_id` Union[str, int, categorical]: an identifier for the series\n",
    "        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a\n",
    "            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "        Raise:\n",
    "            KeyError: DataFrame is missing `unique_id` and/or `ds` columns.\n",
    "        \"\"\"\n",
    "        required_columns = [\"unique_id\", \"ds\"]\n",
    "        matches = all(rc in dataframe.columns for rc in required_columns)\n",
    "        if not matches:\n",
    "            raise KeyError(\n",
    "                \"The DataFrame doesn't contain {} columns\".format(\n",
    "                    \", \".join(required_columns)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _check_datetime(self, arr: np.ndarray) -> Union[pd.DatetimeIndex, np.ndarray]:\n",
    "        dt_check = pd.api.types.is_datetime64_any_dtype(arr)\n",
    "        int_float_check = arr.dtype.kind in [\"i\", \"f\"]\n",
    "        if not dt_check and not int_float_check:\n",
    "            self._ds_is_dt = True\n",
    "            try:\n",
    "                return pd.to_datetime(arr)\n",
    "            except Exception as e:\n",
    "                msg = (\n",
    "                    \"Failed to parse `ds` column as datetime. \"\n",
    "                    \"Please use `pd.to_datetime` outside to fix the error. \"\n",
    "                    f\"{e}\"\n",
    "                )\n",
    "                raise Exception(msg) from e\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd1f31-05f4-4bda-9599-8f8922233c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataFrameProcessing2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ):\n",
    "        # Required columns\n",
    "        missing_cols = {id_col, time_col, target_col} - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f'The following columns are missing: {missing_cols}')\n",
    "\n",
    "        # Time col\n",
    "        times = df[time_col].to_numpy()\n",
    "        times_dtype = times.dtype\n",
    "        if not (np.issubdtype(times_dtype, np.datetime64) or np.issubdtype(times_dtype, np.integer)):\n",
    "            raise ValueError(f\"The '{time_col}' column should have either datetimes or integers, got {times_dtype}.\")\n",
    "\n",
    "        # Ids\n",
    "        uids = df[id_col].to_numpy()\n",
    "        id_counts = pd.Series(uids).value_counts().sort_index()\n",
    "        self.uids = id_counts.index\n",
    "\n",
    "        # Indptr\n",
    "        self.indptr = np.append(0, id_counts.values.cumsum())\n",
    "        last_idxs = self.indptr[1:] - 1        \n",
    "\n",
    "        # Data\n",
    "        value_cols = [col for col in df.columns if col not in (id_col, time_col)]\n",
    "        self.data = df[value_cols].to_numpy()\n",
    "        if self.data.dtype not in (np.float32, np.float64):\n",
    "            self.data = self.data.astype(np.float32)\n",
    "\n",
    "        # Check if we need to sort\n",
    "        idx = pd.MultiIndex.from_arrays([uids, times])\n",
    "        if not idx.is_monotonic_increasing:\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                sort_idxs = idx.argsort()\n",
    "            else:\n",
    "                import polars as pl\n",
    "\n",
    "                sort_idxs = df.select(\n",
    "                    pl.arg_sort_by(['unique_id', 'ds']).alias('idx')\n",
    "                )['idx'].to_numpy()\n",
    "            self.data = self.data[sort_idxs]\n",
    "            last_idxs = sort_idxs[last_idxs]\n",
    "        self.times = times[last_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8f04c-f20e-4118-99c7-2e2a9e9a177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.data import generate_series\n",
    "series = generate_series(10_000, n_static_features=10, static_as_categorical=False)\n",
    "scrambled = series.sample(frac=1.0)\n",
    "pdf = pl.DataFrame(series.to_records(index=False))\n",
    "spdf = pdf.sample(fraction=1.0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022b4ee-dba5-4a23-9d1a-3e37c879c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4bd7d-edd0-44d8-87d4-4a04ae3097ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2029.04 MiB, increment: 846.00 MiB\n",
      "peak memory: 2228.66 MiB, increment: 283.59 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit a = DataFrameProcessing(series, True)\n",
    "%memit b = DataFrameProcessing2(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b465a0-fbd3-4415-ac8d-131ecd8ce7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3022.79 MiB, increment: 845.04 MiB\n",
      "peak memory: 2564.96 MiB, increment: 387.14 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit a = DataFrameProcessing(scrambled, True)\n",
    "%memit b = DataFrameProcessing2(scrambled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e4fa7-7f47-464d-91ab-7596147c64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2773.46 MiB, increment: 595.56 MiB\n",
      "peak memory: 2331.13 MiB, increment: 402.69 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit a = DataFrameProcessing(pdf, True)\n",
    "%memit b = DataFrameProcessing2(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527240cd-763f-4c82-a23f-968092b46a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3080.02 MiB, increment: 866.73 MiB\n",
      "peak memory: 2918.02 MiB, increment: 429.36 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit a = DataFrameProcessing(spdf, True)\n",
    "%memit b = DataFrameProcessing2(spdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626da95-8cc4-4a47-86e4-a4749b9dc3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274.6705513000488"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.memory_usage(deep=True).sum() / 2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae3fbb-da3e-41f6-933e-07e1e98ca01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274.67042541503906"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.estimated_size('mb')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7708cdb-1636-4a7a-bd08-a3f15055e1b9",
   "metadata": {},
   "source": [
    "sizes = np.diff(b.indptr)\n",
    "b.times[0] - sizes[0] * np.timedelta64(1, 'D')\n",
    "%time starts = b.times - sizes * np.timedelta64(1, 'D')\n",
    "%time dates = np.timedelta64(1, 'D') + np.concatenate([np.arange(start, end, dtype='datetime64[D]') for start, end in zip(starts, b.times)])\n",
    "pd.testing.assert_series_equal(pd.Series(dates.astype('datetime64[ns]'), name='ds'), series['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce60a7-e8a8-4b1e-9344-f508d142b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 954 ms, sys: 123 ms, total: 1.08 s\n",
      "Wall time: 1.08 s\n",
      "CPU times: user 135 ms, sys: 47.7 ms, total: 182 ms\n",
      "Wall time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "%time a = DataFrameProcessing(series, True)\n",
    "%time b = DataFrameProcessing2(series)\n",
    "np.testing.assert_equal(a.grouped_array().data, b.data)\n",
    "np.testing.assert_equal(a.grouped_array().indptr, b.indptr)\n",
    "np.testing.assert_equal(a.dates, b.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930e87-bf40-4ee6-9dae-623223189f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 200 ms, total: 2.05 s\n",
      "Wall time: 2.05 s\n",
      "CPU times: user 1.28 s, sys: 104 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%time a = DataFrameProcessing(scrambled, True)\n",
    "%time b = DataFrameProcessing2(scrambled)\n",
    "np.testing.assert_equal(a.grouped_array().data, b.data)\n",
    "np.testing.assert_equal(a.grouped_array().indptr, b.indptr)\n",
    "np.testing.assert_equal(a.dates, b.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec20b7-ba92-48e5-a88d-4f21f2fe8063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 738 ms, sys: 92.3 ms, total: 830 ms\n",
      "Wall time: 787 ms\n",
      "CPU times: user 229 ms, sys: 218 ms, total: 447 ms\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%time a = DataFrameProcessing(pdf, True)\n",
    "%time b = DataFrameProcessing2(pdf)\n",
    "np.testing.assert_equal(a.grouped_array().data, b.data)\n",
    "np.testing.assert_equal(a.grouped_array().indptr, b.indptr)\n",
    "np.testing.assert_equal(a.dates, b.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d8607-5b61-4a07-8f3e-d057ab1b22c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.61 s, sys: 342 ms, total: 4.95 s\n",
      "Wall time: 1.45 s\n",
      "CPU times: user 2.17 s, sys: 308 ms, total: 2.48 s\n",
      "Wall time: 838 ms\n"
     ]
    }
   ],
   "source": [
    "%time a = DataFrameProcessing(spdf, True)\n",
    "%time b = DataFrameProcessing2(spdf)\n",
    "np.testing.assert_equal(a.grouped_array().data, b.data)\n",
    "np.testing.assert_equal(a.grouped_array().indptr, b.indptr)\n",
    "np.testing.assert_equal(a.dates, b.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52bd54-9cf6-4aea-b43d-609a476c02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from fastcore.test import test_eq\n",
    "\n",
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac7668-b96f-4159-b10a-d3317e9bf7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_series(10_000, n_static_features=2, equal_ends=False, engine='pandas').set_index('unique_id')\n",
    "sorted_series = series.sort_values(['unique_id', 'ds'])\n",
    "unsorted_series = sorted_series.sample(frac=1.0)\n",
    "\n",
    "df_process = DataFrameProcessing(dataframe=unsorted_series, sort_dataframe=True)\n",
    "ga = df_process.grouped_array()\n",
    "indices = df_process.indices\n",
    "dates = df_process.dates\n",
    "ds = df_process.index\n",
    "\n",
    "np.testing.assert_allclose(ga.data, sorted_series.drop(columns='ds').values)\n",
    "test_eq(indices, sorted_series.index.unique(level='unique_id'))\n",
    "test_eq(dates, series.groupby('unique_id')['ds'].max().values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
