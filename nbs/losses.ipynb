{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses\n",
    "#| all_polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Loss functions for model evaluation.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important train signal is the forecast error, which is the difference between the observed value $y_{\\tau}$ and the prediction $\\hat{y}_{\\tau}$, at time $y_{\\tau}$:\n",
    "\n",
    "$$\n",
    "e_{\\tau} = y_{\\tau}-\\hat{y}_{\\tau} \\qquad \\qquad \\tau \\in \\{t+1,\\dots,t+H \\}\n",
    "$$\n",
    "\n",
    "The train loss summarizes the forecast errors in different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utilsforecast.processing as ufp\n",
    "from utilsforecast.compat import DFType, DataFrame, pl_DataFrame, pl, pl_Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from nbdev import show_doc\n",
    "\n",
    "from utilsforecast.compat import POLARS_INSTALLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.filterwarnings('ignore', message='Unknown section References')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "models = ['model0', 'model1']\n",
    "series = generate_series(10, n_models=2, level=[80])\n",
    "series_pl = generate_series(10, n_models=2, level=[80], engine='polars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scale-dependent Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/mae_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _base_docstring(*args, **kwargs) -> Callable:\n",
    "    base_docstring = \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, actual values and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "    \"\"\"\n",
    "    def docstring_decorator(f: Callable):\n",
    "        if f.__doc__ is not None:\n",
    "            f.__doc__ += base_docstring\n",
    "        return f\n",
    "\n",
    "    return docstring_decorator(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _pl_agg_expr(\n",
    "    df: pl_DataFrame,\n",
    "    models: Union[List[str], List[Tuple[str, str]]],\n",
    "    id_col: str,\n",
    "    gen_expr: Callable[[Union[str, Tuple[str, str]]], 'pl.Expr'],\n",
    ") -> pl_DataFrame:\n",
    "    exprs = [gen_expr(model) for model in models]\n",
    "    df = df.select([id_col, *exprs])\n",
    "    return ufp.group_by(df, id_col, maintain_order=True).mean()\n",
    "\n",
    "\n",
    "def _scale_loss(\n",
    "    loss_df: DFType,\n",
    "    scale_type: str,\n",
    "    models: List[str],\n",
    "    seasonality: int,\n",
    "    train_df: DFType,\n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_df : pandas or polars DataFrame\n",
    "        Input dataframe with id, actuals, predictions and losses results.\n",
    "    scale_type : str\n",
    "        Type of scaling. Possible values are 'absolute_error' or 'squared_error'.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    seasonality : int\n",
    "        Main frequency of the time series;\n",
    "        Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    train_df : pandas or polars DataFrame\n",
    "        Training dataframe with id and actual values. Must be sorted by time.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/papers/mase.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(train_df, pd.DataFrame):\n",
    "        loss_df = loss_df.set_index(id_col)\n",
    "        # assume train_df is sorted\n",
    "        lagged = train_df.groupby(id_col, observed=True)[target_col].shift(seasonality)\n",
    "        if scale_type == \"absolute_error\":\n",
    "            scale = train_df[target_col].sub(lagged).abs()\n",
    "        else:\n",
    "            scale = train_df[target_col].sub(lagged).pow(2)\n",
    "        scale = scale.groupby(train_df[id_col], observed=True).mean()\n",
    "        res = loss_df.div(_zero_to_nan(scale), axis=0).fillna(0)\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        # assume train_df is sorted\n",
    "        lagged = pl.col(target_col).shift(seasonality).over(id_col)\n",
    "        if scale_type == \"absolute_error\":\n",
    "            scale_expr = pl.col(target_col).sub(lagged).abs().alias(\"scale\")\n",
    "        else:\n",
    "            scale_expr = pl.col(target_col).sub(lagged).pow(2).alias(\"scale\")\n",
    "        scale = train_df.select([id_col, scale_expr])\n",
    "        scale = ufp.group_by(scale, id_col).mean()\n",
    "        scale = scale.with_columns(_zero_to_nan(pl.col(\"scale\")))\n",
    "\n",
    "        def gen_expr(model):\n",
    "            return pl.col(model).truediv(pl.col(\"scale\")).fill_nan(0).alias(model)\n",
    "\n",
    "        full_df = loss_df.join(scale, on=id_col, how=\"left\")\n",
    "        res = _pl_agg_expr(full_df, models, id_col, gen_expr)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def mae(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Mean Absolute Error (MAE)\n",
    "\n",
    "    MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res = (df[models].sub(df[target_col], axis=0)).abs().groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return pl.col(target_col).sub(pl.col(model)).abs().alias(model)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L129){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mae\n",
       "\n",
       ">      mae (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">           target_col:str='y')\n",
       "\n",
       "*Mean Absolute Error (MAE)\n",
       "\n",
       "MAE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the\n",
       "deviation of the prediction and the true\n",
       "value at a given time and averages these devations\n",
       "over the length of the series.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L129){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mae\n",
       "\n",
       ">      mae (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">           target_col:str='y')\n",
       "\n",
       "*Mean Absolute Error (MAE)\n",
       "\n",
       "MAE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the\n",
       "deviation of the prediction and the true\n",
       "value at a given time and averages these devations\n",
       "over the length of the series.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mae, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_vs_pl(pd_df, pl_df, models):\n",
    "    np.testing.assert_allclose(\n",
    "        pd_df[models].to_numpy(),\n",
    "        pl_df.sort('unique_id').select(models).to_numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    mae(series, models),\n",
    "    mae(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/mse_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def mse(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Mean Squared Error (MSE)\n",
    "    \n",
    "    MSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the \n",
    "    squared deviation of the prediction and the true\n",
    "    value at a given time, and averages these devations\n",
    "    over the length of the series.\"\"\"    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res = (df[models].sub(df[target_col], axis=0)).pow(2).groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return pl.col(target_col).sub(pl.col(model)).pow(2).alias(model)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mse\n",
       "\n",
       ">      mse (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">           target_col:str='y')\n",
       "\n",
       "*Mean Squared Error (MSE)\n",
       "\n",
       "MSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the \n",
       "squared deviation of the prediction and the true\n",
       "value at a given time, and averages these devations\n",
       "over the length of the series.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L161){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mse\n",
       "\n",
       ">      mse (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">           target_col:str='y')\n",
       "\n",
       "*Mean Squared Error (MSE)\n",
       "\n",
       "MSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the \n",
       "squared deviation of the prediction and the true\n",
       "value at a given time, and averages these devations\n",
       "over the length of the series.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mse, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    mse(series, models),\n",
    "    mse(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/rmse_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def rmse(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Root Mean Squared Error (RMSE)\n",
    "    \n",
    "    RMSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the squared deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    Finally the RMSE will be in the same scale\n",
    "    as the original time series so its comparison with other\n",
    "    series is possible only if they share a common scale. \n",
    "    RMSE has a direct connection to the L2 norm.\"\"\"    \n",
    "    res = mse(df, models, id_col, target_col)\n",
    "    if isinstance(res, pd.DataFrame):\n",
    "        res[models] = res[models].pow(0.5)\n",
    "    else:\n",
    "        res = res.with_columns(*[pl.col(c).pow(0.5) for c in models])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L193){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmse\n",
       "\n",
       ">      rmse (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Root Mean Squared Error (RMSE)\n",
       "\n",
       "RMSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the squared deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "Finally the RMSE will be in the same scale\n",
       "as the original time series so its comparison with other\n",
       "series is possible only if they share a common scale. \n",
       "RMSE has a direct connection to the L2 norm.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L193){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmse\n",
       "\n",
       ">      rmse (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Root Mean Squared Error (RMSE)\n",
       "\n",
       "RMSE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the squared deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "Finally the RMSE will be in the same scale\n",
       "as the original time series so its comparison with other\n",
       "series is possible only if they share a common scale. \n",
       "RMSE has a direct connection to the L2 norm.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(rmse, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    rmse(series, models),\n",
    "    rmse(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def bias(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Forecast estimator bias.\n",
    "    \n",
    "    Defined as prediction - actual\"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res = (df[models].sub(df[target_col], axis=0)).groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return pl.col(model).sub(pl.col(target_col)).alias(model)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L218){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### bias\n",
       "\n",
       ">      bias (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Forecast estimator bias.\n",
       "\n",
       "Defined as prediction - actual*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L218){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### bias\n",
       "\n",
       ">      bias (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Forecast estimator bias.\n",
       "\n",
       "Defined as prediction - actual*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(bias, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    bias(series, models),\n",
    "    bias(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Percentage Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error\n",
    "\n",
    "$$\n",
    "\\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/mape_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _zero_to_nan(series: Union[pd.Series, 'pl.Expr']) -> Union[pd.Series, 'pl.Expr']:\n",
    "    if isinstance(series, pd.Series):\n",
    "        res = series.replace(0, np.nan)\n",
    "    else:\n",
    "        res = (\n",
    "            pl.when(series == 0)\n",
    "            .then(float('nan'))\n",
    "            .otherwise(series.abs())\n",
    "        )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def mape(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Mean Absolute Percentage Error (MAPE)\n",
    "    \n",
    "    MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the percentual deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    The closer to zero an observed value is, the higher penalty MAPE loss\n",
    "    assigns to the corresponding error.\"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res = (\n",
    "            df[models]\n",
    "            .sub(df[target_col], axis=0)\n",
    "            .abs()\n",
    "            .div(_zero_to_nan(df[target_col].abs()), axis=0)\n",
    "            .groupby(df[id_col], observed=True).mean()\n",
    "        )\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            abs_err = pl.col(target_col).sub(pl.col(model)).abs()\n",
    "            abs_target = _zero_to_nan(pl.col(target_col))\n",
    "            ratio = abs_err.truediv(abs_target).alias(model)\n",
    "            return ratio.fill_nan(None)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mape\n",
       "\n",
       ">      mape (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Mean Absolute Percentage Error (MAPE)\n",
       "\n",
       "MAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the percentual deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "The closer to zero an observed value is, the higher penalty MAPE loss\n",
       "assigns to the corresponding error.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L253){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mape\n",
       "\n",
       ">      mape (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Mean Absolute Percentage Error (MAPE)\n",
       "\n",
       "MAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the percentual deviation\n",
       "of the prediction and the observed value at a given time and\n",
       "averages these devations over the length of the series.\n",
       "The closer to zero an observed value is, the higher penalty MAPE loss\n",
       "assigns to the corresponding error.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mape, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    mape(series, models),\n",
    "    mape(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Mean Absolute Percentage Error\n",
    "\n",
    "$$\n",
    "\\mathrm{SMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@_base_docstring\n",
    "def smape(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "\n",
    "    SMAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the relative deviation\n",
    "    of the prediction and the observed value scaled by the sum of the\n",
    "    absolute values for the prediction and observed value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 100% which is desirable compared to normal MAPE that\n",
    "    may be undetermined when the target is zero.\"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        delta_y = df[models].sub(df[target_col], axis=0).abs()\n",
    "        scale = df[models].abs().add(df[target_col].abs(), axis=0)\n",
    "        raw = delta_y.div(scale).fillna(0)\n",
    "        res = raw.groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            abs_err = pl.col(model).sub(pl.col(target_col)).abs()\n",
    "            denominator = _zero_to_nan(pl.col(model).abs().add(pl.col(target_col)).abs())\n",
    "            ratio = abs_err.truediv(denominator).alias(model)\n",
    "            return ratio.fill_nan(0)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### smape\n",
       "\n",
       ">      smape (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">             target_col:str='y')\n",
       "\n",
       "*Symmetric Mean Absolute Percentage Error (SMAPE)\n",
       "\n",
       "SMAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the relative deviation\n",
       "of the prediction and the observed value scaled by the sum of the\n",
       "absolute values for the prediction and observed value at a\n",
       "given time, then averages these devations over the length\n",
       "of the series. This allows the SMAPE to have bounds between\n",
       "0% and 100% which is desirable compared to normal MAPE that\n",
       "may be undetermined when the target is zero.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### smape\n",
       "\n",
       ">      smape (df:~DFType, models:List[str], id_col:str='unique_id',\n",
       ">             target_col:str='y')\n",
       "\n",
       "*Symmetric Mean Absolute Percentage Error (SMAPE)\n",
       "\n",
       "SMAPE measures the relative prediction\n",
       "accuracy of a forecasting method by calculating the relative deviation\n",
       "of the prediction and the observed value scaled by the sum of the\n",
       "absolute values for the prediction and observed value at a\n",
       "given time, then averages these devations over the length\n",
       "of the series. This allows the SMAPE to have bounds between\n",
       "0% and 100% which is desirable compared to normal MAPE that\n",
       "may be undetermined when the target is zero.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(smape, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    smape(series, models),\n",
    "    smape(series_pl, models),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scale-independent Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Scaled Error\n",
    "\n",
    "$$\n",
    "\\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \n",
    "\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/mase_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mase(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    seasonality: int,\n",
    "    train_df: DFType,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Mean Absolute Scaled Error (MASE)\n",
    "    \n",
    "    MASE measures the relative prediction\n",
    "    accuracy of a forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the observed value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "    The MASE partially composed the Overall Weighted Average (OWA), \n",
    "    used in the M4 Competition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, actuals and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    seasonality : int\n",
    "        Main frequency of the time series;\n",
    "        Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    train_df : pandas or polars DataFrame\n",
    "        Training dataframe with id and actual values. Must be sorted by time.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://robjhyndman.com/papers/mase.pdf        \n",
    "    \"\"\"\n",
    "    mean_abs_err = mae(df, models, id_col, target_col)\n",
    "    return _scale_loss(\n",
    "        loss_df=mean_abs_err,\n",
    "        scale_type=\"absolute_error\",\n",
    "        models=models,\n",
    "        seasonality=seasonality,\n",
    "        train_df=train_df,\n",
    "        id_col=id_col,\n",
    "        target_col=target_col\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mase\n",
       "\n",
       ">      mase (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Mean Absolute Scaled Error (MASE)\n",
       "\n",
       "MASE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean absolute errors\n",
       "of the prediction and the observed value against the mean\n",
       "absolute errors of the seasonal naive model.\n",
       "The MASE partially composed the Overall Weighted Average (OWA), \n",
       "used in the M4 Competition.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mase\n",
       "\n",
       ">      mase (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Mean Absolute Scaled Error (MASE)\n",
       "\n",
       "MASE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean absolute errors\n",
       "of the prediction and the observed value against the mean\n",
       "absolute errors of the seasonal naive model.\n",
       "The MASE partially composed the Overall Weighted Average (OWA), \n",
       "used in the M4 Competition.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mase, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    mase(series, models, 7, series),\n",
    "    mase(series_pl, models, 7, series_pl),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Mean Absolute Error\n",
    "\n",
    "$$\n",
    "\\mathrm{RMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/rmae_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rmae(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    baseline: str,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Relative Mean Absolute Error (RMAE)\n",
    "    \n",
    "    Calculates the RAME between two sets of forecasts (from two different forecasting methods).\n",
    "    A number smaller than one implies that the forecast in the \n",
    "    numerator is better than the forecast in the denominator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    baseline : str\n",
    "        Column that identifies the baseline model predictions.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "    \"\"\"\n",
    "    numerator = mae(df, models, id_col, target_col)\n",
    "    denominator = mae(df, [baseline], id_col, target_col)\n",
    "    if ufp.is_nan(denominator[baseline]).any():\n",
    "        raise ValueError(f'baseline model ({baseline}) contains NaNs.')\n",
    "    denominator = ufp.rename(denominator, {baseline: f'{baseline}_denominator'})\n",
    "    res = ufp.join(numerator, denominator, on=id_col)\n",
    "    if isinstance(numerator, pd.DataFrame):\n",
    "        for model in models:\n",
    "            res[model] = res[model].div(_zero_to_nan(res[f'{baseline}_denominator'])).fillna(0)\n",
    "        res = res[[id_col, *models]]\n",
    "    else:\n",
    "        def gen_expr(model, baseline) -> pl_Expr:\n",
    "            denominator = _zero_to_nan(pl.col(f'{baseline}_denominator'))\n",
    "            return pl.col(model).truediv(denominator).fill_nan(0).alias(model)\n",
    "\n",
    "        exprs: List[pl_Expr] = [gen_expr(m, baseline) for m in models]\n",
    "        res = res.select([id_col, *exprs])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L382){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmae\n",
       "\n",
       ">      rmae (df:~DFType, models:List[str], baseline:str, id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Relative Mean Absolute Error (RMAE)\n",
       "\n",
       "Calculates the RAME between two sets of forecasts (from two different forecasting methods).\n",
       "A number smaller than one implies that the forecast in the \n",
       "numerator is better than the forecast in the denominator.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| baseline | str |  | Column that identifies the baseline model predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L382){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmae\n",
       "\n",
       ">      rmae (df:~DFType, models:List[str], baseline:str, id_col:str='unique_id',\n",
       ">            target_col:str='y')\n",
       "\n",
       "*Relative Mean Absolute Error (RMAE)\n",
       "\n",
       "Calculates the RAME between two sets of forecasts (from two different forecasting methods).\n",
       "A number smaller than one implies that the forecast in the \n",
       "numerator is better than the forecast in the denominator.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| baseline | str |  | Column that identifies the baseline model predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(rmae, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    rmae(series, models, models[0]),\n",
    "    rmae(series_pl, models, models[0]),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Scaled Error\n",
    "\n",
    "$$\n",
    "\\mathrm{MSSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \n",
    "\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{(y_{\\tau}-\\hat{y}_{\\tau})^2}{\\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def msse(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    seasonality: int,\n",
    "    train_df: DFType,\n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    \"\"\"Mean Squared Scaled Error (MSSE)\n",
    "\n",
    "    MSSE measures the relative prediction\n",
    "    accuracy of a forecasting method by comparinng the mean squared errors\n",
    "    of the prediction and the observed value against the mean\n",
    "    squared errors of the seasonal naive model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, actuals and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    seasonality : int\n",
    "        Main frequency of the time series;\n",
    "        Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    train_df : pandas or polars DataFrame\n",
    "        Training dataframe with id and actual values. Must be sorted by time.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://otexts.com/fpp3/accuracy.html\n",
    "    \"\"\"\n",
    "    mean_sq_err = mse(df=df, models=models, id_col=id_col, target_col=target_col)\n",
    "    return _scale_loss(\n",
    "        loss_df=mean_sq_err,\n",
    "        scale_type=\"squared_error\",\n",
    "        models=models,\n",
    "        seasonality=seasonality,\n",
    "        train_df=train_df,\n",
    "        id_col=id_col,\n",
    "        target_col=target_col\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L436){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### msse\n",
       "\n",
       ">      msse (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Mean Squared Scaled Error (MSSE)\n",
       "\n",
       "MSSE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean squared errors\n",
       "of the prediction and the observed value against the mean\n",
       "squared errors of the seasonal naive model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L436){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### msse\n",
       "\n",
       ">      msse (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Mean Squared Scaled Error (MSSE)\n",
       "\n",
       "MSSE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean squared errors\n",
       "of the prediction and the observed value against the mean\n",
       "squared errors of the seasonal naive model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(msse, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    msse(series, models, 7, series),\n",
    "    msse(series_pl, models, 7, series_pl),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Scaled Error\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \n",
    "\\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{(y_{\\tau}-\\hat{y}_{\\tau})^2}{\\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rmsse(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    seasonality: int,\n",
    "    train_df: DFType,\n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    res = msse(\n",
    "        df,\n",
    "        models=models,\n",
    "        seasonality=seasonality,\n",
    "        train_df=train_df,\n",
    "        id_col=id_col,\n",
    "        target_col=target_col,\n",
    "    )\n",
    "    if isinstance(res, pd.DataFrame):\n",
    "        res[models] = res[models].pow(0.5)\n",
    "    else:\n",
    "        res = res.with_columns(pl.col(models).pow(0.5))\n",
    "    return res\n",
    "\n",
    "rmsse.__doc__ = msse.__doc__.replace(  # type: ignore[union-attr]\n",
    "    'Mean Squared Scaled Error (MSSE)', 'Root Mean Squared Scaled Error (RMSSE)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L488){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmsse\n",
       "\n",
       ">      rmsse (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">             id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Root Mean Squared Scaled Error (RMSSE)\n",
       "\n",
       "MSSE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean squared errors\n",
       "of the prediction and the observed value against the mean\n",
       "squared errors of the seasonal naive model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L488){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### rmsse\n",
       "\n",
       ">      rmsse (df:~DFType, models:List[str], seasonality:int, train_df:~DFType,\n",
       ">             id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Root Mean Squared Scaled Error (RMSSE)\n",
       "\n",
       "MSSE measures the relative prediction\n",
       "accuracy of a forecasting method by comparinng the mean squared errors\n",
       "of the prediction and the observed value against the mean\n",
       "squared errors of the seasonal naive model.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(rmsse, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    rmsse(series, models, 7, series),\n",
    "    rmsse(series_pl, models, 7, series_pl),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Probabilistic Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "\n",
    "$$\n",
    "\\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
    "\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \n",
    "\\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} \n",
    "+ q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/q_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def quantile_loss(\n",
    "    df: DFType,\n",
    "    models: Dict[str, str],\n",
    "    q: float = 0.5,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Quantile Loss (QL)\n",
    "\n",
    "    QL measures the deviation of a quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.    \n",
    "    A common value for q is 0.5 for the deviation from the median.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to str\n",
    "        Mapping from model name to the model predictions for the specified quantile.\n",
    "    q : float (default=0.5)\n",
    "        Quantile for the predictions' comparison.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res: Optional[pd.DataFrame] = None\n",
    "        for model_name, pred_col in models.items():\n",
    "            delta_y = df[target_col].sub(df[pred_col], axis=0)\n",
    "            model_res = (\n",
    "                np.maximum(q * delta_y, (q - 1) * delta_y)\n",
    "                .groupby(df[id_col], observed=True)\n",
    "                .mean()\n",
    "                .rename(model_name)\n",
    "                .reset_index()\n",
    "            )\n",
    "            if res is None:\n",
    "                res = model_res\n",
    "            else:\n",
    "                res[model_name] = model_res[model_name]\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            model_name, pred_col = model\n",
    "            delta_y = pl.col(target_col).sub(pl.col(pred_col))\n",
    "            try:\n",
    "                col_max = pl.max_horizontal([q * delta_y, (q - 1) * delta_y])\n",
    "            except AttributeError:\n",
    "                col_max = pl.max([q * delta_y, (q - 1) * delta_y])\n",
    "            return col_max.alias(model_name)\n",
    "\n",
    "        res = _pl_agg_expr(df, list(models.items()), id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L516){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### quantile_loss\n",
       "\n",
       ">      quantile_loss (df:~DFType, models:Dict[str,str], q:float=0.5,\n",
       ">                     id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Quantile Loss (QL)\n",
       "\n",
       "QL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.    \n",
       "A common value for q is 0.5 for the deviation from the median.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for the specified quantile. |\n",
       "| q | float | 0.5 | Quantile for the predictions' comparison. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L516){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### quantile_loss\n",
       "\n",
       ">      quantile_loss (df:~DFType, models:Dict[str,str], q:float=0.5,\n",
       ">                     id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Quantile Loss (QL)\n",
       "\n",
       "QL measures the deviation of a quantile forecast.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.    \n",
       "A common value for q is 0.5 for the deviation from the median.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for the specified quantile. |\n",
       "| q | float | 0.5 | Quantile for the predictions' comparison. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(quantile_loss, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.DataFrame({\n",
    "    \"unique_id\": [0, 1, 2],\n",
    "    \"y\": [1.0, 2.0, 3.0],\n",
    "    \"overestimation\": [2.0, 3.0, 4.0], # y + 1.\n",
    "    \"underestimation\": [0.0, 1.0, 2.0], # y - 1.\n",
    "})\n",
    "df[\"unique_id\"] = df[\"unique_id\"].astype(\"category\")\n",
    "df = pd.concat([df, df.assign(unique_id=2)]).reset_index(drop=True)\n",
    "\n",
    "ql_models_test = [\"overestimation\", \"underestimation\"]\n",
    "quantiles = np.array([0.1, 0.9])\n",
    "\n",
    "for q in quantiles:\n",
    "    ql_df = quantile_loss(df, models=dict(zip(ql_models_test, ql_models_test)), q=q)\n",
    "    # for overestimation, delta_y = y - y_hat = -1 so ql = max(-q, -(q-1)) \n",
    "    assert all(max(-q, -(q - 1)) == ql for ql in ql_df[\"overestimation\"])\n",
    "    # for underestimation, delta_y = y - y_hat = 1, so ql = max(q, q-1)\n",
    "    assert all(max(q, q - 1) == ql for ql in ql_df[\"underestimation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "q_models = {\n",
    "    0.1: {\n",
    "        'model0': 'model0-lo-80',\n",
    "        'model1': 'model1-lo-80',\n",
    "    },\n",
    "    0.9: {\n",
    "        'model0': 'model0-hi-80',\n",
    "        'model1': 'model1-hi-80',\n",
    "    },\n",
    "}\n",
    "\n",
    "for q in quantiles:\n",
    "    pd_vs_pl(\n",
    "        quantile_loss(series, q_models[q], q=q),\n",
    "        quantile_loss(series_pl, q_models[q], q=q),\n",
    "        models,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Quantile Loss\n",
    "\n",
    "$$\n",
    "\\mathrm{SQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n",
    "\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \n",
    "\\frac{(1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} \n",
    "+ q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+}}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scaled_quantile_loss(\n",
    "    df: DFType,\n",
    "    models: Dict[str, str],\n",
    "    seasonality: int,\n",
    "    train_df: DFType,    \n",
    "    q: float = 0.5,\n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    \"\"\"Scaled Quantile Loss (SQL)\n",
    "\n",
    "    SQL measures the deviation of a quantile forecast scaled by\n",
    "    the mean absolute errors of the seasonal naive model.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "    A common value for q is 0.5 for the deviation from the median.\n",
    "    This was the official measure used in the M5 Uncertainty competition\n",
    "    with seasonality = 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to str\n",
    "        Mapping from model name to the model predictions for the specified quantile.\n",
    "    seasonality : int\n",
    "        Main frequency of the time series;\n",
    "        Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    train_df : pandas or polars DataFrame\n",
    "        Training dataframe with id and actual values. Must be sorted by time.        \n",
    "    q : float (default=0.5)\n",
    "        Quantile for the predictions' comparison.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.sciencedirect.com/science/article/pii/S0169207021001722\n",
    "    \"\"\"     \n",
    "    q_loss = quantile_loss(df=df, models=models, q=q, id_col=id_col, target_col=target_col)\n",
    "    return _scale_loss(\n",
    "        loss_df=q_loss,\n",
    "        scale_type=\"absolute_error\",\n",
    "        models=list(models.keys()),\n",
    "        seasonality=seasonality,\n",
    "        train_df=train_df,\n",
    "        id_col=id_col,\n",
    "        target_col=target_col,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_quantile_loss\n",
       "\n",
       ">      scaled_quantile_loss (df:~DFType, models:Dict[str,str], seasonality:int,\n",
       ">                            train_df:~DFType, q:float=0.5,\n",
       ">                            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Scaled Quantile Loss (SQL)\n",
       "\n",
       "SQL measures the deviation of a quantile forecast scaled by\n",
       "the mean absolute errors of the seasonal naive model.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "A common value for q is 0.5 for the deviation from the median.\n",
       "This was the official measure used in the M5 Uncertainty competition\n",
       "with seasonality = 1.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for the specified quantile. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time.         |\n",
       "| q | float | 0.5 | Quantile for the predictions' comparison. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_quantile_loss\n",
       "\n",
       ">      scaled_quantile_loss (df:~DFType, models:Dict[str,str], seasonality:int,\n",
       ">                            train_df:~DFType, q:float=0.5,\n",
       ">                            id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Scaled Quantile Loss (SQL)\n",
       "\n",
       "SQL measures the deviation of a quantile forecast scaled by\n",
       "the mean absolute errors of the seasonal naive model.\n",
       "By weighting the absolute deviation in a non symmetric way, the\n",
       "loss pays more attention to under or over estimation.\n",
       "A common value for q is 0.5 for the deviation from the median.\n",
       "This was the official measure used in the M5 Uncertainty competition\n",
       "with seasonality = 1.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for the specified quantile. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time.         |\n",
       "| q | float | 0.5 | Quantile for the predictions' comparison. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(scaled_quantile_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "q_models = {\n",
    "    0.1: {\n",
    "        'model0': 'model0-lo-80',\n",
    "        'model1': 'model1-lo-80',\n",
    "    },\n",
    "    0.9: {\n",
    "        'model0': 'model0-hi-80',\n",
    "        'model1': 'model1-hi-80',\n",
    "    },\n",
    "}\n",
    "\n",
    "for q in quantiles:\n",
    "    pd_vs_pl(\n",
    "        scaled_quantile_loss(series, q_models[q], seasonality=1, train_df=series, q=q),\n",
    "        scaled_quantile_loss(series_pl, q_models[q], seasonality=1, train_df=series_pl, q=q),\n",
    "        models,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Quantile Loss\n",
    "\n",
    "$$\n",
    "\\mathrm{MQL}(\\mathbf{y}_{\\tau},\n",
    "[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \n",
    "\\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/losses/mq_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mqloss(\n",
    "    df: DFType,\n",
    "    models: Dict[str, List[str]],\n",
    "    quantiles: np.ndarray,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Multi-Quantile loss (MQL)\n",
    "    \n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.\n",
    "\n",
    "    The limit behavior of MQL allows to measure the accuracy \n",
    "    of a full predictive distribution with \n",
    "    the continuous ranked probability score (CRPS). This can be achieved \n",
    "    through a numerical integration technique, that discretizes the quantiles \n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "    uniformly distanced quantiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to list of str\n",
    "        Mapping from model name to the model predictions for each quantile.\n",
    "    quantiles : numpy array\n",
    "        Quantiles to compare against.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.jstor.org/stable/2629907\n",
    "    \"\"\"\n",
    "    res: Optional[DataFrame] = None\n",
    "    error = np.empty((df.shape[0], quantiles.size))\n",
    "    for model, predictions in models.items():\n",
    "        for j, q_preds in enumerate(predictions):\n",
    "            error[:, j] = (df[target_col] - df[q_preds]).to_numpy()\n",
    "        loss = np.maximum(error * quantiles, error * (quantiles - 1)).mean(axis=1)\n",
    "        model_res = type(df)({id_col: df[id_col], model: loss})\n",
    "        model_res = ufp.group_by_agg(\n",
    "            model_res, by=id_col, aggs={model: 'mean'}, maintain_order=True\n",
    "        )\n",
    "        if res is None:\n",
    "            res = model_res\n",
    "        else:\n",
    "            res = ufp.assign_columns(res, model, model_res[model])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L638){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mqloss\n",
       "\n",
       ">      mqloss (df:~DFType, models:Dict[str,List[str]], quantiles:numpy.ndarray,\n",
       ">              id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Multi-Quantile loss (MQL)\n",
       "\n",
       "MQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute \n",
       "difference between predicted quantiles and observed values.\n",
       "\n",
       "The limit behavior of MQL allows to measure the accuracy \n",
       "of a full predictive distribution with \n",
       "the continuous ranked probability score (CRPS). This can be achieved \n",
       "through a numerical integration technique, that discretizes the quantiles \n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over \n",
       "uniformly distanced quantiles.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L638){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mqloss\n",
       "\n",
       ">      mqloss (df:~DFType, models:Dict[str,List[str]], quantiles:numpy.ndarray,\n",
       ">              id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Multi-Quantile loss (MQL)\n",
       "\n",
       "MQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute \n",
       "difference between predicted quantiles and observed values.\n",
       "\n",
       "The limit behavior of MQL allows to measure the accuracy \n",
       "of a full predictive distribution with \n",
       "the continuous ranked probability score (CRPS). This can be achieved \n",
       "through a numerical integration technique, that discretizes the quantiles \n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over \n",
       "uniformly distanced quantiles.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mqloss, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "mq_models = {\n",
    "    'model0': ['model0-lo-80', 'model0-hi-80'],\n",
    "    'model1': ['model1-lo-80', 'model1-hi-80'],\n",
    "}\n",
    "\n",
    "expected = pd.concat(\n",
    "    [\n",
    "        quantile_loss(series, models=q_models[q], q=q)\n",
    "        for i, q in enumerate(quantiles)\n",
    "    ]\n",
    ").groupby('unique_id', observed=True, as_index=False).mean()\n",
    "actual = mqloss(\n",
    "    series,\n",
    "    models=mq_models,\n",
    "    quantiles=quantiles,\n",
    ")\n",
    "pd.testing.assert_frame_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    mqloss(series, mq_models, quantiles=quantiles),\n",
    "    mqloss(series_pl, mq_models, quantiles=quantiles),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| polars\n",
    "for series_df in [series, series_pl]:\n",
    "    if isinstance(series_df, pd.DataFrame):\n",
    "        df_test = series_df.assign(unique_id=lambda df: df[\"unique_id\"].astype(str))\n",
    "    else:\n",
    "        df_test = series_df.with_columns(pl.col(\"unique_id\").cast(pl.Utf8))\n",
    "    mql_df = mqloss(\n",
    "        df_test,\n",
    "        mq_models, \n",
    "        quantiles=quantiles,\n",
    "    )\n",
    "    assert mql_df.shape == (series[\"unique_id\"].nunique(), 1 + len(models))\n",
    "    if isinstance(mql_df, pd.DataFrame):\n",
    "        null_vals = mql_df.isna().sum().sum()\n",
    "    else:\n",
    "        null_vals = series_df.select(pl.all().is_null().sum()).sum_horizontal()\n",
    "    assert null_vals.item() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Multi-Quantile Loss\n",
    "\n",
    "$$\n",
    "\\mathrm{MQL}(\\mathbf{y}_{\\tau},\n",
    "[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \n",
    "\\frac{1}{n} \\sum_{q_{i}} \\frac{\\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau})}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scaled_mqloss(\n",
    "    df: DFType,\n",
    "    models: Dict[str, List[str]],\n",
    "    quantiles: np.ndarray,\n",
    "    seasonality: int,\n",
    "    train_df: DFType,\n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    \"\"\"Scaled Multi-Quantile loss (SMQL)\n",
    "\n",
    "    SMQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute\n",
    "    difference between predicted quantiles and observed values \n",
    "    scaled by the mean absolute errors of the seasonal naive model. \n",
    "    The limit behavior of MQL allows to measure the accuracy\n",
    "    of a full predictive distribution with\n",
    "    the continuous ranked probability score (CRPS). This can be achieved\n",
    "    through a numerical integration technique, that discretizes the quantiles\n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over\n",
    "    uniformly distanced quantiles.\n",
    "    This was the official measure used in the M5 Uncertainty competition\n",
    "    with seasonality = 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to list of str\n",
    "        Mapping from model name to the model predictions for each quantile.\n",
    "    quantiles : numpy array\n",
    "        Quantiles to compare against.\n",
    "    seasonality : int\n",
    "        Main frequency of the time series;\n",
    "        Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "    train_df : pandas or polars DataFrame\n",
    "        Training dataframe with id and actual values. Must be sorted by time.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.sciencedirect.com/science/article/pii/S0169207021001722\n",
    "    \"\"\"\n",
    "    mq_loss = mqloss(df=df, models=models, quantiles=quantiles, id_col=id_col, target_col=target_col)\n",
    "    return _scale_loss(\n",
    "        loss_df=mq_loss,\n",
    "        scale_type=\"absolute_error\",\n",
    "        models=list(models.keys()),\n",
    "        seasonality=seasonality,\n",
    "        train_df=train_df,\n",
    "        id_col=id_col,\n",
    "        target_col=target_col,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L697){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_mqloss\n",
       "\n",
       ">      scaled_mqloss (df:~DFType, models:Dict[str,List[str]],\n",
       ">                     quantiles:numpy.ndarray, seasonality:int,\n",
       ">                     train_df:~DFType, id_col:str='unique_id',\n",
       ">                     target_col:str='y')\n",
       "\n",
       "*Scaled Multi-Quantile loss (SMQL)\n",
       "\n",
       "SMQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute\n",
       "difference between predicted quantiles and observed values \n",
       "scaled by the mean absolute errors of the seasonal naive model. \n",
       "The limit behavior of MQL allows to measure the accuracy\n",
       "of a full predictive distribution with\n",
       "the continuous ranked probability score (CRPS). This can be achieved\n",
       "through a numerical integration technique, that discretizes the quantiles\n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over\n",
       "uniformly distanced quantiles.\n",
       "This was the official measure used in the M5 Uncertainty competition\n",
       "with seasonality = 1.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L697){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_mqloss\n",
       "\n",
       ">      scaled_mqloss (df:~DFType, models:Dict[str,List[str]],\n",
       ">                     quantiles:numpy.ndarray, seasonality:int,\n",
       ">                     train_df:~DFType, id_col:str='unique_id',\n",
       ">                     target_col:str='y')\n",
       "\n",
       "*Scaled Multi-Quantile loss (SMQL)\n",
       "\n",
       "SMQL calculates the average multi-quantile Loss for\n",
       "a given set of quantiles, based on the absolute\n",
       "difference between predicted quantiles and observed values \n",
       "scaled by the mean absolute errors of the seasonal naive model. \n",
       "The limit behavior of MQL allows to measure the accuracy\n",
       "of a full predictive distribution with\n",
       "the continuous ranked probability score (CRPS). This can be achieved\n",
       "through a numerical integration technique, that discretizes the quantiles\n",
       "and treats the CRPS integral with a left Riemann approximation, averaging over\n",
       "uniformly distanced quantiles.\n",
       "This was the official measure used in the M5 Uncertainty competition\n",
       "with seasonality = 1.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| seasonality | int |  | Main frequency of the time series;<br>Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1. |\n",
       "| train_df | DFType |  | Training dataframe with id and actual values. Must be sorted by time. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(scaled_mqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    scaled_mqloss(series, mq_models, quantiles=quantiles, seasonality=1, train_df=series),\n",
    "    scaled_mqloss(series_pl, mq_models, quantiles=quantiles, seasonality=1, train_df=series_pl),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def coverage(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    level: int,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Coverage of y with y_hat_lo and y_hat_hi.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    level : int\n",
    "        Confidence level used for intervals.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.jstor.org/stable/2629907        \n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        out = np.empty((df.shape[0], len(models)))\n",
    "        for j, model in enumerate(models):\n",
    "            out[:, j] = df[target_col].between(df[f'{model}-lo-{level}'], df[f'{model}-hi-{level}'])\n",
    "        res = pd.DataFrame(\n",
    "            out, columns=models, index=df.index\n",
    "        ).groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return pl.col(target_col).is_between(pl.col(f'{model}-lo-{level}'), pl.col(f'{model}-hi-{level}')).alias(model)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L762){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### coverage\n",
       "\n",
       ">      coverage (df:~DFType, models:List[str], level:int,\n",
       ">                id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Coverage of y with y_hat_lo and y_hat_hi.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| level | int |  | Confidence level used for intervals. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L762){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### coverage\n",
       "\n",
       ">      coverage (df:~DFType, models:List[str], level:int,\n",
       ">                id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Coverage of y with y_hat_lo and y_hat_hi.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| level | int |  | Confidence level used for intervals. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(coverage, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    coverage(series, models, 80),\n",
    "    coverage(series_pl, models, 80),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calibration(\n",
    "    df: DFType,\n",
    "    models: Dict[str, str],\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"\n",
    "    Fraction of y that is lower than the model's predictions. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to str\n",
    "        Mapping from model name to the model predictions.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.jstor.org/stable/2629907            \n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        out = np.empty((df.shape[0], len(models)))\n",
    "        for j, q_preds in enumerate(models.values()):\n",
    "            out[:, j] = df[target_col].le(df[q_preds])\n",
    "        res = pd.DataFrame(out, columns=models.keys(), index=df.index).groupby(df[id_col], observed=True).mean()\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            model_name, q_preds = model\n",
    "            return pl.col(target_col).le(pl.col(q_preds)).alias(model_name)\n",
    "\n",
    "        res = _pl_agg_expr(df, list(models.items()), id_col, gen_expr)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L821){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### calibration\n",
       "\n",
       ">      calibration (df:~DFType, models:Dict[str,str], id_col:str='unique_id',\n",
       ">                   target_col:str='y')\n",
       "\n",
       "*Fraction of y that is lower than the model's predictions.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L821){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### calibration\n",
       "\n",
       ">      calibration (df:~DFType, models:Dict[str,str], id_col:str='unique_id',\n",
       ">                   target_col:str='y')\n",
       "\n",
       "*Fraction of y that is lower than the model's predictions.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(calibration, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    calibration(series, q_models[0.1]),\n",
    "    calibration(series_pl, q_models[0.1]),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRPS\n",
    "\n",
    "$$\n",
    "\\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n",
    "\\int^{1}_{0} \\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq\n",
    "$$\n",
    "\n",
    "Where $\\hat{F}_{\\tau}$ is the an estimated multivariate distribution, and $y_{i,\\tau}$ are its realizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scaled_crps(\n",
    "    df: DFType,\n",
    "    models: Dict[str, List[str]],\n",
    "    quantiles: np.ndarray,\n",
    "    id_col: str = 'unique_id',\n",
    "    target_col: str = 'y',\n",
    ") -> DFType:\n",
    "    \"\"\"Scaled Continues Ranked Probability Score\n",
    "    \n",
    "    Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
    "    to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
    "    This metric averages percentual weighted absolute deviations as \n",
    "    defined by the quantile losses.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, times, actuals and predictions.\n",
    "    models : dict from str to list of str\n",
    "        Mapping from model name to the model predictions for each quantile.\n",
    "    quantiles : numpy array\n",
    "        Quantiles to compare against.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars Dataframe\n",
    "        dataframe with one row per id and one column per model.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://proceedings.mlr.press/v139/rangapuram21a.html        \n",
    "    \"\"\"\n",
    "    eps: np.float64 = np.finfo(np.float64).eps\n",
    "    quantiles = np.asarray(quantiles)\n",
    "    loss = mqloss(df, models, quantiles, id_col, target_col)\n",
    "    sizes = ufp.counts_by_id(df, id_col)\n",
    "    if isinstance(loss, pd.DataFrame):\n",
    "        loss = loss.set_index(id_col)\n",
    "        sizes = sizes.set_index(id_col)\n",
    "        assert isinstance(df, pd.DataFrame)\n",
    "        norm = df[target_col].abs().groupby(df[id_col], observed=True).sum()\n",
    "        res = 2 * loss.mul(sizes['counts'], axis=0).div(norm + eps, axis=0)\n",
    "        res.index.name = id_col\n",
    "        res = res.reset_index()\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return (2 * pl.col(model) * pl.col('counts') / (pl.col('norm') + eps)).alias(model)\n",
    "\n",
    "        grouped_df = ufp.group_by(df, id_col)\n",
    "        norm = grouped_df.agg(pl.col(target_col).abs().sum().alias('norm'))\n",
    "        res = _pl_agg_expr(\n",
    "            loss.join(sizes, on=id_col).join(norm, on=id_col),\n",
    "            list(models.keys()),\n",
    "            id_col,\n",
    "            gen_expr,\n",
    "        )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L871){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### scaled_crps\n",
       "\n",
       ">      scaled_crps (df:~DFType, models:Dict[str,List[str]],\n",
       ">                   quantiles:numpy.ndarray, id_col:str='unique_id',\n",
       ">                   target_col:str='y')\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L871){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### scaled_crps\n",
       "\n",
       ">      scaled_crps (df:~DFType, models:Dict[str,List[str]],\n",
       ">                   quantiles:numpy.ndarray, id_col:str='unique_id',\n",
       ">                   target_col:str='y')\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, times, actuals and predictions. |\n",
       "| models | Dict |  | Mapping from model name to the model predictions for each quantile. |\n",
       "| quantiles | ndarray |  | Quantiles to compare against. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(scaled_crps, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    scaled_crps(series, mq_models, quantiles),\n",
    "    scaled_crps(series_pl, mq_models, quantiles),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweedie Deviance\n",
    "\n",
    "For a set of forecasts $\\{\\mu_i\\}_{i=1}^N$ and observations $\\{y_i\\}_{i=1}^N$, the mean Tweedie deviance with power $p$ is\n",
    "\n",
    "$$\n",
    "\\mathrm{TD}_{p}(\\boldsymbol{\\mu}, \\mathbf{y})\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} d_{p}(y_i, \\mu_i)\n",
    "$$\n",
    "\n",
    "where the unit-scaled deviance for each pair $(y,\\mu)$ is\n",
    "\n",
    "$$\n",
    "d_{p}(y,\\mu)\n",
    "=\n",
    "2\n",
    "\\begin{cases}\n",
    "\\displaystyle\n",
    "\\frac{y^{2-p}}{(1-p)(2-p)}\n",
    "\\;-\\;\n",
    "\\frac{y\\,\\mu^{1-p}}{1-p}\n",
    "\\;+\\;\n",
    "\\frac{\\mu^{2-p}}{2-p}, \n",
    "& p \\notin\\{1,2\\},\\\\[1em]\n",
    "\\displaystyle\n",
    "y\\,\\ln\\!\\frac{y}{\\mu}\\;-\\;(y-\\mu),\n",
    "& p = 1\\quad(\\text{Poisson deviance}),\\\\[0.5em]\n",
    "\\displaystyle\n",
    "-2\\Bigl[\\ln\\!\\frac{y}{\\mu}\\;-\\;\\frac{y-\\mu}{\\mu}\\Bigr],\n",
    "& p = 2\\quad(\\text{Gamma deviance}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $y_i$ are the true values, $\\mu_i$ the predicted means.  \n",
    "- $p$ controls the variance relationship $\\mathrm{Var}(Y)\\propto\\mu^{p}$.  \n",
    "- When $1<p<2$, this smoothly interpolates between Poisson ($p=1$) and Gamma ($p=2$) deviance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_tweedie_deviance(y_true: ArrayLike, y_pred: ArrayLike, power: float):\n",
    "    \"\"\"\n",
    "    Compute the average Tweedie deviance between true values and predictions.\n",
    "\n",
    "    The Tweedie deviance is defined differently depending on the power parameter:\n",
    "      - power = 0: equivalent to mean squared error.\n",
    "      - power = 1: equivalent to mean Poisson deviance.\n",
    "      - power = 2: equivalent to mean gamma deviance.\n",
    "      - other powers: general Tweedie deviance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values. Must be convertible to a NumPy array of floats.\n",
    "    y_pred : array-like\n",
    "        Predicted target values. Must be convertible to a NumPy array of floats and strictly positive.\n",
    "    power : float\n",
    "        Tweedie power parameter. Determines the distribution:\n",
    "        0 for normal, 1 for Poisson, 2 for gamma, else general.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Average Tweedie deviance over all samples\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    if np.any(y_pred <= 0):\n",
    "        raise ValueError(\"All predictions must be strictly positive for Tweedie deviance.\")\n",
    "\n",
    "    if power == 0:\n",
    "        dev = (y_true - y_pred) ** 2\n",
    "\n",
    "    elif power == 1:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            dev = 2 * (y_true * np.log(y_true / y_pred) - (y_true - y_pred))\n",
    "        zero_mask = (y_true == 0)\n",
    "        dev[zero_mask] = 2 * y_pred[zero_mask]\n",
    "\n",
    "    elif power == 2:\n",
    "        dev = 2 * (-np.log(y_true / y_pred) + (y_true / y_pred) - 1)\n",
    "\n",
    "    else:\n",
    "        dev = 2 * (\n",
    "            y_true**(2 - power) / ((1 - power) * (2 - power))\n",
    "            - y_true * y_pred**(1 - power) / (1 - power)\n",
    "            + y_pred**(2 - power) / (2 - power)\n",
    "        )\n",
    "\n",
    "    return np.mean(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L937){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mean_tweedie_deviance\n",
       "\n",
       ">      mean_tweedie_deviance (y_true:Union[numpy._typing._array_like._Buffer,num\n",
       ">                             py._typing._array_like._SupportsArray[numpy.dtype[\n",
       ">                             Any]],numpy._typing._nested_sequence._NestedSequen\n",
       ">                             ce[numpy._typing._array_like._SupportsArray[numpy.\n",
       ">                             dtype[Any]]],bool,int,float,complex,str,bytes,nump\n",
       ">                             y._typing._nested_sequence._NestedSequence[bool|in\n",
       ">                             t|float|complex|str|bytes]], y_pred:Union[numpy._t\n",
       ">                             yping._array_like._Buffer,numpy._typing._array_lik\n",
       ">                             e._SupportsArray[numpy.dtype[Any]],numpy._typing._\n",
       ">                             nested_sequence._NestedSequence[numpy._typing._arr\n",
       ">                             ay_like._SupportsArray[numpy.dtype[Any]]],bool,int\n",
       ">                             ,float,complex,str,bytes,numpy._typing._nested_seq\n",
       ">                             uence._NestedSequence[bool|int|float|complex|str|b\n",
       ">                             ytes]], power:float)\n",
       "\n",
       "*Compute the average Tweedie deviance between true values and predictions.\n",
       "\n",
       "The Tweedie deviance is defined differently depending on the power parameter:\n",
       "  - power = 0: equivalent to mean squared error.\n",
       "  - power = 1: equivalent to mean Poisson deviance.\n",
       "  - power = 2: equivalent to mean gamma deviance.\n",
       "  - other powers: general Tweedie deviance.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| y_true | Union | Ground truth (correct) target values. Must be convertible to a NumPy array of floats. |\n",
       "| y_pred | Union | Predicted target values. Must be convertible to a NumPy array of floats and strictly positive. |\n",
       "| power | float | Tweedie power parameter. Determines the distribution:<br>0 for normal, 1 for Poisson, 2 for gamma, else general. |\n",
       "| **Returns** | **float** | **Average Tweedie deviance over all samples** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L937){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### mean_tweedie_deviance\n",
       "\n",
       ">      mean_tweedie_deviance (y_true:Union[numpy._typing._array_like._Buffer,num\n",
       ">                             py._typing._array_like._SupportsArray[numpy.dtype[\n",
       ">                             Any]],numpy._typing._nested_sequence._NestedSequen\n",
       ">                             ce[numpy._typing._array_like._SupportsArray[numpy.\n",
       ">                             dtype[Any]]],bool,int,float,complex,str,bytes,nump\n",
       ">                             y._typing._nested_sequence._NestedSequence[bool|in\n",
       ">                             t|float|complex|str|bytes]], y_pred:Union[numpy._t\n",
       ">                             yping._array_like._Buffer,numpy._typing._array_lik\n",
       ">                             e._SupportsArray[numpy.dtype[Any]],numpy._typing._\n",
       ">                             nested_sequence._NestedSequence[numpy._typing._arr\n",
       ">                             ay_like._SupportsArray[numpy.dtype[Any]]],bool,int\n",
       ">                             ,float,complex,str,bytes,numpy._typing._nested_seq\n",
       ">                             uence._NestedSequence[bool|int|float|complex|str|b\n",
       ">                             ytes]], power:float)\n",
       "\n",
       "*Compute the average Tweedie deviance between true values and predictions.\n",
       "\n",
       "The Tweedie deviance is defined differently depending on the power parameter:\n",
       "  - power = 0: equivalent to mean squared error.\n",
       "  - power = 1: equivalent to mean Poisson deviance.\n",
       "  - power = 2: equivalent to mean gamma deviance.\n",
       "  - other powers: general Tweedie deviance.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| y_true | Union | Ground truth (correct) target values. Must be convertible to a NumPy array of floats. |\n",
       "| y_pred | Union | Predicted target values. Must be convertible to a NumPy array of floats and strictly positive. |\n",
       "| power | float | Tweedie power parameter. Determines the distribution:<br>0 for normal, 1 for Poisson, 2 for gamma, else general. |\n",
       "| **Returns** | **float** | **Average Tweedie deviance over all samples** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mean_tweedie_deviance, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tweedie_deviance(\n",
    "    df: DFType,\n",
    "    models: List[str],\n",
    "    power: float = 1.5,  \n",
    "    id_col: str = \"unique_id\",\n",
    "    target_col: str = \"y\",\n",
    ") -> DFType:\n",
    "    \"\"\"Compute the Tweedie deviance loss for one or multiple models, grouped by an identifier.\n",
    "\n",
    "    Each group's deviance is calculated using the mean_tweedie_deviance function, which\n",
    "    measures the deviation between actual and predicted values under the Tweedie distribution.\n",
    "\n",
    "    The power parameter defines the specific compound distribution:\n",
    "      - 1: Poisson\n",
    "      - (1, 2): Compound Poisson-Gamma\n",
    "      - 2: Gamma\n",
    "      - >2: Inverse Gaussian\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Input dataframe with id, actual values and predictions.\n",
    "    models : list of str\n",
    "        Columns that identify the models predictions.\n",
    "    power : float, optional (default=1.5)\n",
    "        Tweedie power parameter defining the distribution.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas or polars DataFrame\n",
    "        dataframe with one row per id and one column per model.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        res = (\n",
    "            df.groupby(id_col, observed=True)\n",
    "            .apply(lambda g: {\n",
    "                model: mean_tweedie_deviance(\n",
    "                    y_true=g[target_col],\n",
    "                    y_pred=g[model],\n",
    "                    power=power\n",
    "                )\n",
    "                for model in models\n",
    "            })\n",
    "            .apply(pd.Series)\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        def gen_expr(model):\n",
    "            return pl.struct([target_col, model]).map_elements(\n",
    "                lambda s: mean_tweedie_deviance(\n",
    "                    y_true=[s[target_col]],\n",
    "                    y_pred=[s[model]],\n",
    "                    power=power\n",
    "                ),\n",
    "                return_dtype=pl.Float64\n",
    "            ).alias(model)\n",
    "\n",
    "        res = _pl_agg_expr(df, models, id_col, gen_expr)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L998){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### tweedie_deviance\n",
       "\n",
       ">      tweedie_deviance (df:~DFType, models:List[str], power:float=1.5,\n",
       ">                        id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Compute the Tweedie deviance loss for one or multiple models, grouped by an identifier.\n",
       "\n",
       "Each group's deviance is calculated using the mean_tweedie_deviance function, which\n",
       "measures the deviation between actual and predicted values under the Tweedie distribution.\n",
       "\n",
       "The power parameter defines the specific compound distribution:\n",
       "  - 1: Poisson\n",
       "  - (1, 2): Compound Poisson-Gamma\n",
       "  - 2: Gamma\n",
       "  - >2: Inverse Gaussian*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| power | float | 1.5 | Tweedie power parameter defining the distribution. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/utilsforecast/blob/main/utilsforecast/losses.py#L998){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### tweedie_deviance\n",
       "\n",
       ">      tweedie_deviance (df:~DFType, models:List[str], power:float=1.5,\n",
       ">                        id_col:str='unique_id', target_col:str='y')\n",
       "\n",
       "*Compute the Tweedie deviance loss for one or multiple models, grouped by an identifier.\n",
       "\n",
       "Each group's deviance is calculated using the mean_tweedie_deviance function, which\n",
       "measures the deviation between actual and predicted values under the Tweedie distribution.\n",
       "\n",
       "The power parameter defines the specific compound distribution:\n",
       "  - 1: Poisson\n",
       "  - (1, 2): Compound Poisson-Gamma\n",
       "  - 2: Gamma\n",
       "  - >2: Inverse Gaussian*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DFType |  | Input dataframe with id, actual values and predictions. |\n",
       "| models | List |  | Columns that identify the models predictions. |\n",
       "| power | float | 1.5 | Tweedie power parameter defining the distribution. |\n",
       "| id_col | str | unique_id | Column that identifies each serie. |\n",
       "| target_col | str | y | Column that contains the target. |\n",
       "| **Returns** | **DFType** |  | **dataframe with one row per id and one column per model.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tweedie_deviance, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/xrd363zx571_wq54f3htnbxr0000gn/T/ipykernel_8131/2024453147.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: {\n"
     ]
    }
   ],
   "source": [
    "#| polars\n",
    "pd_vs_pl(\n",
    "    tweedie_deviance(series,   models, target_col=\"y\", power=1.5),\n",
    "    tweedie_deviance(series_pl,models, target_col=\"y\", power=1.5),\n",
    "    models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
